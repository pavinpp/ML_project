{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5986cd18",
   "metadata": {},
   "source": [
    "# Rice Yield Prediction: PyTorch LSTM (Seq_Len=3) with Optuna Tuning\n",
    "\n",
    "## Overview\n",
    "This notebook trains a **Long Short-Term Memory (LSTM)** network using **PyTorch** to predict Rice yields. \n",
    "\n",
    "**Updates:**\n",
    "1.  **Sequence Length:** Increased to **3** to capture temporal dependencies (Last 3 years -> Current Year).\n",
    "2.  **Initial Hyperparameters:** Updated based on previous Optuna results to provide a better baseline.\n",
    "\n",
    "## Methodology\n",
    "1.  **Data Processing:** \n",
    "    * Impute and Scale data.\n",
    "    * Generate sliding window sequences of length 3, grouped by Area/Location.\n",
    "2.  **Split:** Time-series split by Year on the generated sequences.\n",
    "3.  **Model:** Stacked LSTM.\n",
    "4.  **Optimization:** Optuna tuning.\n",
    "5.  **Evaluation:** RMSE & R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a747c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Optuna Viz\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b386",
   "metadata": {},
   "source": [
    "### 1. Data Loading & Preprocessing\n",
    "To improve performance, we create **sequences of 3 years** for the LSTM input. This requires sorting by Area and Year and using a sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_parquet('Parquet/XY_version1.parquet')\n",
    "TARGET_COL = 'Y_rice'\n",
    "LAG_1_FEATURE = 'avg_yield_rice_1y'\n",
    "\n",
    "# Clean Missing Targets\n",
    "df_model = df.dropna(subset=[TARGET_COL]).copy()\n",
    "\n",
    "# Identify Features (exclude metadata)\n",
    "feature_cols = [c for c in df_model.columns if not c.startswith('Y_') and c not in ['year', 'area']]\n",
    "\n",
    "# --- PREPROCESSING ---\n",
    "# 1. Impute & Scale (Fit on Train, Transform All)\n",
    "# We need to do this BEFORE creating sequences to ensure consistency\n",
    "TRAIN_END_YEAR = 2016\n",
    "VAL_END_YEAR = 2020\n",
    "\n",
    "# Determine train population for fitting scaler\n",
    "train_mask_fit = df_model['year'] < TRAIN_END_YEAR\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data subset\n",
    "X_fit = df_model.loc[train_mask_fit, feature_cols]\n",
    "imputer.fit(X_fit)\n",
    "scaler.fit(imputer.transform(X_fit))\n",
    "\n",
    "# Transform entire dataframe\n",
    "df_model[feature_cols] = scaler.transform(imputer.transform(df_model[feature_cols]))\n",
    "\n",
    "# --- SEQUENCE GENERATION ---\n",
    "SEQ_LEN = 3\n",
    "\n",
    "def create_sequences(data, seq_len, feat_cols, target_col, group_col='area', time_col='year'):\n",
    "    \"\"\"\n",
    "    Creates (Batch, Seq, Feat) tensors using sliding window, grouped by area.\n",
    "    \"\"\"\n",
    "    # Sort properly to ensure temporal order\n",
    "    data = data.sort_values([group_col, time_col]).reset_index(drop=True)\n",
    "    \n",
    "    # Create shifted features for the window [t-(N-1), ..., t]\n",
    "    # We will shift the dataframe 'down' to align past rows with current target\n",
    "    # Note: shift(1) means t-1 moves to t's position.\n",
    "    \n",
    "    sequences = []\n",
    "    # We want: [t-2, t-1, t]\n",
    "    # Shift(2) brings t-2 to row t\n",
    "    # Shift(1) brings t-1 to row t\n",
    "    # Shift(0) is t\n",
    "    for i in range(seq_len - 1, -1, -1):\n",
    "        sequences.append(data[feat_cols].shift(i).values)\n",
    "    \n",
    "    # Stack to get (N, Seq_Len, Features)\n",
    "    X_stacked = np.stack(sequences, axis=1)\n",
    "    y_arr = data[target_col].values\n",
    "    years_arr = data[time_col].values\n",
    "    areas_arr = data[group_col].values\n",
    "    \n",
    "    # Validate Sequences: Ensure all steps in a sequence belong to the same Area\n",
    "    # We check if Area at t == Area at t-(N-1)\n",
    "    valid_mask = (data[group_col] == data[group_col].shift(seq_len - 1))\n",
    "    # Also drop NaNs created by shifting\n",
    "    valid_mask = valid_mask & ~data[feat_cols].shift(seq_len - 1).isna().any(axis=1)\n",
    "    \n",
    "    return X_stacked[valid_mask], y_arr[valid_mask], years_arr[valid_mask]\n",
    "\n",
    "print(f\"Generating sequences with length {SEQ_LEN}...\")\n",
    "X_seq, y_seq, years_seq = create_sequences(df_model, SEQ_LEN, feature_cols, TARGET_COL)\n",
    "\n",
    "# --- SPLIT ---\n",
    "mask_train = years_seq < TRAIN_END_YEAR\n",
    "mask_val = (years_seq >= TRAIN_END_YEAR) & (years_seq < VAL_END_YEAR)\n",
    "mask_test = years_seq >= VAL_END_YEAR\n",
    "\n",
    "X_train_t = torch.tensor(X_seq[mask_train], dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_seq[mask_train], dtype=torch.float32).to(device)\n",
    "\n",
    "X_val_t = torch.tensor(X_seq[mask_val], dtype=torch.float32).to(device)\n",
    "y_val_t = torch.tensor(y_seq[mask_val], dtype=torch.float32).to(device)\n",
    "\n",
    "X_test_t = torch.tensor(X_seq[mask_test], dtype=torch.float32).to(device)\n",
    "y_test_t = torch.tensor(y_seq[mask_test], dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"Train Shape: {X_train_t.shape}\")\n",
    "print(f\"Val Shape:   {X_val_t.shape}\")\n",
    "print(f\"Test Shape:  {X_test_t.shape}\")\n",
    "\n",
    "# Calculate Baseline (needs aligning with test set)\n",
    "# We use the original dataframe to get the LAG_1 feature for the corresponding test rows\n",
    "# (Actually, since we dropped rows during sequence generation, we must rely on index/matching or simpler logic)\n",
    "# Simpler logic: Re-extract baseline for the valid indices found in sequence generation\n",
    "# However, since we don't track indices easily, we can just assume 'Last Year Yield' is a feature in the input sequence.\n",
    "# LAG_1_FEATURE is 'avg_yield_rice_1y'. In our sequence [t-2, t-1, t], the features at step 't' (index -1) contains lag1 of t+1? \n",
    "# No, features at row t are inputs for predicting y_t.\n",
    "# 'avg_yield_rice_1y' at row t IS the yield of t-1. So that is our baseline prediction.\n",
    "# We need the index of 'avg_yield_rice_1y' in feature_cols.\n",
    "\n",
    "lag1_idx = feature_cols.index(LAG_1_FEATURE)\n",
    "# Extract baseline predictions from the test input tensor (Last Step, Lag1 Feature)\n",
    "# Note: Data is scaled. We must unscale it or scale y_test? \n",
    "# Better: use the raw y values for metric. The baseline feature is scaled.\n",
    "# Reverse scaling for baseline:\n",
    "y_pred_baseline_scaled = X_test_t[:, -1, lag1_idx].cpu().numpy()\n",
    "\n",
    "# To unscale single feature is messy. Let's just re-fetch raw baseline for valid test rows.\n",
    "# We sort df same way as create_sequences did\n",
    "df_sorted = df_model.sort_values(['area', 'year']).reset_index(drop=True)\n",
    "# Re-apply mask logic (copy-paste from func)\n",
    "valid_mask = (df_sorted['area'] == df_sorted['area'].shift(SEQ_LEN - 1))\n",
    "valid_mask = valid_mask & ~df_sorted[feature_cols].shift(SEQ_LEN - 1).isna().any(axis=1)\n",
    "# Get valid rows\n",
    "df_valid = df_sorted[valid_mask]\n",
    "# Filter for test years\n",
    "df_valid_test = df_valid[df_valid['year'] >= VAL_END_YEAR]\n",
    "\n",
    "# Raw baseline from raw dataframe (unscaled)\n",
    "# We must reload raw df because df_model was scaled in-place\n",
    "df_raw = pd.read_parquet('Parquet/XY_version1.parquet').dropna(subset=[TARGET_COL])\n",
    "df_raw = df_raw.sort_values(['area', 'year']).reset_index(drop=True)\n",
    "df_raw_valid = df_raw[valid_mask]\n",
    "df_raw_test = df_raw_valid[df_raw_valid['year'] >= VAL_END_YEAR]\n",
    "\n",
    "y_test_raw = df_raw_test[TARGET_COL].values\n",
    "y_base_raw = df_raw_test[LAG_1_FEATURE].values\n",
    "\n",
    "rmse_base = np.sqrt(mean_squared_error(y_test_raw, y_base_raw))\n",
    "print(f\"Baseline RMSE: {rmse_base:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607fbb7",
   "metadata": {},
   "source": [
    "### 2. LSTM Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774242a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiceYieldLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size=64, n_layers=2, dropout=0.1):\n",
    "        super(RiceYieldLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, feat)\n",
    "        out, _ = self.lstm(x)\n",
    "        # Last time step\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "def train_model(model, train_loader, X_val, y_val, epochs=100, lr=0.001, patience=15):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    history = {'rmse': [], 'val_rmse': []}\n",
    "    best_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_losses = []\n",
    "        for X_b, y_b in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_b)\n",
    "            loss = criterion(pred, y_b.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(loss.item())\n",
    "            \n",
    "        # Val\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val.unsqueeze(1))\n",
    "            val_rmse = np.sqrt(val_loss.item())\n",
    "            \n",
    "        history['rmse'].append(np.sqrt(np.mean(batch_losses)))\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        \n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_weights = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "            \n",
    "    if best_weights: model.load_state_dict(best_weights)\n",
    "    return history\n",
    "\n",
    "# --- INITIAL MODEL (Updated Params) ---\n",
    "# Params from previous Optuna tuning (rounded)\n",
    "init_params = {\n",
    "    'hidden_size': 192,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.28,\n",
    "    'learning_rate': 0.027,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=init_params['batch_size'], shuffle=True)\n",
    "\n",
    "model_init = RiceYieldLSTM(\n",
    "    input_dim=X_train_t.shape[2], \n",
    "    hidden_size=init_params['hidden_size'], \n",
    "    n_layers=init_params['n_layers'],\n",
    "    dropout=init_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "print(\"Training Initial LSTM (Seq Len=3)...\")\n",
    "hist_init = train_model(\n",
    "    model_init, train_loader, X_val_t, y_val_t, \n",
    "    epochs=150, \n",
    "    lr=init_params['learning_rate'], \n",
    "    patience=15\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist_init['rmse'], label='Train RMSE')\n",
    "plt.plot(hist_init['val_rmse'], label='Val RMSE')\n",
    "plt.title('Initial LSTM Training History')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Init Eval\n",
    "model_init.eval()\n",
    "with torch.no_grad():\n",
    "    init_pred = model_init(X_test_t).cpu().numpy().flatten()\n",
    "rmse_init = np.sqrt(mean_squared_error(y_test_raw, init_pred))\n",
    "print(f\"Initial LSTM Test RMSE: {rmse_init:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a1556",
   "metadata": {},
   "source": [
    "### 3. Optuna Optimization\n",
    "Tuning with sequence-based data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d396ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256, step=32)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.4)\n",
    "    lr = trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    \n",
    "    ds = TensorDataset(X_train_t, y_train_t)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = RiceYieldLSTM(\n",
    "        input_dim=X_train_t.shape[2], \n",
    "        hidden_size=hidden_size,\n",
    "        n_layers=n_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Reduced patience for tuning speed\n",
    "    hist = train_model(model, loader, X_val_t, y_val_t, epochs=80, lr=lr, patience=8)\n",
    "    return min(hist['val_rmse'])\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name='Rice_LSTM_Seq3')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best Params:\", study.best_params)\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654b21e",
   "metadata": {},
   "source": [
    "### 4. Final Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combine Train+Val for Final Training\n",
    "X_final_t = torch.cat([X_train_t, X_val_t], dim=0)\n",
    "y_final_t = torch.cat([y_train_t, y_val_t], dim=0)\n",
    "\n",
    "# 2. Params\n",
    "bp = study.best_params\n",
    "final_loader = DataLoader(TensorDataset(X_final_t, y_final_t), batch_size=bp['batch_size'], shuffle=True)\n",
    "\n",
    "# 3. Model\n",
    "final_model = RiceYieldLSTM(\n",
    "    input_dim=X_train_t.shape[2], \n",
    "    hidden_size=bp['hidden_size'], \n",
    "    n_layers=bp['n_layers'],\n",
    "    dropout=bp['dropout']\n",
    ").to(device)\n",
    "\n",
    "# 4. Train\n",
    "print(\"Training Final Model (Full Data)...\")\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=bp['learning_rate'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(150):\n",
    "    for X_b, y_b in final_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = final_model(X_b)\n",
    "        loss = criterion(pred, y_b.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 5. Eval\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = final_model(X_test_t).cpu().numpy().flatten()\n",
    "\n",
    "rmse_final = np.sqrt(mean_squared_error(y_test_raw, final_pred))\n",
    "r2_final = r2_score(y_test_raw, final_pred)\n",
    "imp = (rmse_base - rmse_final) / rmse_base * 100\n",
    "\n",
    "print(f\"\\nFinal LSTM Test RMSE: {rmse_final:.2f} (Improved {imp:.2f}%)\")\n",
    "print(f\"Final LSTM Test R2:   {r2_final:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test_raw, y_base_raw, alpha=0.5, color='gray')\n",
    "plt.plot([y_test_raw.min(), y_test_raw.max()], [y_test_raw.min(), y_test_raw.max()], 'r--')\n",
    "plt.title(f'Baseline RMSE: {rmse_base:.2f}')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_test_raw, init_pred, alpha=0.5, color='orange')\n",
    "plt.plot([y_test_raw.min(), y_test_raw.max()], [y_test_raw.min(), y_test_raw.max()], 'r--')\n",
    "plt.title(f'Initial LSTM RMSE: {rmse_init:.2f}')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(y_test_raw, final_pred, alpha=0.5, color='green')\n",
    "plt.plot([y_test_raw.min(), y_test_raw.max()], [y_test_raw.min(), y_test_raw.max()], 'r--')\n",
    "plt.title(f'Tuned LSTM RMSE: {rmse_final:.2f}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}