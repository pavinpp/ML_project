{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import copy\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optuna Visualization Tools\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4805a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. LOAD & CONFIG\n",
    "# ==============================\n",
    "CHOSEN_CROP = \"rice\" \n",
    "TARGET_COL = f\"Y_{CHOSEN_CROP}\"\n",
    "PARQUET_PATH = \"Parquet/XY_v2.parquet\"\n",
    "SEQ_LEN     = 5              # try larger windows; model uses past SEQ_LEN years\n",
    "BATCH_SIZE  = 32\n",
    "CLIP_QUANT  = 0.01           # 1%-99% clipping to tame outliers\n",
    "EPOCHS      = 100\n",
    "PATIENCE    = 15\n",
    "LR          = 2e-4\n",
    "\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df = df[(df[\"year\"] >= 1982) & (df[\"year\"] <= 2023)].copy()\n",
    "df = df.dropna(subset=[TARGET_COL]).copy()\n",
    "print(f\"--> Filtered years (1982-2023) and dropped NaN targets. New Shape: {df.shape}\")\n",
    "\n",
    "LAG_1_FEATURE = f'avg_yield_{CHOSEN_CROP}_1y'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 1. PREPROCESSING + SEQUENCE BUILDING  (WITH VERBOSE PRINTS)\n",
    "# ============================================================\n",
    "def preprocess_and_build_loaders(\n",
    "    df: pd.DataFrame,\n",
    "    chosen_crop: str,\n",
    "    target_col: str,\n",
    "    seq_len: int = 5,\n",
    "    batch_size: int = 32,\n",
    "    clip_quant: float = 0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full preprocessing + sequence-building pipeline for panel time series LSTM.\n",
    "    Now with verbose prints so you can see what each step is doing.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"========== PREPROCESSING START ==========\")\n",
    "    print(f\"Chosen crop: {chosen_crop}\")\n",
    "    print(f\"Target col:  {target_col}\")\n",
    "    print(f\"Seq len:     {seq_len}\")\n",
    "    print(f\"Batch size:  {batch_size}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    df_model = df.copy()\n",
    "    print(f\"Initial df shape: {df_model.shape}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 1) Drop rows with NaN target\n",
    "    # -------------------------------------------\n",
    "    n_before = len(df_model)\n",
    "    df_model = df_model[~df_model[target_col].isna()].copy()\n",
    "    n_after = len(df_model)\n",
    "    print(f\"[Step 1] Dropped rows with NaN target: {n_before - n_after} rows removed\")\n",
    "    print(f\"        Remaining rows: {n_after}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 2) Target & feature selection\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 2] Target & feature selection...\")\n",
    "    # Drop other target columns\n",
    "    all_targets = [c for c in df_model.columns if c.startswith(\"Y_\")]\n",
    "    drop_other_targets = [c for c in all_targets if c != target_col]\n",
    "    df_model = df_model.drop(columns=drop_other_targets)\n",
    "    print(f\"        Target columns found: {all_targets}\")\n",
    "    print(f\"        Keeping target: {target_col}\")\n",
    "    print(f\"        Dropping other targets: {drop_other_targets}\")\n",
    "\n",
    "    # Drop unrelated avg_yield_* columns\n",
    "    avg_yield_cols = [c for c in df_model.columns if c.startswith(\"avg_yield_\")]\n",
    "    chosen_prefix = f\"avg_yield_{chosen_crop}_\"\n",
    "    keep_avg_yield = [c for c in avg_yield_cols if c.startswith(chosen_prefix)]\n",
    "    drop_avg_yield = list(set(avg_yield_cols) - set(keep_avg_yield))\n",
    "    df_model = df_model.drop(columns=drop_avg_yield)\n",
    "\n",
    "    print(f\"        Total avg_yield_* cols: {len(avg_yield_cols)}\")\n",
    "    print(f\"        Keeping {len(keep_avg_yield)} for {chosen_crop}\")\n",
    "    print(f\"        Dropping {len(drop_avg_yield)} from other crops\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 3) Add time features (trend over years)\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 3] Adding time features year_index and year_norm...\")\n",
    "    base_year = df_model[\"year\"].min()\n",
    "    df_model[\"year_index\"] = df_model[\"year\"] - base_year\n",
    "    max_index = df_model[\"year_index\"].max() if df_model[\"year_index\"].max() > 0 else 1\n",
    "    df_model[\"year_norm\"] = df_model[\"year_index\"] / max_index\n",
    "    print(f\"        Base year: {base_year}\")\n",
    "    print(f\"        Max year_index: {max_index}\")\n",
    "    print(f\"        year range in df: {df_model['year'].min()}–{df_model['year'].max()}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 4) Sort for time consistency\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 4] Sorting by ['area', 'year']...\")\n",
    "    df_model = df_model.sort_values([\"area\", \"year\"]).reset_index(drop=True)\n",
    "    print(f\"        After sort shape: {df_model.shape}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 5) Define feature columns\n",
    "    # -------------------------------------------\n",
    "    feature_cols = [\n",
    "        c\n",
    "        for c in df_model.columns\n",
    "        if c not in [\"area\", \"year\", target_col] and not c.startswith(\"Y_\")\n",
    "    ]\n",
    "    print(\"[Step 5] Defining feature columns...\")\n",
    "    print(f\"        Selected {len(feature_cols)} input features for prediction.\")\n",
    "    print(f\"        First 10 features: {feature_cols[:10]}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 6) Group-wise ffill/bfill for feature columns\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 6] Group-wise ffill/bfill within each area...\")\n",
    "    na_before = df_model[feature_cols].isna().sum().sum()\n",
    "    df_model[feature_cols] = (\n",
    "        df_model.groupby(\"area\", group_keys=False)[feature_cols]\n",
    "        .apply(lambda g: g.ffill().bfill())\n",
    "    )\n",
    "    na_after = df_model[feature_cols].isna().sum().sum()\n",
    "    print(f\"        NaNs before ffill/bfill: {na_before}\")\n",
    "    print(f\"        NaNs after  ffill/bfill: {na_after}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 7) Time-based masks (on rows; for scaling only)\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 7] Creating time-based masks (rows)...\")\n",
    "    train_mask_rows = df_model[\"year\"] < 2014\n",
    "    val_mask_rows   = (df_model[\"year\"] >= 2014) & (df_model[\"year\"] <= 2018)\n",
    "    test_mask_rows  = df_model[\"year\"] >= 2019\n",
    "\n",
    "    print(f\"        Train rows: {int(train_mask_rows.sum())}\")\n",
    "    print(f\"        Val rows:   {int(val_mask_rows.sum())}\")\n",
    "    print(f\"        Test rows:  {int(test_mask_rows.sum())}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 8) Raw arrays + NaN imputation (train-mean)\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 8] Building raw arrays and imputing remaining NaNs with train mean...\")\n",
    "    X_raw = df_model[feature_cols].values.astype(np.float32)\n",
    "    y_raw = df_model[[target_col]].values.astype(np.float32)\n",
    "\n",
    "    train_X = X_raw[train_mask_rows]\n",
    "    train_mean = np.nanmean(train_X, axis=0)\n",
    "    nan_inds = np.where(np.isnan(X_raw))\n",
    "    n_nan_total = len(nan_inds[0])\n",
    "    print(f\"        Remaining NaNs before mean-impute: {n_nan_total}\")\n",
    "    X_raw[nan_inds] = np.take(train_mean, nan_inds[1])\n",
    "    print(\"        NaNs after mean-impute: 0\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 9) Outlier clipping (train-based quantiles)\n",
    "    # -------------------------------------------\n",
    "    if clip_quant is not None and 0.0 < clip_quant < 0.5:\n",
    "        print(f\"[Step 9] Clipping outliers using train quantiles {clip_quant:.2f} and {1-clip_quant:.2f}...\")\n",
    "        train_X_no_nan = X_raw[train_mask_rows]\n",
    "        q_low = np.quantile(train_X_no_nan, clip_quant, axis=0)\n",
    "        q_high = np.quantile(train_X_no_nan, 1.0 - clip_quant, axis=0)\n",
    "\n",
    "        n_clipped_cols = 0\n",
    "        for j in range(train_X_no_nan.shape[1]):\n",
    "            if q_low[j] < q_high[j]:\n",
    "                X_raw[:, j] = np.clip(X_raw[:, j], q_low[j], q_high[j])\n",
    "                n_clipped_cols += 1\n",
    "        print(f\"        Columns clipped: {n_clipped_cols}/{train_X_no_nan.shape[1]}\")\n",
    "    else:\n",
    "        print(\"[Step 9] Skipping clipping (clip_quant is None or invalid).\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 10) Scaling (fit only on train)\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 10] Scaling X and y (StandardScaler, train-only fit)...\")\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_scaled = X_raw.copy()\n",
    "    y_scaled = y_raw.copy()\n",
    "\n",
    "    X_scaled[train_mask_rows] = scaler_X.fit_transform(X_raw[train_mask_rows])\n",
    "    X_scaled[val_mask_rows]   = scaler_X.transform(X_raw[val_mask_rows])\n",
    "    X_scaled[test_mask_rows]  = scaler_X.transform(X_raw[test_mask_rows])\n",
    "\n",
    "    y_scaled[train_mask_rows] = scaler_y.fit_transform(y_raw[train_mask_rows])\n",
    "    y_scaled[val_mask_rows]   = scaler_y.transform(y_raw[val_mask_rows])\n",
    "    y_scaled[test_mask_rows]  = scaler_y.transform(y_raw[test_mask_rows])\n",
    "\n",
    "    print(\"        Scaling complete.\")\n",
    "    print(f\"        y_scaled train mean (should be ~0): {y_scaled[train_mask_rows].mean():.4f}\")\n",
    "    print(f\"        y_scaled train std  (should be ~1): {y_scaled[train_mask_rows].std():.4f}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 11) Rebuild scaled DataFrame for sequences\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 11] Rebuilding scaled DataFrame for sequence building...\")\n",
    "    df_scaled = df_model[[\"area\", \"year\"]].copy()\n",
    "    df_scaled[\"target_scaled\"] = y_scaled.reshape(-1)\n",
    "\n",
    "    X_df = pd.DataFrame(X_scaled, columns=feature_cols, index=df_model.index)\n",
    "    df_final = pd.concat([df_scaled, X_df], axis=1)\n",
    "    print(f\"        df_final shape: {df_final.shape}\")\n",
    "    print(\"        Sample df_final head():\")\n",
    "    print(df_final.head())\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 12) Sequence building with split by TARGET YEAR\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 12] Building sequences and splitting by TARGET YEAR...\")\n",
    "\n",
    "    def build_sequences_by_split(\n",
    "        df_seq: pd.DataFrame,\n",
    "        feat_cols: list,\n",
    "        target_col_name: str,\n",
    "        seq_len_inner: int,\n",
    "    ):\n",
    "        X_train_list, y_train_list = [], []\n",
    "        X_val_list, y_val_list     = [], []\n",
    "        X_test_list, y_test_list   = [], []\n",
    "\n",
    "        for area_name, g in df_seq.groupby(\"area\"):\n",
    "            g = g.sort_values(\"year\")\n",
    "            feats = g[feat_cols].values.astype(np.float32)\n",
    "            targs = g[target_col_name].values.astype(np.float32)\n",
    "            years = g[\"year\"].values\n",
    "\n",
    "            if len(g) < seq_len_inner:\n",
    "                continue\n",
    "\n",
    "            # Sliding window: [t-seq_len+1 ... t] -> target at t\n",
    "            for i in range(len(g) - seq_len_inner + 1):\n",
    "                X_seq = feats[i : i + seq_len_inner]\n",
    "                y_t   = targs[i + seq_len_inner - 1]\n",
    "                y_yr  = years[i + seq_len_inner - 1]\n",
    "\n",
    "                if y_yr < 2014:\n",
    "                    X_train_list.append(X_seq)\n",
    "                    y_train_list.append(y_t)\n",
    "                elif 2014 <= y_yr <= 2018:\n",
    "                    X_val_list.append(X_seq)\n",
    "                    y_val_list.append(y_t)\n",
    "                else:  # y_yr >= 2019\n",
    "                    X_test_list.append(X_seq)\n",
    "                    y_test_list.append(y_t)\n",
    "\n",
    "        def to_array(x_list, y_list):\n",
    "            if len(x_list) == 0:\n",
    "                return (\n",
    "                    np.empty((0, seq_len_inner, len(feat_cols)), dtype=np.float32),\n",
    "                    np.empty((0,), dtype=np.float32),\n",
    "                )\n",
    "            X_arr = np.stack(x_list).astype(np.float32)\n",
    "            y_arr = np.array(y_list, dtype=np.float32)\n",
    "            return X_arr, y_arr\n",
    "\n",
    "        X_tr, y_tr = to_array(X_train_list, y_train_list)\n",
    "        X_v, y_v   = to_array(X_val_list, y_val_list)\n",
    "        X_te, y_te = to_array(X_test_list, y_test_list)\n",
    "\n",
    "        return X_tr, y_tr, X_v, y_v, X_te, y_te\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = build_sequences_by_split(\n",
    "        df_final,\n",
    "        feature_cols,\n",
    "        \"target_scaled\",\n",
    "        seq_len,\n",
    "    )\n",
    "\n",
    "    print(f\"        Sequence shapes:\")\n",
    "    print(f\"           Train: {X_train.shape}, targets: {y_train.shape}\")\n",
    "    print(f\"           Val:   {X_val.shape}, targets: {y_val.shape}\")\n",
    "    print(f\"           Test:  {X_test.shape}, targets: {y_test.shape}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 13) Build PyTorch DataLoaders\n",
    "    # -------------------------------------------\n",
    "    print(\"[Step 13] Building PyTorch DataLoaders...\")\n",
    "\n",
    "    def make_loader(X, y, batch_size_inner: int, shuffle: bool):\n",
    "        X_tensor = torch.from_numpy(X)  # (N, seq_len, num_features)\n",
    "        y_tensor = torch.from_numpy(y).unsqueeze(-1)  # (N, 1)\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        return DataLoader(dataset, batch_size=batch_size_inner, shuffle=shuffle)\n",
    "\n",
    "    train_loader = make_loader(X_train, y_train, batch_size, shuffle=True)\n",
    "    val_loader   = make_loader(X_val,   y_val,   batch_size, shuffle=False)\n",
    "    test_loader  = make_loader(X_test,  y_test,  batch_size, shuffle=False)\n",
    "\n",
    "    print(\"========== PREPROCESSING DONE ==========\")\n",
    "    print(f\"Train sequences: {len(train_loader.dataset)}\")\n",
    "    print(f\"Val sequences:   {len(val_loader.dataset)}\")\n",
    "    print(f\"Test sequences:  {len(test_loader.dataset)}\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, feature_cols, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_rmse(df, target_col):\n",
    "    \"\"\"\n",
    "    Compute baseline model: y(t) ≈ y(t-1)\n",
    "    Using raw (unscaled) yearly panel data.\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values([\"area\", \"year\"]).copy()\n",
    "    df_sorted[\"y_lag1\"] = df_sorted.groupby(\"area\")[target_col].shift(1)\n",
    "\n",
    "    # Only test years\n",
    "    test_mask = df_sorted[\"year\"] >= 2019\n",
    "    df_test = df_sorted[test_mask]\n",
    "\n",
    "    y_true = df_test[target_col]\n",
    "    y_pred = df_test[\"y_lag1\"]\n",
    "\n",
    "    # Drop rows where lag is not available (first year per country)\n",
    "    mask = (~y_true.isna()) & (~y_pred.isna())\n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
    "\n",
    "    return rmse, r2, y_true_clean.values, y_pred_clean.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6babbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM data loaders\n",
    "train_loader, val_loader, test_loader, feature_cols, scaler_y = preprocess_and_build_loaders(\n",
    "    df=df,\n",
    "    chosen_crop=CHOSEN_CROP,\n",
    "    target_col=TARGET_COL,\n",
    "    seq_len=SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    clip_quant=CLIP_QUANT\n",
    ")\n",
    "\n",
    "# ---- Compute baseline BEFORE training ----\n",
    "rmse_baseline, r2_baseline, y_test_clean, y_pred_clean = compute_baseline_rmse(df, TARGET_COL)\n",
    "\n",
    "print(f\"\\nBaseline model: RMSE={rmse_baseline:.2f}, R²={r2_baseline:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118602b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. MODEL DEFINITION (same as yours, but shape-consistent)\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2, activation=\"ReLU\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        # 2. Output head: Linear -> Activation -> Linear\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            getattr(nn, activation)(),  # e.g. nn.ReLU(), nn.Tanh()\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        out, _ = self.lstm(x)\n",
    "        last_step_out = out[:, -1, :]    # (batch, hidden_dim)\n",
    "        prediction = self.fc(last_step_out)  # (batch, 1)\n",
    "        return prediction                # keep (batch, 1) to match y shape\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. GET DATA LOADERS FROM DF\n",
    "#    (df must already exist in your environment)\n",
    "# ============================================================\n",
    "train_loader, val_loader, test_loader, feature_cols, scaler_y = preprocess_and_build_loaders(\n",
    "    df=df,\n",
    "    chosen_crop=CHOSEN_CROP,\n",
    "    target_col=TARGET_COL,\n",
    "    seq_len=SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    clip_quant=CLIP_QUANT,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4. INITIALIZE MODEL, OPTIMIZER, LOSS\n",
    "# ============================================================\n",
    "model = LSTMRegressor(\n",
    "    input_dim=len(feature_cols),\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    activation=\"ReLU\",\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ============================================================\n",
    "# 5. TRAINING LOOP (with early stopping + RMSE tracking)\n",
    "# ============================================================\n",
    "best_rmse = float(\"inf\")\n",
    "best_weights = None\n",
    "counter = 0\n",
    "\n",
    "train_rmse_history = []  # RMSE in original units\n",
    "val_rmse_history = []    # RMSE in original units\n",
    "\n",
    "print(f\"Starting training on {len(train_loader)} batches...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_b, y_b in train_loader:\n",
    "        X_b = X_b.to(device)\n",
    "        y_b = y_b.to(device)  # (batch, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_b)     # (batch, 1)\n",
    "        loss = criterion(pred, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss_scaled = train_loss / max(len(train_loader), 1)\n",
    "    train_rmse_scaled = np.sqrt(avg_train_loss_scaled)\n",
    "    # Convert scaled RMSE to original units via scaler_y\n",
    "    train_rmse_orig = train_rmse_scaled * scaler_y.scale_[0]\n",
    "    train_rmse_history.append(train_rmse_orig)\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader:\n",
    "            X_b = X_b.to(device)\n",
    "            pred = model(X_b).cpu().numpy()      # (batch, 1)\n",
    "            val_preds.append(pred)\n",
    "            val_trues.append(y_b.numpy())        # (batch, 1)\n",
    "\n",
    "    if val_preds:\n",
    "        vp_scaled = np.concatenate(val_preds, axis=0)      # (N, 1)\n",
    "        vt_scaled = np.concatenate(val_trues, axis=0)      # (N, 1)\n",
    "\n",
    "        vp_inv = scaler_y.inverse_transform(vp_scaled)     # (N, 1)\n",
    "        vt_inv = scaler_y.inverse_transform(vt_scaled)     # (N, 1)\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(vt_inv, vp_inv))\n",
    "        val_rmse_history.append(val_rmse)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d}: Train RMSE {train_rmse_orig:.4f} | Val RMSE {val_rmse:.4f}\")\n",
    "\n",
    "        # Early stopping based on Val RMSE\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= PATIENCE:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1:03d}: Train RMSE {train_rmse_orig:.4f} | Val RMSE N/A (no val sequences)\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. PLOT TRAIN vs VAL RMSE (ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_rmse_history, label=\"Train RMSE (Original Units)\")\n",
    "plt.plot(val_rmse_history, label=\"Val RMSE (Original Units)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(f\"RMSE (Yield: {CHOSEN_CROP})\")\n",
    "plt.title(\"Learning Curve: Train vs Validation Error\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Load best weights (if any)\n",
    "if best_weights is not None:\n",
    "    model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. FINAL EVALUATION (TRAIN / VAL / TEST, ORIGINAL UNITS)\n",
    "# ============================================================\n",
    "def evaluate_loader(model, loader, scaler_y, split_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model on a given loader:\n",
    "    - Inverse-transforms y and predictions.\n",
    "    - Returns RMSE, MAE, R2, preds, trues.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_scaled = []\n",
    "    trues_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in loader:\n",
    "            X_b = X_b.to(device)\n",
    "            pred = model(X_b).cpu().numpy()  # (batch, 1)\n",
    "            preds_scaled.append(pred)\n",
    "            trues_scaled.append(y_b.numpy()) # (batch, 1)\n",
    "\n",
    "    if not preds_scaled:\n",
    "        print(f\"[{split_name}] No sequences.\")\n",
    "        return np.nan, np.nan, np.nan, np.array([]), np.array([])\n",
    "\n",
    "    preds_scaled = np.concatenate(preds_scaled, axis=0)   # (N, 1)\n",
    "    trues_scaled = np.concatenate(trues_scaled, axis=0)   # (N, 1)\n",
    "\n",
    "    preds = scaler_y.inverse_transform(preds_scaled).ravel()\n",
    "    trues = scaler_y.inverse_transform(trues_scaled).ravel()\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    r2  = r2_score(trues, preds)\n",
    "\n",
    "    print(f\"[{split_name}] RMSE: {rmse:.2f} | MAE: {mae:.2f} | R²: {r2:.4f}\")\n",
    "    return rmse, mae, r2, preds, trues\n",
    "\n",
    "\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "train_rmse, train_mae, train_r2, _, _ = evaluate_loader(model, train_loader, scaler_y, split_name=\"Train\")\n",
    "val_rmse,   val_mae,   val_r2,   _, _ = evaluate_loader(model, val_loader,   scaler_y, split_name=\"Val\")\n",
    "test_rmse,  test_mae,  test_r2,  p_test, t_test = evaluate_loader(model, test_loader,  scaler_y, split_name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_build_loaders(\n",
    "    df: pd.DataFrame,\n",
    "    chosen_crop: str,\n",
    "    target_col: str,\n",
    "    seq_len: int = 5,\n",
    "    batch_size: int = 32,\n",
    "    clip_quant: float = 0.01,\n",
    "    verbose: bool = False,   # NEW: control printing\n",
    "):\n",
    "    \"\"\"Preprocess panel data + build LSTM sequences (clean Optuna-friendly version)\"\"\"\n",
    "\n",
    "    df_model = df.copy()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Drop NaN targets\n",
    "    # ----------------------------\n",
    "    df_model = df_model[~df_model[target_col].isna()].copy()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Target and Feature Selection\n",
    "    # ----------------------------\n",
    "    all_targets = [c for c in df_model.columns if c.startswith(\"Y_\")]\n",
    "    df_model = df_model.drop(columns=[c for c in all_targets if c != target_col])\n",
    "\n",
    "    avg_yield_cols = [c for c in df_model.columns if c.startswith(\"avg_yield_\")]\n",
    "    chosen_prefix = f\"avg_yield_{chosen_crop}_\"\n",
    "    keep_avg = [c for c in avg_yield_cols if c.startswith(chosen_prefix)]\n",
    "    drop_avg = list(set(avg_yield_cols) - set(keep_avg))\n",
    "    df_model = df_model.drop(columns=drop_avg)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Add time features\n",
    "    # ----------------------------\n",
    "    base_year = df_model[\"year\"].min()\n",
    "    df_model[\"year_index\"] = df_model[\"year\"] - base_year\n",
    "    max_index = max(df_model[\"year_index\"].max(), 1)\n",
    "    df_model[\"year_norm\"] = df_model[\"year_index\"] / max_index\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) Sort\n",
    "    # ----------------------------\n",
    "    df_model = df_model.sort_values([\"area\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5) Define feature columns\n",
    "    # ----------------------------\n",
    "    feature_cols = [\n",
    "        c for c in df_model.columns\n",
    "        if c not in [\"area\", \"year\", target_col] and not c.startswith(\"Y_\")\n",
    "    ]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6) ffill/bfill per area\n",
    "    # ----------------------------\n",
    "    df_model[feature_cols] = (\n",
    "        df_model.groupby(\"area\", group_keys=False)[feature_cols]\n",
    "        .apply(lambda g: g.ffill().bfill())\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7) Time-based masks\n",
    "    # ----------------------------\n",
    "    train_mask = df_model[\"year\"] < 2014\n",
    "    val_mask   = (df_model[\"year\"] >= 2014) & (df_model[\"year\"] <= 2018)\n",
    "    test_mask  = df_model[\"year\"] >= 2019\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8) NaN imputation\n",
    "    # ----------------------------\n",
    "    X_raw = df_model[feature_cols].values.astype(np.float32)\n",
    "    y_raw = df_model[[target_col]].values.astype(np.float32)\n",
    "\n",
    "    train_X = X_raw[train_mask]\n",
    "    train_mean = np.nanmean(train_X, axis=0)\n",
    "    nan_idx = np.where(np.isnan(X_raw))\n",
    "    X_raw[nan_idx] = np.take(train_mean, nan_idx[1])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9) Outlier clipping\n",
    "    # ----------------------------\n",
    "    if clip_quant is not None and 0 < clip_quant < 0.5:\n",
    "        train_vals = X_raw[train_mask]\n",
    "        q_low = np.quantile(train_vals, clip_quant, axis=0)\n",
    "        q_high = np.quantile(train_vals, 1 - clip_quant, axis=0)\n",
    "        for j in range(train_vals.shape[1]):\n",
    "            if q_low[j] < q_high[j]:\n",
    "                X_raw[:, j] = np.clip(X_raw[:, j], q_low[j], q_high[j])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 10) Scaling\n",
    "    # ----------------------------\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_scaled = X_raw.copy()\n",
    "    y_scaled = y_raw.copy()\n",
    "\n",
    "    X_scaled[train_mask] = scaler_X.fit_transform(X_raw[train_mask])\n",
    "    X_scaled[val_mask]   = scaler_X.transform(X_raw[val_mask])\n",
    "    X_scaled[test_mask]  = scaler_X.transform(X_raw[test_mask])\n",
    "\n",
    "    y_scaled[train_mask] = scaler_y.fit_transform(y_raw[train_mask])\n",
    "    y_scaled[val_mask]   = scaler_y.transform(y_raw[val_mask])\n",
    "    y_scaled[test_mask]  = scaler_y.transform(y_raw[test_mask])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 11) Rebuild scaled DF\n",
    "    # ----------------------------\n",
    "    df_scaled = df_model[[\"area\", \"year\"]].copy()\n",
    "    df_scaled[\"target_scaled\"] = y_scaled.reshape(-1)\n",
    "\n",
    "    X_df = pd.DataFrame(X_scaled, columns=feature_cols, index=df_model.index)\n",
    "    df_final = pd.concat([df_scaled, X_df], axis=1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 12) Sequence Building\n",
    "    # ----------------------------\n",
    "    def build_sequences(df_seq):\n",
    "        X_tr, y_tr = [], []\n",
    "        X_val_l, y_val_l = [], []\n",
    "        X_te, y_te = [], []\n",
    "\n",
    "        for _, g in df_seq.groupby(\"area\"):\n",
    "            g = g.sort_values(\"year\")\n",
    "            feats = g[feature_cols].values\n",
    "            targs = g[\"target_scaled\"].values\n",
    "            years = g[\"year\"].values\n",
    "\n",
    "            if len(g) < seq_len:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(g) - seq_len + 1):\n",
    "                X_seq = feats[i:i+seq_len]\n",
    "                y_t = targs[i + seq_len - 1]\n",
    "                yr = years[i + seq_len - 1]\n",
    "\n",
    "                if yr < 2014:\n",
    "                    X_tr.append(X_seq); y_tr.append(y_t)\n",
    "                elif yr <= 2018:\n",
    "                    X_val_l.append(X_seq); y_val_l.append(y_t)\n",
    "                else:\n",
    "                    X_te.append(X_seq); y_te.append(y_t)\n",
    "\n",
    "        def to_np(X, y):\n",
    "            return (\n",
    "                np.array(X, dtype=np.float32) if len(X) else np.empty((0, seq_len, len(feature_cols))),\n",
    "                np.array(y, dtype=np.float32)\n",
    "            )\n",
    "\n",
    "        return to_np(X_tr, y_tr), to_np(X_val_l, y_val_l), to_np(X_te, y_te)\n",
    "\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = build_sequences(df_final)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 13) Loaders\n",
    "    # ----------------------------\n",
    "    def make_loader(X, y, shuffle):\n",
    "        return DataLoader(\n",
    "            TensorDataset(torch.from_numpy(X), torch.from_numpy(y).unsqueeze(-1)),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        make_loader(X_train, y_train, True),\n",
    "        make_loader(X_val, y_val, False),\n",
    "        make_loader(X_test, y_test, False),\n",
    "        feature_cols,\n",
    "        scaler_y,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddf871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. OPTUNA TUNING FOR LSTM HYPERPARAMETERS\n",
    "# ============================================================\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function:\n",
    "    - Suggests LSTM + training hyperparameters\n",
    "    - Rebuilds dataloaders with suggested seq_len\n",
    "    - Trains on train set, evaluates on val set\n",
    "    - Returns best validation RMSE (original units)\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Hyperparameters to tune ----\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128, step=32)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"ReLU\", \"Tanh\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)\n",
    "    seq_len = trial.suggest_int(\"seq_len\", 3, 7)\n",
    "\n",
    "    # ---- Build loaders for this seq_len ----\n",
    "    train_loader_opt, val_loader_opt, test_loader_opt, feature_cols_opt, scaler_y_opt = preprocess_and_build_loaders(\n",
    "        df=df,\n",
    "        chosen_crop=CHOSEN_CROP,\n",
    "        target_col=TARGET_COL,\n",
    "        seq_len=seq_len,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        clip_quant=CLIP_QUANT,\n",
    "    )\n",
    "\n",
    "    # If val set is empty for this seq_len, return large loss\n",
    "    if len(val_loader_opt.dataset) == 0 or len(train_loader_opt.dataset) == 0:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    # ---- Build model for this trial ----\n",
    "    model_opt = LSTMRegressor(\n",
    "        input_dim=len(feature_cols_opt),\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        activation=activation,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer_opt = torch.optim.Adam(model_opt.parameters(), lr=lr)\n",
    "    criterion_opt = nn.MSELoss()\n",
    "\n",
    "    max_epochs = 80     # you can increase later\n",
    "    patience = 5\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # ---------- TRAIN ----------\n",
    "        model_opt.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_b, y_b in train_loader_opt:\n",
    "            X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "            optimizer_opt.zero_grad()\n",
    "            pred = model_opt(X_b)\n",
    "            loss = criterion_opt(pred, y_b)\n",
    "            loss.backward()\n",
    "            optimizer_opt.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # ---------- VALIDATION ----------\n",
    "        model_opt.eval()\n",
    "        val_preds_scaled, val_trues_scaled = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_b, y_b in val_loader_opt:\n",
    "                X_b = X_b.to(device)\n",
    "                pred = model_opt(X_b).cpu().numpy()\n",
    "                val_preds_scaled.append(pred)\n",
    "                val_trues_scaled.append(y_b.numpy())\n",
    "\n",
    "        if not val_preds_scaled:\n",
    "            # no val batches\n",
    "            return float(\"inf\")\n",
    "\n",
    "        vp_scaled = np.concatenate(val_preds_scaled, axis=0).reshape(-1, 1)\n",
    "        vt_scaled = np.concatenate(val_trues_scaled, axis=0).reshape(-1, 1)\n",
    "\n",
    "        # inverse transform to original units\n",
    "        vp_inv = scaler_y_opt.inverse_transform(vp_scaled)\n",
    "        vt_inv = scaler_y_opt.inverse_transform(vt_scaled)\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(vt_inv, vp_inv))\n",
    "\n",
    "        # early stopping inside a trial\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "        # report to Optuna (for pruning)\n",
    "        trial.report(best_val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return best_val_rmse\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. RUN OPTUNA STUDY\n",
    "# ============================================================\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"LSTM_optuna\",\n",
    ")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"=== OPTUNA BEST RESULT ===\")\n",
    "print(\"Best Val RMSE:\", study.best_value)\n",
    "print(\"Best Params:\", study.best_params)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\n",
    "# ============================================================\n",
    "best_params = study.best_params\n",
    "\n",
    "best_hidden_dim = best_params[\"hidden_dim\"]\n",
    "best_num_layers = best_params[\"num_layers\"]\n",
    "best_dropout = best_params[\"dropout\"]\n",
    "best_activation = best_params[\"activation\"]\n",
    "best_lr = best_params[\"lr\"]\n",
    "best_seq_len = best_params[\"seq_len\"]\n",
    "\n",
    "print(\"\\n=== Retraining final LSTM with best Optuna params ===\")\n",
    "print(f\"hidden_dim={best_hidden_dim}, num_layers={best_num_layers}, \"\n",
    "      f\"dropout={best_dropout}, activation={best_activation}, \"\n",
    "      f\"lr={best_lr}, seq_len={best_seq_len}\")\n",
    "\n",
    "# Rebuild loaders with best seq_len\n",
    "train_loader_opt, val_loader_opt, test_loader_opt, feature_cols_opt, scaler_y_opt = preprocess_and_build_loaders(\n",
    "    df=df,\n",
    "    chosen_crop=CHOSEN_CROP,\n",
    "    target_col=TARGET_COL,\n",
    "    seq_len=best_seq_len,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    clip_quant=CLIP_QUANT,\n",
    ")\n",
    "\n",
    "# Define final model\n",
    "final_model = LSTMRegressor(\n",
    "    input_dim=len(feature_cols_opt),\n",
    "    hidden_dim=best_hidden_dim,\n",
    "    num_layers=best_num_layers,\n",
    "    dropout=best_dropout,\n",
    "    activation=best_activation,\n",
    ").to(device)\n",
    "\n",
    "optimizer_final = torch.optim.Adam(final_model.parameters(), lr=best_lr)\n",
    "criterion_final = nn.MSELoss()\n",
    "\n",
    "best_rmse_final = float(\"inf\")\n",
    "best_weights_final = None\n",
    "counter_final = 0\n",
    "\n",
    "EPOCHS_FINAL = 100\n",
    "PATIENCE_FINAL = 15\n",
    "\n",
    "train_rmse_hist_final = []\n",
    "val_rmse_hist_final = []\n",
    "\n",
    "for epoch in range(EPOCHS_FINAL):\n",
    "    # ---------- TRAIN ----------\n",
    "    final_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_b, y_b in train_loader_opt:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        optimizer_final.zero_grad()\n",
    "        pred = final_model(X_b)\n",
    "        loss = criterion_final(pred, y_b)\n",
    "        loss.backward()\n",
    "        optimizer_final.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss_scaled = train_loss / max(len(train_loader_opt), 1)\n",
    "    train_rmse_scaled = np.sqrt(avg_train_loss_scaled)\n",
    "    train_rmse_orig = train_rmse_scaled * scaler_y_opt.scale_[0]\n",
    "    train_rmse_hist_final.append(train_rmse_orig)\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    final_model.eval()\n",
    "    val_preds_scaled, val_trues_scaled = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader_opt:\n",
    "            X_b = X_b.to(device)\n",
    "            pred = final_model(X_b).cpu().numpy()\n",
    "            val_preds_scaled.append(pred)\n",
    "            val_trues_scaled.append(y_b.numpy())\n",
    "\n",
    "    if val_preds_scaled:\n",
    "        vp_scaled = np.concatenate(val_preds_scaled, axis=0).reshape(-1, 1)\n",
    "        vt_scaled = np.concatenate(val_trues_scaled, axis=0).reshape(-1, 1)\n",
    "\n",
    "        vp_inv = scaler_y_opt.inverse_transform(vp_scaled)\n",
    "        vt_inv = scaler_y_opt.inverse_transform(vt_scaled)\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(vt_inv, vp_inv))\n",
    "        val_rmse_hist_final.append(val_rmse)\n",
    "\n",
    "        print(f\"[FINAL] Epoch {epoch+1:03d}: Train RMSE {train_rmse_orig:.4f} | Val RMSE {val_rmse:.4f}\")\n",
    "\n",
    "        if val_rmse < best_rmse_final:\n",
    "            best_rmse_final = val_rmse\n",
    "            best_weights_final = copy.deepcopy(final_model.state_dict())\n",
    "            counter_final = 0\n",
    "        else:\n",
    "            counter_final += 1\n",
    "            if counter_final >= PATIENCE_FINAL:\n",
    "                print(f\"[FINAL] Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"[FINAL] Epoch {epoch+1:03d}: Train RMSE {train_rmse_orig:.4f} | Val RMSE N/A\")\n",
    "\n",
    "print(\"Final training with best Optuna params completed.\")\n",
    "\n",
    "# Load best weights\n",
    "if best_weights_final is not None:\n",
    "    final_model.load_state_dict(best_weights_final)\n",
    "\n",
    "# Plot final learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_rmse_hist_final, label=\"Train RMSE (Optuna Best)\")\n",
    "plt.plot(val_rmse_hist_final, label=\"Val RMSE (Optuna Best)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(f\"RMSE (Yield: {CHOSEN_CROP})\")\n",
    "plt.title(\"Learning Curve: Train vs Validation (Optuna Best)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 11. EVALUATE FINAL MODEL (TRAIN / VAL / TEST)\n",
    "# ============================================================\n",
    "print(\"\\n=== Final Evaluation: Optuna-Tuned LSTM ===\")\n",
    "_ = evaluate_loader(final_model, train_loader_opt, scaler_y_opt, split_name=\"Train (Optuna)\")\n",
    "_ = evaluate_loader(final_model, val_loader_opt,   scaler_y_opt, split_name=\"Val (Optuna)\")\n",
    "_ = evaluate_loader(final_model, test_loader_opt,  scaler_y_opt, split_name=\"Test (Optuna)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Visualizations\n",
    "name = f\"{CHOSEN_CROP.capitalize()}_Yield_LSTM\"\n",
    "\n",
    "# 1. Optimization History (best value over trials)\n",
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(\n",
    "    title=f\"{name} – Optimization History (Val RMSE)\",\n",
    "    xaxis_title=\"Trial\",\n",
    "    yaxis_title=\"Best Val RMSE\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 2. Parallel Coordinate Plot (how hyperparams interact)\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.update_layout(\n",
    "    title=f\"{name} – Parallel Coordinate Plot\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 3. Slice Plot (effect of each hyperparameter individually)\n",
    "fig = plot_slice(study)\n",
    "fig.update_layout(\n",
    "    title=f\"{name} – Slice Plot\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 4. Hyperparameter Importance (which params matter most)\n",
    "try:\n",
    "    fig = plot_param_importances(study)\n",
    "    fig.update_layout(\n",
    "        title=f\"{name} – Hyperparameter Importance\",\n",
    "        width=900,\n",
    "        height=500,\n",
    "    )\n",
    "    fig.show()\n",
    "except (ValueError, RuntimeError) as e:\n",
    "    print(f\"Could not plot parameter importance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE INITIAL LSTM & OPTUNA-TUNED LSTM ON TEST SET\n",
    "# ============================================================\n",
    "\n",
    "# Initial LSTM (before Optuna)\n",
    "rmse_lstm_init, mae_lstm_init, r2_lstm_init, y_pred_lstm_init, y_true_lstm_init = evaluate_loader(\n",
    "    model,                # initial LSTM\n",
    "    test_loader,\n",
    "    scaler_y,\n",
    "    split_name=\"Test (Initial LSTM)\"\n",
    ")\n",
    "\n",
    "# Optuna-tuned LSTM (final_model)\n",
    "rmse_lstm_opt, mae_lstm_opt, r2_lstm_opt, y_pred_lstm_opt, y_true_lstm_opt = evaluate_loader(\n",
    "    final_model,          # tuned LSTM\n",
    "    test_loader_opt,\n",
    "    scaler_y_opt,\n",
    "    split_name=\"Test (Optuna LSTM)\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PERFORMANCE REPORT vs BASELINE\n",
    "# ============================================================\n",
    "imp_final_lstm = (rmse_baseline - rmse_lstm_opt) / rmse_baseline * 100\n",
    "\n",
    "print(\"\\n--- Final LSTM Performance Report (Test Set) ---\")\n",
    "print(f\"Baseline Model:   RMSE={rmse_baseline:.2f}, R2={r2_baseline:.4f}\")\n",
    "print(f\"Initial LSTM:     RMSE={rmse_lstm_init:.2f}, R2={r2_lstm_init:.4f}\")\n",
    "print(f\"Tuned LSTM (Opt): RMSE={rmse_lstm_opt:.2f}, R2={r2_lstm_opt:.4f} \"\n",
    "      f\"(RMSE Improved {imp_final_lstm:.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOTTING: BASELINE vs INITIAL LSTM vs TUNED LSTM\n",
    "# ============================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Flatten everything\n",
    "y_base_true  = np.ravel(y_test_clean)\n",
    "y_base_pred  = np.ravel(y_pred_clean)\n",
    "\n",
    "y_init_true  = np.ravel(y_true_lstm_init)\n",
    "y_init_pred  = np.ravel(y_pred_lstm_init)\n",
    "\n",
    "y_opt_true   = np.ravel(y_true_lstm_opt)\n",
    "y_opt_pred   = np.ravel(y_pred_lstm_opt)\n",
    "\n",
    "# Axis limits based on all predictions and truths\n",
    "all_preds = np.concatenate([y_base_pred, y_init_pred, y_opt_pred])\n",
    "all_true  = np.concatenate([y_base_true, y_init_true, y_opt_true])\n",
    "\n",
    "min_val = min(all_preds.min(), all_true.min())\n",
    "max_val = max(all_preds.max(), all_true.max())\n",
    "\n",
    "# 1. Baseline Plot\n",
    "axes[0].scatter(y_base_true, y_base_pred, alpha=0.4, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[0].set_title(f'Baseline Model\\nRMSE: {rmse_baseline:.2f} | R²: {r2_baseline:.3f}')\n",
    "axes[0].set_xlabel(\"Actual\")\n",
    "axes[0].set_ylabel(\"Predicted\")\n",
    "\n",
    "# 2. Initial LSTM Plot\n",
    "axes[1].scatter(y_init_true, y_init_pred, alpha=0.4, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[1].set_title(f'Initial LSTM Model\\nRMSE: {rmse_lstm_init:.2f} | R²: {r2_lstm_init:.3f}')\n",
    "axes[1].set_xlabel(\"Actual\")\n",
    "\n",
    "# 3. Tuned LSTM Plot\n",
    "axes[2].scatter(y_opt_true, y_opt_pred, alpha=0.4, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[2].set_title(f'Tuned LSTM Model (Optuna)\\nRMSE: {rmse_lstm_opt:.2f} | R²: {r2_lstm_opt:.3f}')\n",
    "axes[2].set_xlabel(\"Actual\")\n",
    "\n",
    "plt.suptitle(f'{CHOSEN_CROP.capitalize()} Yield: Baseline vs LSTM Performance (Actual vs Predicted)',\n",
    "             fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequence_years(df, seq_len):\n",
    "    \"\"\"\n",
    "    Reconstruct the target year for each sequence:\n",
    "    - group by area\n",
    "    - sort by year\n",
    "    - slide windows of length seq_len\n",
    "    - the target year for each sequence = last year in that window\n",
    "    Split into train/val/test by TRAIN_END_YEAR and VAL_END_YEAR.\n",
    "    \"\"\"\n",
    "    df_model = df.copy()\n",
    "    df_model = df_model[~df_model[TARGET_COL].isna()].copy()\n",
    "    df_model = df_model.sort_values([\"area\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "    seq_years_train = []\n",
    "    seq_years_val = []\n",
    "    seq_years_test = []\n",
    "\n",
    "    for _, g in df_model.groupby(\"area\"):\n",
    "        g = g.sort_values(\"year\")\n",
    "        years = g[\"year\"].values\n",
    "\n",
    "        if len(g) < seq_len:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(g) - seq_len + 1):\n",
    "            yr = years[i + seq_len - 1]\n",
    "            if yr < TRAIN_END_YEAR:\n",
    "                seq_years_train.append(yr)\n",
    "            elif yr < VAL_END_YEAR:\n",
    "                seq_years_val.append(yr)\n",
    "            else:\n",
    "                seq_years_test.append(yr)\n",
    "\n",
    "    return (\n",
    "        np.array(seq_years_train, dtype=int),\n",
    "        np.array(seq_years_val, dtype=int),\n",
    "        np.array(seq_years_test, dtype=int),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST TIMELINE: ACTUAL vs PREDICTED (Optuna LSTM)\n",
    "# ============================================================\n",
    "\n",
    "# Get years for all sequences with the tuned sequence length\n",
    "seq_years_train_opt, seq_years_val_opt, seq_years_test_opt = build_sequence_years(\n",
    "    df=df,\n",
    "    seq_len=best_seq_len,\n",
    ")\n",
    "\n",
    "# Align test years with test true/pred arrays from evaluate_loader\n",
    "assert len(seq_years_test_opt) == len(y_true_lstm_opt), \\\n",
    "    f\"Mismatch: {len(seq_years_test_opt)} years vs {len(y_true_lstm_opt)} test points\"\n",
    "\n",
    "# Build test-period dataframe\n",
    "df_test_timeline = pd.DataFrame({\n",
    "    \"Year\":      seq_years_test_opt,\n",
    "    \"Actual\":    y_true_lstm_opt,\n",
    "    \"Predicted\": y_pred_lstm_opt,\n",
    "})\n",
    "\n",
    "# Aggregate by year (e.g. 2019–2023)\n",
    "yearly_test = (\n",
    "    df_test_timeline\n",
    "    .groupby(\"Year\")[[\"Actual\", \"Predicted\"]]\n",
    "    .mean()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# ----------------- Plot -----------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(\n",
    "    yearly_test.index, yearly_test[\"Actual\"],\n",
    "    marker=\"o\", linewidth=2.5,\n",
    "    label=\"Actual Yield (Test)\", color=\"#4B0082\"  # purple-ish\n",
    ")\n",
    "ax.plot(\n",
    "    yearly_test.index, yearly_test[\"Predicted\"],\n",
    "    marker=\"x\", linestyle=\"--\", linewidth=2.0,\n",
    "    label=\"Predicted Yield (Optuna LSTM, Test)\", color=\"#FFA500\"\n",
    ")\n",
    "\n",
    "ax.set_title(f\"Test Period Timeline: Actual vs Predicted Yield – {CHOSEN_CROP.capitalize()}\",\n",
    "             fontsize=14)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Yield (hg/ha)\")\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(yearly_test.index)\n",
    "ax.legend(loc=\"upper left\", frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
