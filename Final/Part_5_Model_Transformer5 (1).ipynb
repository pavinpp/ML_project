{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84903a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optuna Visualization Tools\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501425e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_parquet('Parquet/XY_v2.parquet')\n",
    "\n",
    "# --- LIST AVAILABLE CROPS ---\n",
    "# Assumes targets start with 'Y_'\n",
    "target_columns = [col for col in df.columns if col.startswith('Y_')]\n",
    "available_crops = [col.replace('Y_', '') for col in target_columns]\n",
    "\n",
    "print(\"--- Available Crops found in Dataset ---\")\n",
    "print(available_crops)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. CHOOSE CROP DYNAMICALLY\n",
    "# ==========================================\n",
    "CHOSEN_CROP = \"rice\"\n",
    "TARGET_COL = f\"Y_{CHOSEN_CROP}\"\n",
    "print(\"Chosen crop:\", CHOSEN_CROP)\n",
    "print(\"Target column:\", TARGET_COL)\n",
    "\n",
    "# Safety check: make sure the target exists\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Target column {TARGET_COL} not found in df.columns\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. BUILD BASE df_model\n",
    "# ==========================================\n",
    "df_model = df.copy()\n",
    "\n",
    "# ==========================================\n",
    "# 3. DROP avg_yield_* FOR OTHER CROPS\n",
    "#    Keep only avg_yield_<CHOSEN_CROP>_*\n",
    "# ==========================================\n",
    "crop_prefix = f\"avg_yield_{CHOSEN_CROP}_\"\n",
    "\n",
    "cols_to_drop = [\n",
    "    c for c in df_model.columns\n",
    "    if c.startswith(\"avg_yield_\") and not c.startswith(crop_prefix)\n",
    "]\n",
    "\n",
    "df_model = df_model.drop(columns=cols_to_drop)\n",
    "print(f\"Dropped {len(cols_to_drop)} avg_yield_* columns not for {CHOSEN_CROP}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DEFINE META + TARGETS + FEATURES\n",
    "# ==========================================\n",
    "meta_cols = [\"year\", \"area\"]\n",
    "\n",
    "# All Y_* target columns (we will exclude them from features)\n",
    "target_cols = [c for c in df_model.columns if c.startswith(\"Y_\")]\n",
    "\n",
    "# Feature columns = everything except meta + all Y_* targets\n",
    "FEATURE_COLS = [\n",
    "    c for c in df_model.columns\n",
    "    if c not in meta_cols and c not in target_cols\n",
    "]\n",
    "\n",
    "print(\"Number of features:\", len(FEATURE_COLS))\n",
    "print(\"Example features:\", FEATURE_COLS[:22])\n",
    "\n",
    "# Reorder df_model for clarity\n",
    "df_model = df_model[meta_cols + FEATURE_COLS + [TARGET_COL]].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. TRAIN / VAL / TEST SPLIT + IMPUTATION + SCALING\n",
    "# ==========================================\n",
    "TRAIN_END_YEAR = 2014\n",
    "VAL_END_YEAR   = 2019\n",
    "\n",
    "# Masks for splits\n",
    "mask_train = df_model[\"year\"] < TRAIN_END_YEAR\n",
    "mask_val   = (df_model[\"year\"] >= TRAIN_END_YEAR) & (df_model[\"year\"] < VAL_END_YEAR)\n",
    "mask_test  = df_model[\"year\"] >= VAL_END_YEAR\n",
    "\n",
    "# Raw feature + target splits\n",
    "X_train_raw = df_model.loc[mask_train, FEATURE_COLS]\n",
    "X_val_raw   = df_model.loc[mask_val,   FEATURE_COLS]\n",
    "X_test_raw  = df_model.loc[mask_test,  FEATURE_COLS]\n",
    "\n",
    "y_train = df_model.loc[mask_train, TARGET_COL]\n",
    "y_val   = df_model.loc[mask_val,   TARGET_COL]\n",
    "y_test  = df_model.loc[mask_test,  TARGET_COL]\n",
    "\n",
    "print(\"Train years:\", df_model.loc[mask_train, \"year\"].min(), \"->\", df_model.loc[mask_train, \"year\"].max())\n",
    "print(\"Val years  :\", df_model.loc[mask_val,   \"year\"].min(), \"->\", df_model.loc[mask_val,   \"year\"].max())\n",
    "print(\"Test years :\", df_model.loc[mask_test,  \"year\"].min(), \"->\", df_model.loc[mask_test,  \"year\"].max())\n",
    "\n",
    "# Impute NaNs in features using TRAIN statistics\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=FEATURE_COLS)\n",
    "X_val_imputed   = pd.DataFrame(imputer.transform(X_val_raw),       columns=FEATURE_COLS)\n",
    "X_test_imputed  = pd.DataFrame(imputer.transform(X_test_raw),      columns=FEATURE_COLS)\n",
    "\n",
    "# Scale features using TRAIN statistics\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=FEATURE_COLS)\n",
    "X_val_scaled   = pd.DataFrame(scaler.transform(X_val_imputed),       columns=FEATURE_COLS)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test_imputed),      columns=FEATURE_COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39460d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. REBUILD train_df / val_df / test_df (CLEAN)\n",
    "# ==========================================\n",
    "train_df = pd.concat(\n",
    "    [\n",
    "        df_model.loc[mask_train, [\"year\", \"area\"]].reset_index(drop=True),\n",
    "        X_train_scaled.reset_index(drop=True),\n",
    "        y_train.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "val_df = pd.concat(\n",
    "    [\n",
    "        df_model.loc[mask_val, [\"year\", \"area\"]].reset_index(drop=True),\n",
    "        X_val_scaled.reset_index(drop=True),\n",
    "        y_val.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "test_df = pd.concat(\n",
    "    [\n",
    "        df_model.loc[mask_test, [\"year\", \"area\"]].reset_index(drop=True),\n",
    "        X_test_scaled.reset_index(drop=True),\n",
    "        y_test.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape  :\", val_df.shape)\n",
    "print(\"test_df shape :\", test_df.shape)\n",
    "print(\"Columns in train_df:\", train_df.columns.tolist())\n",
    "\n",
    "# Final cleaning helper\n",
    "def clean_df(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)   # convert inf to NaN\n",
    "    df = df.dropna(axis=0, how=\"any\")            # drop any row with NaN\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_df = clean_df(train_df)\n",
    "val_df   = clean_df(val_df)\n",
    "test_df  = clean_df(test_df)\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(\"train_df shape:\", train_df.shape)\n",
    "print(\"val_df shape  :\", val_df.shape)\n",
    "print(\"test_df shape :\", test_df.shape)\n",
    "print(\"NaNs in train_df:\", train_df.isna().sum().sum())\n",
    "print(\"NaNs in val_df  :\", val_df.isna().sum().sum())\n",
    "print(\"NaNs in test_df :\", test_df.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. CONCAT CLEANED DFS INTO full_df FOR SEQUENCE DATASET\n",
    "# ==========================================\n",
    "full_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "# Ensure columns are in the right order\n",
    "full_df = full_df[[\"year\", \"area\"] + FEATURE_COLS + [TARGET_COL]].copy()\n",
    "\n",
    "print(\"full_df shape:\", full_df.shape)\n",
    "\n",
    "# Sanity check: no NaNs\n",
    "assert not full_df.isna().any().any(), \"NaNs remain in full_df!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22390bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. TFT-STYLE SEQUENCE DATASET\n",
    "# ==========================================\n",
    "class RiceYieldTFTDatasetFull(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        seq_len,\n",
    "        feature_cols,\n",
    "        target_col=\"Y_rice\",\n",
    "        group_col=\"area\",\n",
    "        time_col=\"year\",\n",
    "        mode=\"train\",\n",
    "        train_end=2014,\n",
    "        val_end=2019,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        df: DataFrame with columns [time_col, group_col] + feature_cols + [target_col]\n",
    "        seq_len: input sequence length\n",
    "        mode: \"train\", \"val\", or \"test\" → determined by TARGET YEAR\n",
    "        \"\"\"\n",
    "        self.seq_len    = seq_len\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.group_col  = group_col\n",
    "        self.time_col   = time_col\n",
    "        self.mode       = mode\n",
    "        self.train_end  = train_end\n",
    "        self.val_end    = val_end\n",
    "\n",
    "        # Sort globally\n",
    "        self.df = df.sort_values([group_col, time_col]).reset_index(drop=True)\n",
    "\n",
    "        self.groups = {}\n",
    "        self.index  = []  # list of (group_key, start_idx)\n",
    "\n",
    "        for g, gdf in self.df.groupby(group_col):\n",
    "            gdf = gdf.sort_values(time_col).reset_index(drop=True)\n",
    "            self.groups[g] = gdf\n",
    "\n",
    "            years = gdf[time_col].values\n",
    "            if len(gdf) > seq_len:\n",
    "                for start in range(0, len(gdf) - seq_len):\n",
    "                    target_idx   = start + seq_len\n",
    "                    target_year  = years[target_idx]\n",
    "\n",
    "                    if mode == \"train\" and target_year < train_end:\n",
    "                        self.index.append((g, start))\n",
    "                    elif mode == \"val\" and (train_end <= target_year < val_end):\n",
    "                        self.index.append((g, start))\n",
    "                    elif mode == \"test\" and target_year >= val_end:\n",
    "                        self.index.append((g, start))\n",
    "\n",
    "        print(f\"[{mode}] Built {len(self.index)} windows for seq_len={seq_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g, start = self.index[idx]\n",
    "        gdf = self.groups[g]\n",
    "\n",
    "        # x: seq_len rows of features\n",
    "        x_window = gdf.iloc[start : start + self.seq_len][self.feature_cols].values\n",
    "\n",
    "        # y: target at next time step (one-step-ahead)\n",
    "        target_idx = start + self.seq_len\n",
    "        y_value = gdf.iloc[target_idx][self.target_col]\n",
    "\n",
    "        x_tensor = torch.tensor(x_window, dtype=torch.float32)      # (T, F)\n",
    "        y_tensor = torch.tensor([y_value], dtype=torch.float32)     # (1,) so batch → (B,1)\n",
    "        return x_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 5. CREATE DATASETS + DATALOADERS\n",
    "# ==========================================\n",
    "SEQ_LEN = 5  \n",
    "\n",
    "train_dataset = RiceYieldTFTDatasetFull(\n",
    "    full_df,\n",
    "    seq_len=SEQ_LEN,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    target_col=TARGET_COL,\n",
    "    mode=\"train\",\n",
    "    train_end=TRAIN_END_YEAR,\n",
    "    val_end=VAL_END_YEAR,\n",
    ")\n",
    "\n",
    "val_dataset = RiceYieldTFTDatasetFull(\n",
    "    full_df,\n",
    "    seq_len=SEQ_LEN,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    target_col=TARGET_COL,\n",
    "    mode=\"val\",\n",
    "    train_end=TRAIN_END_YEAR,\n",
    "    val_end=VAL_END_YEAR,\n",
    ")\n",
    "\n",
    "test_dataset = RiceYieldTFTDatasetFull(\n",
    "    full_df,\n",
    "    seq_len=SEQ_LEN,\n",
    "    feature_cols=FEATURE_COLS,\n",
    "    target_col=TARGET_COL,\n",
    "    mode=\"test\",\n",
    "    train_end=TRAIN_END_YEAR,\n",
    "    val_end=VAL_END_YEAR,\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_dim = len(FEATURE_COLS)\n",
    "print(\"Input dim:\", input_dim)\n",
    "print(\"Train samples:\", len(train_dataset),\n",
    "      \"Val samples:\",   len(val_dataset),\n",
    "      \"Test samples:\",  len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 6. TEMPORAL FUSION TRANSFORMER-LIKE MODEL\n",
    "# ==========================================\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "        )\n",
    "\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        returns: (batch, 1) → predicted Y at t+1\n",
    "        \"\"\"\n",
    "        # Project inputs\n",
    "        x = self.input_proj(x)                 # (B, T, H)\n",
    "\n",
    "        # Encoder LSTM\n",
    "        enc_out, (h, c) = self.encoder_lstm(x) # enc_out: (B, T, H)\n",
    "\n",
    "        # Self-attention over encoder outputs\n",
    "        attn_out, _ = self.attention(enc_out, enc_out, enc_out)  # (B, T, H)\n",
    "\n",
    "        # Residual + norm\n",
    "        x = self.norm1(enc_out + attn_out)\n",
    "\n",
    "        # Position-wise feed-forward\n",
    "        ffn_out = self.ffn(x)\n",
    "\n",
    "        # Another residual + norm\n",
    "        x = self.norm2(x + ffn_out)\n",
    "\n",
    "        # Decoder LSTM (reuse encoder hidden state)\n",
    "        dec_out, _ = self.decoder_lstm(x, (h, c))  # (B, T, H)\n",
    "\n",
    "        # Take last time step as representation\n",
    "        last_step = dec_out[:, -1, :]  # (B, H)\n",
    "\n",
    "        # Project to scalar\n",
    "        out = self.output_proj(last_step)  # (B, 1)\n",
    "        return out\n",
    "\n",
    "# Instantiate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = TemporalFusionTransformer(\n",
    "    input_size=input_dim,\n",
    "    hidden_size=64,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 7. METRICS + TRAINING LOOP\n",
    "# ==========================================\n",
    "def compute_rmse(y_true, y_pred):\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_pred = np.array(y_pred).reshape(-1)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def compute_mae(y_true, y_pred):\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_pred = np.array(y_pred).reshape(-1)\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    y_true = np.array(y_true).reshape(-1)\n",
    "    y_pred = np.array(y_pred).reshape(-1)\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "history = {\n",
    "    \"train_rmse\": [],\n",
    "    \"val_rmse\":   [],\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ------ TRAIN ------\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_y_true = []\n",
    "    train_y_pred = []\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)           # (B, T, F)\n",
    "        y_batch = y_batch.to(device)           # (B, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)               # (B, 1)\n",
    "\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (helps stability)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_y_true.append(y_batch.detach().cpu().numpy())\n",
    "        train_y_pred.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "    train_y_true = np.concatenate(train_y_true).reshape(-1)\n",
    "    train_y_pred = np.concatenate(train_y_pred).reshape(-1)\n",
    "    train_rmse = compute_rmse(train_y_true, train_y_pred)\n",
    "\n",
    "    # ------ VALIDATION ------\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_y_true = []\n",
    "    val_y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_y_true.append(y_batch.cpu().numpy())\n",
    "            val_y_pred.append(outputs.cpu().numpy())\n",
    "\n",
    "    if len(val_y_true) > 0:\n",
    "        val_y_true = np.concatenate(val_y_true).reshape(-1)\n",
    "        val_y_pred = np.concatenate(val_y_pred).reshape(-1)\n",
    "        val_rmse = compute_rmse(val_y_true, val_y_pred)\n",
    "        val_loss = np.mean(val_losses)\n",
    "    else:\n",
    "        val_rmse = np.nan\n",
    "        val_loss = np.nan\n",
    "\n",
    "    history[\"train_rmse\"].append(train_rmse)\n",
    "    history[\"val_rmse\"].append(val_rmse)\n",
    "\n",
    "    if not np.isnan(val_loss):\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs-1} | \"\n",
    "            f\"Train RMSE: {train_rmse:.2f} | \"\n",
    "            f\"Val RMSE: {val_rmse:.2f}\"\n",
    "        )\n",
    "# ==========================================\n",
    "# 8. PLOT TRAIN vs VALIDATION RMSE\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, history[\"train_rmse\"], label=\"Train Loss (RMSE)\", linewidth=2)\n",
    "plt.plot(epochs, history[\"val_rmse\"], label=\"Val Loss (RMSE)\", linewidth=2)\n",
    "\n",
    "plt.title(\"Transformer Model – Train vs Validation Loss\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss (RMSE)\", fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c641a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8. EVALUATE ON TEST SET\n",
    "# ==========================================\n",
    "model.eval()\n",
    "test_y_true = []\n",
    "test_y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)          # (B, 1)\n",
    "\n",
    "        test_y_true.append(y_batch.cpu().numpy())\n",
    "        test_y_pred.append(outputs.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "test_y_true = np.concatenate(test_y_true).reshape(-1)\n",
    "test_y_pred = np.concatenate(test_y_pred).reshape(-1)\n",
    "\n",
    "test_rmse = compute_rmse(test_y_true, test_y_pred)\n",
    "test_mae  = compute_mae(test_y_true, test_y_pred)\n",
    "test_r2   = compute_r2(test_y_true, test_y_pred)\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "print(f\"Test MAE : {test_mae:.2f}\")\n",
    "print(f\"Test R²  : {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63864b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(test_y_true, test_y_pred, alpha=0.4)\n",
    "min_val = min(test_y_true.min(), test_y_pred.min())\n",
    "max_val = max(test_y_true.max(), test_y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], \"r--\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Actual Yield\")\n",
    "plt.ylabel(\"Predicted Yield\")\n",
    "plt.title(\"Transformer – Test Set: Actual vs Predicted\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf94cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 8. OPTUNA TUNING FOR RMSE\n",
    "# ==========================================\n",
    "import optuna\n",
    "import math\n",
    "import gc\n",
    "\n",
    "def create_dataloaders(seq_len, batch_size=16):\n",
    "    \"\"\"Rebuild datasets & dataloaders for a given seq_len.\"\"\"\n",
    "    train_dataset = RiceYieldTFTDatasetFull(\n",
    "        full_df,\n",
    "        seq_len=seq_len,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        target_col=TARGET_COL,\n",
    "        mode=\"train\",\n",
    "        train_end=TRAIN_END_YEAR,\n",
    "        val_end=VAL_END_YEAR,\n",
    "    )\n",
    "\n",
    "    val_dataset = RiceYieldTFTDatasetFull(\n",
    "        full_df,\n",
    "        seq_len=seq_len,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        target_col=TARGET_COL,\n",
    "        mode=\"val\",\n",
    "        train_end=TRAIN_END_YEAR,\n",
    "        val_end=VAL_END_YEAR,\n",
    "    )\n",
    "\n",
    "    test_dataset = RiceYieldTFTDatasetFull(\n",
    "        full_df,\n",
    "        seq_len=seq_len,\n",
    "        feature_cols=FEATURE_COLS,\n",
    "        target_col=TARGET_COL,\n",
    "        mode=\"test\",\n",
    "        train_end=TRAIN_END_YEAR,\n",
    "        val_end=VAL_END_YEAR,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def build_model_from_trial(trial, input_dim):\n",
    "    \"\"\"Sample hyperparameters and build a TemporalFusionTransformer.\"\"\"\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128])\n",
    "    # num_heads must divide hidden_size; choose safe combos\n",
    "    possible_heads = [h for h in [2, 4, 8] if hidden_size % h == 0]\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", possible_heads)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.3)\n",
    "\n",
    "    model = TemporalFusionTransformer(\n",
    "        input_size=input_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # --------- Hyperparameters to tune ----------\n",
    "    seq_len = trial.suggest_categorical(\"seq_len\", [3, 5, 7])\n",
    "    lr      = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size   = trial.suggest_categorical(\"batch_size\", [8,16,32])\n",
    "\n",
    "    # Rebuild loaders for this seq_len / batch_size\n",
    "    train_loader, val_loader, _ = create_dataloaders(seq_len=seq_len, batch_size=batch_size)\n",
    "\n",
    "    # Build model\n",
    "    model = build_model_from_trial(trial, input_dim=input_dim)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Slightly fewer epochs for tuning\n",
    "    max_epochs = 60\n",
    "    best_val_rmse = math.inf\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        train_y_true = []\n",
    "        train_y_pred = []\n",
    "\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_y_true.append(y_batch.detach().cpu().numpy())\n",
    "            train_y_pred.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "        # ---------- VALIDATION ----------\n",
    "        model.eval()\n",
    "        val_y_true = []\n",
    "        val_y_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(x_batch)\n",
    "                val_y_true.append(y_batch.cpu().numpy())\n",
    "                val_y_pred.append(outputs.cpu().numpy())\n",
    "\n",
    "        if len(val_y_true) == 0:\n",
    "            # No validation windows (shouldn't happen, but just in case)\n",
    "            return math.inf\n",
    "\n",
    "        val_y_true = np.concatenate(val_y_true).reshape(-1)\n",
    "        val_y_pred = np.concatenate(val_y_pred).reshape(-1)\n",
    "        val_rmse = compute_rmse(val_y_true, val_y_pred)\n",
    "\n",
    "        # Track best RMSE for this trial\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "\n",
    "        # Report to Optuna & allow pruning\n",
    "        trial.report(val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return best_val_rmse\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 9. RUN OPTUNA STUDY\n",
    "# ==========================================\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20, timeout=None)  # adjust n_trials as you like\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(\"  Value (best Val RMSE):\", best_trial.value)\n",
    "print(\"  Params:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
