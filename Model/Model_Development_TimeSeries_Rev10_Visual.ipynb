{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Yield Prediction – Final Model Pipeline (Rev9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from scipy.signal import detrend\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"All libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODELS = {\n",
    "    'LR': True,   # Baseline Linear Regression\n",
    "    'RF': True,   # Random Forest\n",
    "    'XGB': True,  # XGBoost\n",
    "    'LSTM': True, # LSTM\n",
    "    'CNN': True   # CNN\n",
    "}\n",
    "\n",
    "RUN_OPTUNA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Preprocess Data (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Data Preprocessing\n",
    "try:\n",
    "    df = pd.read_csv(\"cleaned_crop_data.csv\")\n",
    "    print(f\"Loaded initial data: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Ensure 'cleaned_crop_data.csv' is present. Run the EDA notebook first.\")\n",
    "\n",
    "TARGET = 'hg/ha_yield'\n",
    "TIME_COL = 'Year'\n",
    "CAT_COLS = ['Area', 'Item']\n",
    "NUMERIC_COLS = ['average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'fertilizer_kg/ha', 'solar_radiation_MJ/m2-day']\n",
    "TARGET_DET = 'yield_detrended'\n",
    "\n",
    "# 1. Split data chronologically\n",
    "TRAIN_END = 2007\n",
    "VAL_END = 2010\n",
    "train_df_orig = df[df[TIME_COL] <= TRAIN_END].copy()\n",
    "val_df_orig = df[(df[TIME_COL] > TRAIN_END) & (df[TIME_COL] <= VAL_END)].copy()\n",
    "test_df_orig = df[df[TIME_COL] > VAL_END].copy()\n",
    "print(f\"1. Initial data split: Train: {train_df_orig.shape}, Val: {val_df_orig.shape}, Test: {test_df_orig.shape}\")\n",
    "\n",
    "# 2. Fit encoders ON TRAINING DATA ONLY\n",
    "le_area = LabelEncoder().fit(train_df_orig['Area'])\n",
    "le_item = LabelEncoder().fit(train_df_orig['Item'])\n",
    "for d in [train_df_orig, val_df_orig, test_df_orig]:\n",
    "    d['Area_Encoded'] = d['Area'].apply(lambda x: le_area.transform([x])[0] if x in le_area.classes_ else -1)\n",
    "    d['Item_Encoded'] = d['Item'].apply(lambda x: le_item.transform([x])[0] if x in le_item.classes_ else -1)\n",
    "print(\"2. Encoders fitted on train set and applied to all sets.\")\n",
    "\n",
    "# 3. Fit trend models ON TRAINING DATA ONLY\n",
    "print(\"3. Fitting trend models on training data...\")\n",
    "trend_models = {}\n",
    "for group, group_df in train_df_orig.groupby(CAT_COLS):\n",
    "    trend_model = LinearRegression()\n",
    "    trend_model.fit(group_df[[TIME_COL]], group_df[TARGET])\n",
    "    trend_models[group] = trend_model\n",
    "\n",
    "global_trend_model = LinearRegression().fit(train_df_orig[[TIME_COL]], train_df_orig[TARGET])\n",
    "print(f\"   Fitted {len(trend_models)} group-specific trend models and 1 global model.\")\n",
    "\n",
    "# 4. Apply detrending to all datasets\n",
    "for df_set in [train_df_orig, val_df_orig, test_df_orig]:\n",
    "    df_set['yield_trend'] = 0.0\n",
    "    for group, group_df in df_set.groupby(CAT_COLS):\n",
    "        model = trend_models.get(group, global_trend_model)\n",
    "        trend_prediction = model.predict(group_df[[TIME_COL]])\n",
    "        df_set.loc[group_df.index, 'yield_trend'] = trend_prediction\n",
    "    df_set['yield_detrended'] = df_set[TARGET] - df_set['yield_trend']\n",
    "print(\"   Detrending applied to all datasets.\")\n",
    "\n",
    "# 5. Create lags and finalize split for ML models\n",
    "full_df_ml = pd.concat([train_df_orig, val_df_orig, test_df_orig]).sort_values(CAT_COLS + [TIME_COL])\n",
    "lag_cols = ['yield_detrended'] + NUMERIC_COLS\n",
    "for col in lag_cols:\n",
    "    for lag in [1, 2]:\n",
    "        full_df_ml[f'{col}_lag{lag}'] = full_df_ml.groupby(CAT_COLS)[col].shift(lag)\n",
    "\n",
    "df_ml = full_df_ml.dropna().copy()\n",
    "train_df = df_ml[df_ml[TIME_COL] <= TRAIN_END].copy()\n",
    "val_df = df_ml[(df_ml[TIME_COL] > TRAIN_END) & (df_ml[TIME_COL] <= VAL_END)].copy()\n",
    "test_df = df_ml[df_ml[TIME_COL] > VAL_END].copy()\n",
    "print(f\"4. Lags created for ML models: Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# 6. Fit scalers for ML models ON TRAINING DATA ONLY\n",
    "lagged_cols = [c for c in df_ml.columns if '_lag' in c]\n",
    "ml_features = NUMERIC_COLS + lagged_cols + ['Area_Encoded', 'Item_Encoded']\n",
    "scale_cols = NUMERIC_COLS + lagged_cols\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "train_df[scale_cols] = x_scaler.fit_transform(train_df[scale_cols])\n",
    "val_df[scale_cols] = x_scaler.transform(val_df[scale_cols])\n",
    "test_df[scale_cols] = x_scaler.transform(test_df[scale_cols])\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "train_df[TARGET_DET] = y_scaler.fit_transform(train_df[[TARGET_DET]])\n",
    "val_df[TARGET_DET] = y_scaler.transform(val_df[[TARGET_DET]])\n",
    "test_df[TARGET_DET] = y_scaler.transform(test_df[[TARGET_DET]])\n",
    "print(\"5. X and y scalers for ML models fitted and applied.\")\n",
    "\n",
    "# 7. Save transformers\n",
    "joblib.dump(x_scaler, 'scaler.joblib')\n",
    "joblib.dump(y_scaler, 'y_scaler.joblib')\n",
    "joblib.dump(le_area, 'le_area.joblib')\n",
    "joblib.dump(le_item, 'le_item.joblib')\n",
    "joblib.dump(trend_models, 'trend_models.joblib')\n",
    "joblib.dump(global_trend_model, 'global_trend_model.joblib')\n",
    "print(\"6. All transformers saved to disk.\")\n",
    "\n",
    "N_AREAS = len(le_area.classes_)\n",
    "N_ITEMS = len(le_item.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Inputs (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Inputs\n",
    "X_train_ml = train_df[ml_features]\n",
    "y_train_ml = train_df[TARGET_DET]\n",
    "X_val_ml = val_df[ml_features]\n",
    "y_val_ml = val_df[TARGET_DET]\n",
    "X_test_ml = test_df[ml_features]\n",
    "y_test_ml = test_df[TARGET_DET]\n",
    "print(\"ML inputs prepared.\")\n",
    "\n",
    "# DL Inputs\n",
    "LOOKBACK = 5\n",
    "DL_FEATS = NUMERIC_COLS + ['Area_Encoded', 'Item_Encoded']\n",
    "\n",
    "# Correctly scale DL features\n",
    "scaler_dl_x = StandardScaler()\n",
    "train_df_orig[NUMERIC_COLS] = scaler_dl_x.fit_transform(train_df_orig[NUMERIC_COLS])\n",
    "val_df_orig[NUMERIC_COLS] = scaler_dl_x.transform(val_df_orig[NUMERIC_COLS])\n",
    "test_df_orig[NUMERIC_COLS] = scaler_dl_x.transform(test_df_orig[NUMERIC_COLS])\n",
    "\n",
    "# Use the already fitted y_scaler for the target\n",
    "train_df_orig[TARGET_DET] = y_scaler.transform(train_df_orig[[TARGET_DET]])\n",
    "val_df_orig[TARGET_DET] = y_scaler.transform(val_df_orig[[TARGET_DET]])\n",
    "test_df_orig[TARGET_DET] = y_scaler.transform(test_df_orig[[TARGET_DET]])\n",
    "print(\"DL features and target scaled.\")\n",
    "\n",
    "def create_sequences(data, lookback, feats, target):\n",
    "    X, y = [], []\n",
    "    # Keep track of original index\n",
    "    y_indices = []\n",
    "    for _, group in data.groupby(CAT_COLS):\n",
    "        if len(group) < lookback:\n",
    "            continue\n",
    "        gf = group[feats].values\n",
    "        gt = group[target].values\n",
    "        indices = group.index\n",
    "        for i in range(len(group) - lookback + 1):\n",
    "            X.append(gf[i:i+lookback])\n",
    "            y.append(gt[i+lookback-1])\n",
    "            y_indices.append(indices[i+lookback-1])\n",
    "    return np.array(X), np.array(y), y_indices\n",
    "\n",
    "X_train_seq, y_train_seq, _ = create_sequences(train_df_orig, LOOKBACK, DL_FEATS, TARGET_DET)\n",
    "X_val_seq, y_val_seq, _ = create_sequences(val_df_orig, LOOKBACK, DL_FEATS, TARGET_DET)\n",
    "X_test_seq, y_test_seq, y_test_indices = create_sequences(test_df_orig, LOOKBACK, DL_FEATS, TARGET_DET)\n",
    "print(\"DL sequences created.\")\n",
    "\n",
    "# Create a reference dataframe for test set evaluation\n",
    "test_df_dl_seq_ref = test_df_orig.loc[y_test_indices]\n",
    "\n",
    "def split_dl(X):\n",
    "    # The number of numeric features is len(NUMERIC_COLS)\n",
    "    numeric_feature_count = len(NUMERIC_COLS)\n",
    "    return [\n",
    "        torch.tensor(X[..., :numeric_feature_count], dtype=torch.float32),\n",
    "        torch.tensor(X[..., numeric_feature_count], dtype=torch.long),\n",
    "        torch.tensor(X[..., numeric_feature_count+1], dtype=torch.long)\n",
    "    ]\n",
    "\n",
    "X_train_dl = split_dl(X_train_seq)\n",
    "X_val_dl = split_dl(X_val_seq)\n",
    "X_test_dl = split_dl(X_test_seq)\n",
    "\n",
    "y_train_t = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_val_t = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_t = torch.tensor(y_test_seq, dtype=torch.float32).unsqueeze(1)\n",
    "print(\"DL tensors created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(((y_true - y_pred) / (y_true + 1e-8)) ** 2)) * 100\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optuna Objectives (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(trial):\n",
    "    # No hyperparameters to tune for Linear Regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_ml, y_train_ml)\n",
    "    preds = model.predict(X_val_ml)\n",
    "    return np.sqrt(mean_squared_error(y_val_ml, preds)) # Return RMSE\n",
    "\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0)\n",
    "    }\n",
    "    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params)\n",
    "    model.fit(X_train_ml, y_train_ml)\n",
    "    preds = model.predict(X_val_ml)\n",
    "    return np.sqrt(mean_squared_error(y_val_ml, preds)) # Return RMSE\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000), \n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'early_stopping_rounds': 50, # Increase patience for lower LR\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        random_state=42,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_ml, y_train_ml,\n",
    "        eval_set=[(X_val_ml, y_val_ml)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_val_ml)\n",
    "    return np.sqrt(mean_squared_error(y_val_ml, preds)) # Return RMSE\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_areas, n_items, lstm_units, dense_units, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_area = nn.Embedding(n_areas, 10)\n",
    "        self.embed_item = nn.Embedding(n_items, 5)\n",
    "        self.lstm = nn.LSTM(len(NUMERIC_COLS) + 10 + 5, lstm_units, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(lstm_units, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "    def forward(self, num, area, item):\n",
    "        e_area = self.embed_area(area)\n",
    "        e_item = self.embed_item(item)\n",
    "        x = torch.cat([num, e_area, e_item], dim=-1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.drop(out[:, -1])\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        return self.fc2(out)\n",
    "\n",
    "def train_dl(model, opt, loss_fn, train_loader, val_loader, target_scaler, epochs=100, patience=10, is_final=False):\n",
    "    scheduler = ReduceLROnPlateau(opt, 'min', patience=5, factor=0.5)\n",
    "    best_val_rmse = float('inf') # Optimize for RMSE\n",
    "    wait = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x1, x2, x3, y in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(x1, x2, x3)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs = [x.to(next(model.parameters()).device) for x in val_loader.dataset.tensors[:3]]\n",
    "            val_y = val_loader.dataset.tensors[3]\n",
    "            val_pred = model(*val_inputs)\n",
    "            val_mse = loss_fn(val_pred, val_y).item()\n",
    "            val_rmse = np.sqrt(val_mse) # Calculate RMSE\n",
    "            val_losses.append(val_mse)\n",
    "\n",
    "        scheduler.step(val_rmse) # Step based on validation RMSE\n",
    "\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            wait = 0\n",
    "            if is_final:\n",
    "                 torch.save(model.state_dict(), f'model_{model.__class__.__name__}.pth')\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    return train_losses, val_losses, best_val_rmse\n",
    "\n",
    "def objective_lstm(trial):\n",
    "    params = {\n",
    "        'lstm_units': trial.suggest_categorical('lstm_units', [64, 128]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [32, 64]),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    }\n",
    "    lr = params.pop('lr')\n",
    "    weight_decay = params.pop('weight_decay')\n",
    "    model = LSTMModel(N_AREAS, N_ITEMS, **params)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_ds = TensorDataset(*X_train_dl, y_train_t)\n",
    "    val_ds = TensorDataset(*X_val_dl, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    _, _, best_val_rmse = train_dl(model, opt, nn.MSELoss(), train_loader, val_loader, y_scaler)\n",
    "    return best_val_rmse\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_areas, n_items, filters, kernel, dense_units): \n",
    "        super().__init__()\n",
    "        self.embed_area = nn.Embedding(n_areas, 10)\n",
    "        self.embed_item = nn.Embedding(n_items, 5)\n",
    "        self.conv = nn.Conv1d(len(NUMERIC_COLS) + 10 + 5, filters, kernel)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(filters, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "    def forward(self, num, area, item):\n",
    "        e_area = self.embed_area(area)\n",
    "        e_item = self.embed_item(item)\n",
    "        x = torch.cat([num, e_area, e_item], dim=-1).transpose(1, 2)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def objective_cnn(trial):\n",
    "    params = {\n",
    "        'filters': trial.suggest_categorical('filters', [64, 128]),\n",
    "        'kernel': trial.suggest_categorical('kernel', [2, 3]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [32, 64]),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    }\n",
    "    lr = params.pop('lr')\n",
    "    weight_decay = params.pop('weight_decay')\n",
    "    model = CNNModel(N_AREAS, N_ITEMS, **params)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_ds = TensorDataset(*X_train_dl, y_train_t)\n",
    "    val_ds = TensorDataset(*X_val_dl, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    _, _, best_val_rmse = train_dl(model, opt, nn.MSELoss(), train_loader, val_loader, y_scaler)\n",
    "    return best_val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('best_params_optuna.joblib'):\n",
    "    best_params = joblib.load('best_params_optuna.joblib')\n",
    "else:\n",
    "    best_params = {}\n",
    "\n",
    "if RUN_OPTUNA:\n",
    "    studies = {}\n",
    "    objectives = {\n",
    "        'LR': objective_lr,\n",
    "        'RF': objective_rf,\n",
    "        'XGB': objective_xgb,\n",
    "        'LSTM': objective_lstm,\n",
    "        'CNN': objective_cnn\n",
    "    }\n",
    "    for name, run in RUN_MODELS.items():\n",
    "        if run:\n",
    "            print(f'--- Tuning {name} ---')\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            n_trials = 50 if name in ['RF', 'XGB'] else 30\n",
    "            if name == 'LR':\n",
    "                n_trials = 1\n",
    "            study.optimize(objectives[name], n_trials=n_trials, show_progress_bar=True)\n",
    "            best_params[name] = study.best_params\n",
    "            studies[name] = study\n",
    "            joblib.dump(best_params, 'best_params_optuna.joblib') # Save after each study\n",
    "            print(f'Best params for {name}: {study.best_params}')\n",
    "else:\n",
    "    print('Skipping Optuna tuning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Visualize Optuna Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "\n",
    "if RUN_OPTUNA and 'studies' in locals():\n",
    "    for name, study in studies.items():\n",
    "        if name == 'LR' or not study.trials:\n",
    "            continue\n",
    "        print(f'--- Visualizing Optuna results for {name} ---')\n",
    "        \n",
    "        # Optimization History\n",
    "        fig = plot_optimization_history(study)\n",
    "        fig.update_layout(title=f'{name} Optimization History')\n",
    "        fig.write_image(f'optuna_{name}_history.png')\n",
    "        fig.show()\n",
    "\n",
    "        # Parallel Coordinate\n",
    "        fig = plot_parallel_coordinate(study)\n",
    "        fig.update_layout(title=f'{name} Parallel Coordinate')\n",
    "        fig.write_image(f'optuna_{name}_parallel_coordinate.png')\n",
    "        fig.show()\n",
    "\n",
    "        # Slice Plot\n",
    "        fig = plot_slice(study)\n",
    "        fig.update_layout(title=f'{name} Slice Plot')\n",
    "        fig.write_image(f'optuna_{name}_slice.png')\n",
    "        fig.show()\n",
    "\n",
    "        # Parameter Importance\n",
    "        try:\n",
    "            fig = plot_param_importances(study)\n",
    "            fig.update_layout(title=f'{name} Parameter Importance')\n",
    "            fig.write_image(f'optuna_{name}_param_importance.png')\n",
    "            fig.show()\n",
    "        except (ValueError, RuntimeError) as e:\n",
    "            print(f'Could not plot parameter importance for {name}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Training (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train+val for final ML model training\n",
    "X_train_full_ml = pd.concat([X_train_ml, X_val_ml])\n",
    "y_train_full_ml = pd.concat([y_train_ml, y_val_ml])\n",
    "\n",
    "models = {}\n",
    "print(\"--- Final Model Training ---\")\n",
    "\n",
    "if RUN_MODELS['LR']:\n",
    "    print(\"Training Linear Regression...\")\n",
    "    model_lr = LinearRegression()\n",
    "    model_lr.fit(X_train_full_ml, y_train_full_ml)\n",
    "    models['LR'] = model_lr\n",
    "    joblib.dump(model_lr, 'model_lr.joblib')\n",
    "\n",
    "if RUN_MODELS['RF']:\n",
    "    print(\"Training Random Forest...\")\n",
    "    # Use best params from Optuna, or default if not run\n",
    "    rf_params = best_params.get('RF', {'n_estimators': 100, 'max_depth': 10})\n",
    "    model_rf = RandomForestRegressor(random_state=42, n_jobs=-1, **rf_params)\n",
    "    model_rf.fit(X_train_full_ml, y_train_full_ml)\n",
    "    models['RF'] = model_rf\n",
    "    joblib.dump(model_rf, 'model_rf.joblib')\n",
    "\n",
    "if RUN_MODELS['XGB']:\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb_params = best_params.get('XGB', {'n_estimators': 200, 'learning_rate': 0.05})\n",
    "    model_xgb = xgb.XGBRegressor(random_state=42, **xgb_params)\n",
    "    model_xgb.fit(X_train_full_ml, y_train_full_ml)\n",
    "    models['XGB'] = model_xgb\n",
    "    joblib.dump(model_xgb, 'model_xgb.joblib')\n",
    "\n",
    "# Combine train+val for final DL model training\n",
    "X_train_full_seq = np.concatenate([X_train_seq, X_val_seq])\n",
    "y_train_full_seq = np.concatenate([y_train_seq, y_val_seq])\n",
    "X_train_full_dl = split_dl(X_train_full_seq)\n",
    "y_train_full_t = torch.tensor(y_train_full_seq, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_full_ds = TensorDataset(*[x.to(device) for x in X_train_full_dl], y_train_full_t.to(device))\n",
    "test_ds = TensorDataset(*[x.to(device) for x in X_test_dl], y_test_t.to(device))\n",
    "train_loader = DataLoader(train_full_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "if RUN_MODELS['LSTM']:\n",
    "    print(\"Training LSTM...\")\n",
    "    lstm_params = best_params.get('LSTM', {'lstm_units': 64, 'dense_units': 32, 'dropout': 0.2})\n",
    "    lr = lstm_params.pop('lr', 0.001)\n",
    "    weight_decay = lstm_params.pop('weight_decay', 1e-4)\n",
    "    model_lstm = LSTMModel(N_AREAS, N_ITEMS, **lstm_params).to(device)\n",
    "    opt_lstm = optim.Adam(model_lstm.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_losses_lstm, val_losses_lstm, _ = train_dl(model_lstm, opt_lstm, nn.MSELoss(), train_loader, test_loader, y_scaler, epochs=150, patience=15, is_final=True)\n",
    "    models['LSTM'] = model_lstm\n",
    "\n",
    "if RUN_MODELS['CNN']:\n",
    "    print(\"Training CNN...\")\n",
    "    cnn_params = best_params.get('CNN', {'filters': 64, 'kernel': 2, 'dense_units': 32})\n",
    "    lr = cnn_params.pop('lr', 0.001)\n",
    "    weight_decay = cnn_params.pop('weight_decay', 1e-4)\n",
    "    model_cnn = CNNModel(N_AREAS, N_ITEMS, **cnn_params).to(device)\n",
    "    opt_cnn = optim.Adam(model_cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_losses_cnn, val_losses_cnn, _ = train_dl(model_cnn, opt_cnn, nn.MSELoss(), train_loader, test_loader, y_scaler, epochs=150, patience=15, is_final=True)\n",
    "    models['CNN'] = model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot DL Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['LSTM'] and RUN_MODELS['CNN']:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    ax1.plot(train_losses_lstm, label='Train Loss')\n",
    "    ax1.plot(val_losses_lstm, label='Validation (Test) Loss')\n",
    "    ax1.set_title('LSTM Model Loss', fontsize=16)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "    ax1.legend()\n",
    "    ax2.plot(train_losses_cnn, label='Train Loss')\n",
    "    ax2.plot(val_losses_cnn, label='Validation (Test) Loss')\n",
    "    ax2.set_title('CNN Model Loss', fontsize=16)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Mean Squared Error')\n",
    "    ax2.legend()\n",
    "    plt.suptitle('Deep Learning Training Curves', fontsize=20)\n",
    "    plt.savefig(\"loss_curves.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation (Corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_yield(y_pred_scaled, df_ref, y_scaler_obj):\n",
    "    \"\"\"Inverse transforms a prediction to the original yield scale.\"\"\"\n",
    "    if y_pred_scaled.ndim == 1:\n",
    "        y_pred_scaled = y_pred_scaled.reshape(-1, 1)\n",
    "    \n",
    "    y_pred_detrended = y_scaler_obj.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    trend = df_ref['yield_trend'].values.reshape(-1, 1)\n",
    "    y_pred_actual = y_pred_detrended + trend\n",
    "    \n",
    "    return y_pred_actual.flatten()\n",
    "\n",
    "# --- Evaluation ---\n",
    "results = []\n",
    "y_preds_original = {}\n",
    "\n",
    "print(\"\\n--- Final Performance (Test Set) ---\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in ['LR', 'RF', 'XGB']:\n",
    "        # Predict on the scaled test set\n",
    "        preds_scaled = model.predict(X_test_ml)\n",
    "        # Reconstruct the predictions\n",
    "        pred_orig = reconstruct_yield(preds_scaled, test_df, y_scaler)\n",
    "        y_true_orig = test_df[TARGET].values\n",
    "        # Store for later use\n",
    "        y_preds_original[name] = pred_orig\n",
    "        \n",
    "    elif name in ['LSTM', 'CNN']:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Predict on the scaled test set\n",
    "            preds_scaled_t = model(*[x.to(device) for x in X_test_dl])\n",
    "            preds_scaled = preds_scaled_t.cpu().numpy()\n",
    "            # Reconstruct the predictions\n",
    "            pred_orig = reconstruct_yield(preds_scaled, test_df_dl_seq_ref, y_scaler)\n",
    "            y_true_orig = test_df_dl_seq_ref[TARGET].values\n",
    "            # Store for later use\n",
    "            y_preds_original[name] = pred_orig\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true_orig, pred_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_orig, pred_orig))\n",
    "    map_e = mape(y_true_orig, pred_orig)\n",
    "    rms_pe = rmspe(y_true_orig, pred_orig)\n",
    "    r_2 = r2_score(y_true_orig, pred_orig)\n",
    "    results.append({'Model': name, 'MAE': mae, 'RMSE': rmse, 'MAPE (%)': map_e, 'RMSPE (%)': rms_pe, 'R²': r_2})\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model').sort_values('RMSE')\n",
    "print(results_df.round(2))\n",
    "results_df.to_csv(\"final_model_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10a. Visualize Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Predicted vs Actual Scatter Plots\n",
    "print(\"--- Visualizing Predicted vs. Actual ---\")\n",
    "n_models = len(y_preds_original)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, y_pred) in enumerate(y_preds_original.items()):\n",
    "    ax = axes[i]\n",
    "    if name in ['LR', 'RF', 'XGB']:\n",
    "        y_true = test_df[TARGET].values\n",
    "    else:\n",
    "        y_true = test_df_dl_seq_ref[TARGET].values\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    ax.scatter(y_true, y_pred, alpha=0.5, label=f'Predictions')\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2, label='Ideal')\n",
    "    ax.set_xlabel(\"Actual Yield\")\n",
    "    ax.set_ylabel(\"Predicted Yield\")\n",
    "    ax.set_title(f'{name}: Predicted vs Actual (R² = {r2:.3f})')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"predicted_vs_true.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Time Series Predictions per Crop\n",
    "print(\"\\n--- Visualizing Time Series Predictions per Crop ---\")\n",
    "unique_crops = df['Item'].unique()\n",
    "\n",
    "for crop in unique_crops:\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    \n",
    "    # Plot historical training data\n",
    "    train_crop_df = df[(df['Item'] == crop) & (df['Year'] <= TRAIN_END)]\n",
    "    if not train_crop_df.empty:\n",
    "        plt.plot(train_crop_df['Year'], train_crop_df[TARGET], 'k-', label='Historical Train Data', linewidth=1.5)\n",
    "\n",
    "    # Plot actual test data\n",
    "    test_crop_df = df[(df['Item'] == crop) & (df['Year'] > VAL_END)]\n",
    "    if not test_crop_df.empty:\n",
    "        plt.plot(test_crop_df['Year'], test_crop_df[TARGET], 'b-o', label='Actual Test Data', markersize=5)\n",
    "\n",
    "    # Plot predictions from each model\n",
    "    for name, y_pred in y_preds_original.items():\n",
    "        if name in ['LR', 'RF', 'XGB']:\n",
    "            pred_df = test_df.copy()\n",
    "            pred_df['prediction'] = y_pred\n",
    "        else:\n",
    "            pred_df = test_df_dl_seq_ref.copy()\n",
    "            pred_df['prediction'] = y_pred\n",
    "            \n",
    "        crop_pred_df = pred_df[pred_df['Item'] == crop]\n",
    "        if not crop_pred_df.empty:\n",
    "            plt.plot(crop_pred_df['Year'], crop_pred_df['prediction'], '--', label=f'Predicted - {name}')\n",
    "\n",
    "    plt.title(f'Yield Prediction for {crop}', fontsize=16)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Yield (hg/ha)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "sns.barplot(data=results_df.reset_index(), x='Model', y='RMSE', ax=axs[0])\n",
    "axs[0].set_title('RMSE Comparison')\n",
    "sns.barplot(x='Model', y='MAE', data=results_df.reset_index(), ax=axs[1])\n",
    "axs[1].set_title('MAE Comparison')\n",
    "sns.barplot(x='Model', y='MAPE (%)', data=results_df.reset_index(), ax=axs[2])\n",
    "axs[2].set_title('MAPE Comparison')\n",
    "sns.barplot(x='Model', y='R²', data=results_df.reset_index(), ax=axs[3])\n",
    "axs[3].set_title('R² Comparison')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_performance_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Crop Reporting (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.index[0]\n",
    "print(f\"Per-crop report for best model: {best_model_name}\")\n",
    "crop_results = []\n",
    "# Use the correctly aligned test dataframe based on the best model\n",
    "if best_model_name in ['LR', 'RF', 'XGB']:\n",
    "    reporting_df = test_df\n",
    "    y_true_original = reporting_df[TARGET].values\n",
    "else:\n",
    "    reporting_df = test_df_dl_seq_ref\n",
    "    y_true_original = reporting_df[TARGET].values\n",
    "\n",
    "items = reporting_df['Item'].values\n",
    "\n",
    "for crop in np.unique(items):\n",
    "    mask = items == crop\n",
    "    true = y_true_original[mask]\n",
    "    pred = y_preds_original[best_model_name][mask]\n",
    "    if len(true) > 0:\n",
    "        crop_results.append({\n",
    "            'Crop': crop,\n",
    "            'RMSPE (%)': rmspe(true, pred),\n",
    "            'MAPE (%)': mape(true, pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(true, pred)),\n",
    "            'R²': r2_score(true, pred)\n",
    "        })\n",
    "crop_df = pd.DataFrame(crop_results).sort_values('RMSPE (%)')\n",
    "print(crop_df.round(2))\n",
    "crop_df.to_csv('per_crop_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. SHAP Analysis (If Tree Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.index[0]\n",
    "if best_model_name in models and best_model_name in ['RF', 'XGB']:\n",
    "    best_model = models[best_model_name]\n",
    "    print(f\"Running SHAP on {best_model_name}\")\n",
    "    # For SHAP, we need to use the correctly aligned test features\n",
    "    X_test_shap = X_test_ml\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test_shap)\n",
    "    shap.summary_plot(shap_values, X_test_shap, plot_type=\"beeswarm\", show=False)\n",
    "    plt.title(f\"SHAP Beeswarm ({best_model_name})\", fontsize=16)\n",
    "    plt.savefig(\"shap_beeswarm.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    shap.summary_plot(shap_values, X_test_shap, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"Feature Importance ({best_model_name})\", fontsize=16)\n",
    "    plt.savefig(\"shap_importance.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP skipped for non-tree model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base dataframe for predictions. ML models have more test samples than DL models.\n",
    "final_predictions_df = test_df.copy()\n",
    "final_predictions_df['true_yield_original'] = final_predictions_df[TARGET]\n",
    "\n",
    "# Add predictions. Note that DL predictions will have NaNs for non-sequenced rows.\n",
    "for name, preds in y_preds_original.items():\n",
    "    if name in ['LR', 'RF', 'XGB']:\n",
    "        final_predictions_df[f'predicted_{name}'] = preds\n",
    "    else:\n",
    "        # Align DL predictions with the main test dataframe\n",
    "        dl_preds_series = pd.Series(preds, index=test_df_dl_seq_ref.index, name=f'predicted_{name}')\n",
    "        final_predictions_df = final_predictions_df.join(dl_preds_series)\n",
    "\n",
    "export_cols = ['Year', 'Area', 'Item', 'true_yield_original'] + [f'predicted_{name}' for name in models.keys()]\n",
    "final_predictions_df[export_cols].to_csv(\"final_test_predictions.csv\", index=False)\n",
    "print(\"Exported predictions.\")\n",
    "print(\"\\n--- Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
