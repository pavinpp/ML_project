{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development: Time-Series Crop Yield Prediction\n",
    "\n",
    "**Objective:** To develop and compare multiple machine learning models to predict future crop yield (`hg/ha_yield`) based on historical data. \n",
    "\n",
    "This notebook will:\n",
    "1.  Load the cleaned dataset from the previous EDA.\n",
    "2.  Preprocess the data for both tabular and time-series (sequential) modeling.\n",
    "3.  Implement a chronological train-validation-test split.\n",
    "4.  Build, train, and evaluate five model families:\n",
    "    * Linear Regression (Baseline)\n",
    "    * XGBoost Regressor\n",
    "    * Feedforward Neural Network (NN)\n",
    "    * 1D Convolutional Neural Network (CNN)\n",
    "    * Long Short-Term Memory (LSTM) Network\n",
    "5.  Compare all models on key metrics (RMSE, MAE, R²) and discuss trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import\n",
    "\n",
    "Import all necessary libraries for data manipulation, preprocessing, modeling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavin/miniconda3/envs/tfenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Classic ML Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set plotting styles\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_crop_data.csv loaded successfully.\n",
      "Dataset shape: (25932, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year</th>\n",
       "      <th>hg/ha_yield</th>\n",
       "      <th>average_rain_fall_mm_per_year</th>\n",
       "      <th>pesticides_tonnes</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>fertilizer_kg/ha</th>\n",
       "      <th>solar_radiation_MJ/m2-day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1990</td>\n",
       "      <td>36613</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.37</td>\n",
       "      <td>176.165803</td>\n",
       "      <td>15.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Potatoes</td>\n",
       "      <td>1990</td>\n",
       "      <td>66667</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.37</td>\n",
       "      <td>176.165803</td>\n",
       "      <td>15.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Rice, paddy</td>\n",
       "      <td>1990</td>\n",
       "      <td>23333</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.37</td>\n",
       "      <td>176.165803</td>\n",
       "      <td>15.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Sorghum</td>\n",
       "      <td>1990</td>\n",
       "      <td>12500</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.37</td>\n",
       "      <td>176.165803</td>\n",
       "      <td>15.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>1990</td>\n",
       "      <td>7000</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.37</td>\n",
       "      <td>176.165803</td>\n",
       "      <td>15.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Area         Item  Year  hg/ha_yield  average_rain_fall_mm_per_year  \\\n",
       "0  Albania        Maize  1990        36613                           1485   \n",
       "1  Albania     Potatoes  1990        66667                           1485   \n",
       "2  Albania  Rice, paddy  1990        23333                           1485   \n",
       "3  Albania      Sorghum  1990        12500                           1485   \n",
       "4  Albania     Soybeans  1990         7000                           1485   \n",
       "\n",
       "   pesticides_tonnes  avg_temp  fertilizer_kg/ha  solar_radiation_MJ/m2-day  \n",
       "0              121.0     16.37        176.165803                      15.42  \n",
       "1              121.0     16.37        176.165803                      15.42  \n",
       "2              121.0     16.37        176.165803                      15.42  \n",
       "3              121.0     16.37        176.165803                      15.42  \n",
       "4              121.0     16.37        176.165803                      15.42  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25932 entries, 0 to 25931\n",
      "Data columns (total 9 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Area                           25932 non-null  object \n",
      " 1   Item                           25932 non-null  object \n",
      " 2   Year                           25932 non-null  int64  \n",
      " 3   hg/ha_yield                    25932 non-null  int64  \n",
      " 4   average_rain_fall_mm_per_year  25932 non-null  int64  \n",
      " 5   pesticides_tonnes              25932 non-null  float64\n",
      " 6   avg_temp                       25932 non-null  float64\n",
      " 7   fertilizer_kg/ha               25932 non-null  float64\n",
      " 8   solar_radiation_MJ/m2-day      25932 non-null  float64\n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "try:\n",
    "    df = pd.read_csv('cleaned_crop_data.csv')\n",
    "    print(\"cleaned_crop_data.csv loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'cleaned_crop_data.csv' not found.\")\n",
    "    print(\"Please ensure the file from the EDA notebook is in the same directory.\")\n",
    "    df = pd.DataFrame() # Create empty df to avoid downstream errors\n",
    "\n",
    "if not df.empty:\n",
    "    display(df.head())\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "We will perform several key preprocessing steps:\n",
    "1.  **Encode** categorical features (`Area`, `Item`).\n",
    "2.  **Sort** the data chronologically by `Area`, `Item`, and `Year`. This is critical for time-series splits.\n",
    "3.  **Split** the data into train, validation, and test sets based on `Year`.\n",
    "4.  **Scale** the numeric features using `StandardScaler`, fitting *only* on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sorted and encoded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Item</th>\n",
       "      <th>Year</th>\n",
       "      <th>hg/ha_yield</th>\n",
       "      <th>average_rain_fall_mm_per_year</th>\n",
       "      <th>pesticides_tonnes</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>fertilizer_kg/ha</th>\n",
       "      <th>solar_radiation_MJ/m2-day</th>\n",
       "      <th>Area_Encoded</th>\n",
       "      <th>Item_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1990</td>\n",
       "      <td>36613</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.37</td>\n",
       "      <td>176.165803</td>\n",
       "      <td>15.42</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1991</td>\n",
       "      <td>29068</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>15.36</td>\n",
       "      <td>54.152249</td>\n",
       "      <td>14.89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1992</td>\n",
       "      <td>24876</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.06</td>\n",
       "      <td>40.657439</td>\n",
       "      <td>15.17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1993</td>\n",
       "      <td>24185</td>\n",
       "      <td>1485</td>\n",
       "      <td>121.0</td>\n",
       "      <td>16.05</td>\n",
       "      <td>31.005199</td>\n",
       "      <td>15.78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1994</td>\n",
       "      <td>25848</td>\n",
       "      <td>1485</td>\n",
       "      <td>201.0</td>\n",
       "      <td>16.96</td>\n",
       "      <td>25.597920</td>\n",
       "      <td>15.90</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area   Item  Year  hg/ha_yield  average_rain_fall_mm_per_year  \\\n",
       "0   Albania  Maize  1990        36613                           1485   \n",
       "6   Albania  Maize  1991        29068                           1485   \n",
       "12  Albania  Maize  1992        24876                           1485   \n",
       "18  Albania  Maize  1993        24185                           1485   \n",
       "23  Albania  Maize  1994        25848                           1485   \n",
       "\n",
       "    pesticides_tonnes  avg_temp  fertilizer_kg/ha  solar_radiation_MJ/m2-day  \\\n",
       "0               121.0     16.37        176.165803                      15.42   \n",
       "6               121.0     15.36         54.152249                      14.89   \n",
       "12              121.0     16.06         40.657439                      15.17   \n",
       "18              121.0     16.05         31.005199                      15.78   \n",
       "23              201.0     16.96         25.597920                      15.90   \n",
       "\n",
       "    Area_Encoded  Item_Encoded  \n",
       "0              0             1  \n",
       "6              0             1  \n",
       "12             0             1  \n",
       "18             0             1  \n",
       "23             0             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # 1. Encode Categorical Features\n",
    "    le_area = LabelEncoder()\n",
    "    le_item = LabelEncoder()\n",
    "    \n",
    "    df['Area_Encoded'] = le_area.fit_transform(df['Area'])\n",
    "    df['Item_Encoded'] = le_item.fit_transform(df['Item'])\n",
    "    \n",
    "    # 2. Sort Data Chronologically\n",
    "    df = df.sort_values(by=['Area_Encoded', 'Item_Encoded', 'Year'])\n",
    "    \n",
    "    print(\"Data sorted and encoded.\")\n",
    "    display(df.head())\n",
    "\n",
    "    # Define features and target\n",
    "    target_col = 'hg/ha_yield'\n",
    "    \n",
    "    # Original numeric features for scaling\n",
    "    numeric_features = [\n",
    "        'average_rain_fall_mm_per_year',\n",
    "        'pesticides_tonnes',\n",
    "        'avg_temp',\n",
    "        'fertilizer_kg/ha',\n",
    "        'solar_radiation_MJ/m2-day'\n",
    "    ]\n",
    "    \n",
    "    # All features for modeling (after scaling)\n",
    "    all_features = numeric_features + ['Area_Encoded', 'Item_Encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Chronological Train-Val-Test Split\n",
    "\n",
    "The dataset spans 1990-2013. We must split by year to ensure we are predicting the future from the past.\n",
    "\n",
    "* **Train:** 1990 - 2007\n",
    "* **Validation:** 2008 - 2010\n",
    "* **Test:** 2011 - 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:   (19032, 11)\n",
      "Validation set shape: (3424, 11)\n",
      "Test set shape:       (3476, 11)\n",
      "\n",
      "Numeric features scaled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>1.998309e+03</td>\n",
       "      <td>5.209056</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hg/ha_yield</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>7.323899e+04</td>\n",
       "      <td>80559.739884</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>19073.000000</td>\n",
       "      <td>36858.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>487219.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_rain_fall_mm_per_year</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>1.792037e-16</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>-1.527191</td>\n",
       "      <td>-0.772602</td>\n",
       "      <td>-0.090409</td>\n",
       "      <td>0.724046</td>\n",
       "      <td>2.912633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pesticides_tonnes</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>9.557532e-17</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>-0.721695</td>\n",
       "      <td>-0.684090</td>\n",
       "      <td>-0.340466</td>\n",
       "      <td>0.278657</td>\n",
       "      <td>6.437435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_temp</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>4.778766e-17</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>-2.921531</td>\n",
       "      <td>-0.595088</td>\n",
       "      <td>0.150308</td>\n",
       "      <td>0.878586</td>\n",
       "      <td>1.499489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fertilizer_kg/ha</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>-3.434738e-17</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>-0.714818</td>\n",
       "      <td>-0.421498</td>\n",
       "      <td>-0.201176</td>\n",
       "      <td>0.110055</td>\n",
       "      <td>12.738741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solar_radiation_MJ/m2-day</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>-3.165932e-16</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>-3.267316</td>\n",
       "      <td>-0.446426</td>\n",
       "      <td>0.261810</td>\n",
       "      <td>0.638532</td>\n",
       "      <td>1.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area_Encoded</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>4.632950e+01</td>\n",
       "      <td>26.977857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item_Encoded</th>\n",
       "      <td>19032.0</td>\n",
       "      <td>4.398066e+00</td>\n",
       "      <td>2.640535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 count          mean           std  \\\n",
       "Year                           19032.0  1.998309e+03      5.209056   \n",
       "hg/ha_yield                    19032.0  7.323899e+04  80559.739884   \n",
       "average_rain_fall_mm_per_year  19032.0  1.792037e-16      1.000026   \n",
       "pesticides_tonnes              19032.0  9.557532e-17      1.000026   \n",
       "avg_temp                       19032.0  4.778766e-17      1.000026   \n",
       "fertilizer_kg/ha               19032.0 -3.434738e-17      1.000026   \n",
       "solar_radiation_MJ/m2-day      19032.0 -3.165932e-16      1.000026   \n",
       "Area_Encoded                   19032.0  4.632950e+01     26.977857   \n",
       "Item_Encoded                   19032.0  4.398066e+00      2.640535   \n",
       "\n",
       "                                       min           25%           50%  \\\n",
       "Year                           1990.000000   1994.000000   1998.000000   \n",
       "hg/ha_yield                      50.000000  19073.000000  36858.000000   \n",
       "average_rain_fall_mm_per_year    -1.527191     -0.772602     -0.090409   \n",
       "pesticides_tonnes                -0.721695     -0.684090     -0.340466   \n",
       "avg_temp                         -2.921531     -0.595088      0.150308   \n",
       "fertilizer_kg/ha                 -0.714818     -0.421498     -0.201176   \n",
       "solar_radiation_MJ/m2-day        -3.267316     -0.446426      0.261810   \n",
       "Area_Encoded                      0.000000     23.000000     42.000000   \n",
       "Item_Encoded                      0.000000      3.000000      4.000000   \n",
       "\n",
       "                                         75%            max  \n",
       "Year                             2002.000000    2007.000000  \n",
       "hg/ha_yield                    100000.000000  487219.000000  \n",
       "average_rain_fall_mm_per_year       0.724046       2.912633  \n",
       "pesticides_tonnes                   0.278657       6.437435  \n",
       "avg_temp                            0.878586       1.499489  \n",
       "fertilizer_kg/ha                    0.110055      12.738741  \n",
       "solar_radiation_MJ/m2-day           0.638532       1.747600  \n",
       "Area_Encoded                       67.000000     100.000000  \n",
       "Item_Encoded                        7.000000       9.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # 3. Split Data\n",
    "    train_df = df[df['Year'] <= 2007].copy()\n",
    "    val_df = df[(df['Year'] > 2007) & (df['Year'] <= 2010)].copy()\n",
    "    test_df = df[df['Year'] > 2010].copy()\n",
    "\n",
    "    print(f\"Training set shape:   {train_df.shape}\")\n",
    "    print(f\"Validation set shape: {val_df.shape}\")\n",
    "    print(f\"Test set shape:       {test_df.shape}\")\n",
    "\n",
    "    # 4. Scale Numeric Features\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit scaler ONLY on the training data\n",
    "    train_df[numeric_features] = scaler.fit_transform(train_df[numeric_features])\n",
    "    \n",
    "    # Transform validation and test data\n",
    "    val_df[numeric_features] = scaler.transform(val_df[numeric_features])\n",
    "    test_df[numeric_features] = scaler.transform(test_df[numeric_features])\n",
    "    \n",
    "    print(\"\\nNumeric features scaled.\")\n",
    "    display(train_df.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create Tabular Datasets (for LinReg, XGB, NN)\n",
    "\n",
    "These models require a simple 2D input `(samples, features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tab shape: (19032, 7), y_train_tab shape: (19032,)\n",
      "X_val_tab shape:   (3424, 7), y_val_tab shape:   (3424,)\n",
      "X_test_tab shape:  (3476, 7), y_test_tab shape:  (3476,)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # Create Tabular (non-sequential) data\n",
    "    X_train_tab = train_df[all_features]\n",
    "    y_train_tab = train_df[target_col]\n",
    "\n",
    "    X_val_tab = val_df[all_features]\n",
    "    y_val_tab = val_df[target_col]\n",
    "\n",
    "    X_test_tab = test_df[all_features]\n",
    "    y_test_tab = test_df[target_col]\n",
    "\n",
    "    print(f\"X_train_tab shape: {X_train_tab.shape}, y_train_tab shape: {y_train_tab.shape}\")\n",
    "    print(f\"X_val_tab shape:   {X_val_tab.shape}, y_val_tab shape:   {y_val_tab.shape}\")\n",
    "    print(f\"X_test_tab shape:  {X_test_tab.shape}, y_test_tab shape:  {y_test_tab.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequence Generation (for CNN & LSTM)\n",
    "\n",
    "Now we create the 3D sequential data `(samples, timesteps, features)`. We will use a `lookback` window of 5 years to predict the 6th year's yield. \n",
    "\n",
    "**Important:** The sequence generator must respect the `Area` and `Item` groups to avoid creating sequences that cross from one crop/country to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lookback: 5 years\n",
      "X_train_seq shape: (16115, 5, 7), y_train_seq shape: (16115,)\n",
      "X_val_seq shape:   (3405, 5, 7), y_val_seq shape:   (3405,)\n",
      "X_test_seq shape:  (3439, 5, 7), y_test_seq shape:  (3439,)\n",
      "\n",
      "PyTorch Tensors created.\n",
      "PyTorch DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "LOOKBACK = 5\n",
    "\n",
    "def create_sequences(df, feature_cols, target_col, lookback):\n",
    "    \"\"\"\n",
    "    Creates time-series sequences from dataframe.\n",
    "    Respects groups defined by 'Area_Encoded' and 'Item_Encoded'.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    # Group by Area and Item\n",
    "    for _, group in df.groupby(['Area_Encoded', 'Item_Encoded']):\n",
    "        features = group[feature_cols].values\n",
    "        target = group[target_col].values\n",
    "        \n",
    "        # Create sequences for this group\n",
    "        for i in range(len(group) - lookback):\n",
    "            X_seq.append(features[i:i+lookback])\n",
    "            y_seq.append(target[i+lookback])\n",
    "            \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "if not df.empty:\n",
    "    # We need to include the data from previous years to form the first sequences \n",
    "    # for the validation and test sets. We'll use the full dataset for sequence creation\n",
    "    # and then split the *resulting sequences* by their end year.\n",
    "\n",
    "    # Combine all data back together for proper sequence creation\n",
    "    full_scaled_df = pd.concat([train_df, val_df, test_df])\n",
    "    \n",
    "    # We need to add the year to the df to split the sequences later\n",
    "    X_with_year, y_with_year = [], []\n",
    "    \n",
    "    for _, group in full_scaled_df.groupby(['Area_Encoded', 'Item_Encoded']):\n",
    "        features = group[all_features].values\n",
    "        target = group[target_col].values\n",
    "        years = group['Year'].values\n",
    "        \n",
    "        for i in range(len(group) - LOOKBACK):\n",
    "            X_with_year.append(features[i:i+LOOKBACK])\n",
    "            # Store (year, target_value)\n",
    "            y_with_year.append((years[i+LOOKBACK], target[i+LOOKBACK]))\n",
    "\n",
    "    X_seq_full = np.array(X_with_year)\n",
    "    y_seq_full = np.array(y_with_year)\n",
    "    \n",
    "    # Now, split the sequences based on the *target year*\n",
    "    train_indices = y_seq_full[:, 0] <= 2007\n",
    "    val_indices = (y_seq_full[:, 0] > 2007) & (y_seq_full[:, 0] <= 2010)\n",
    "    test_indices = y_seq_full[:, 0] > 2010\n",
    "\n",
    "    X_train_seq = X_seq_full[train_indices]\n",
    "    y_train_seq = y_seq_full[train_indices, 1] # Get only the target value\n",
    "\n",
    "    X_val_seq = X_seq_full[val_indices]\n",
    "    y_val_seq = y_seq_full[val_indices, 1]\n",
    "\n",
    "    X_test_seq = X_seq_full[test_indices]\n",
    "    y_test_seq = y_seq_full[test_indices, 1]\n",
    "\n",
    "    print(f\"Lookback: {LOOKBACK} years\")\n",
    "    print(f\"X_train_seq shape: {X_train_seq.shape}, y_train_seq shape: {y_train_seq.shape}\")\n",
    "    print(f\"X_val_seq shape:   {X_val_seq.shape}, y_val_seq shape:   {y_val_seq.shape}\")\n",
    "    print(f\"X_test_seq shape:  {X_test_seq.shape}, y_test_seq shape:  {y_test_seq.shape}\")\n",
    "\n",
    "    # For LSTM/CNN input shape\n",
    "    n_timesteps = X_train_seq.shape[1]\n",
    "    n_features = X_train_seq.shape[2]\n",
    "\n",
    "    # --- UPDATED FOR EMBEDDINGS ---\n",
    "    # Convert all data to PyTorch Tensors\n",
    "    # Tabular data (split into continuous and categorical)\n",
    "    cat_features = ['Area_Encoded', 'Item_Encoded']\n",
    "    \n",
    "    X_train_cont_t = torch.tensor(X_train_tab[numeric_features].values, dtype=torch.float32)\n",
    "    X_train_cat_t = torch.tensor(X_train_tab[cat_features].values, dtype=torch.long) # Embeddings need Long type\n",
    "    y_train_tab_t = torch.tensor(y_train_tab.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    X_val_cont_t = torch.tensor(X_val_tab[numeric_features].values, dtype=torch.float32)\n",
    "    X_val_cat_t = torch.tensor(X_val_tab[cat_features].values, dtype=torch.long)\n",
    "    y_val_tab_t = torch.tensor(y_val_tab.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    X_test_cont_t = torch.tensor(X_test_tab[numeric_features].values, dtype=torch.float32)\n",
    "    X_test_cat_t = torch.tensor(X_test_tab[cat_features].values, dtype=torch.long)\n",
    "    y_test_tab_t = torch.tensor(y_test_tab.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Sequential data (remains the same)\n",
    "    X_train_seq_t = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "    y_train_seq_t = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1)\n",
    "    X_val_seq_t = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "    y_val_seq_t = torch.tensor(y_val_seq, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_seq_t = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "    y_test_seq_t = torch.tensor(y_test_seq, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    print(\"\\nPyTorch Tensors created.\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    # Tabular loaders (with multiple inputs)\n",
    "    train_tab_dataset = TensorDataset(X_train_cont_t, X_train_cat_t, y_train_tab_t)\n",
    "    train_tab_loader = DataLoader(train_tab_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_tab_dataset = TensorDataset(X_val_cont_t, X_val_cat_t, y_val_tab_t)\n",
    "    val_tab_loader = DataLoader(val_tab_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Sequential loaders (remains the same)\n",
    "    train_seq_dataset = TensorDataset(X_train_seq_t, y_train_seq_t)\n",
    "    train_seq_loader = DataLoader(train_seq_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_seq_dataset = TensorDataset(X_val_seq_t, y_val_seq_t)\n",
    "    val_seq_loader = DataLoader(val_seq_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"PyTorch DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Evaluation Helper\n",
    "\n",
    "We'll create a dictionary to store all model results and a function to plot predictions.\n",
    "We also add PyTorch-specific helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2655937613.py, line 49)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef run_pytorch_training(model, model_name, train_loader, val_loader, epochs=100, patience=15, lr=0.001):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "model_performance = {}\n",
    "\n",
    "def evaluate_model(model_name, y_train, y_train_pred, y_val, y_val_pred, y_test, y_test_pred):\n",
    "    \"Calculates and stores RMSE, MAE, and R2 for a model.\"\n",
    "    \n",
    "    metrics = {\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Val RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'Test R2': r2_score(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    model_performance[model_name] = metrics\n",
    "    print(f\"--- Results for {model_name} ---\")\n",
    "    print(f\"Train RMSE: {metrics['Train RMSE']:.2f}\")\n",
    "    print(f\"Val RMSE:   {metrics['Val RMSE']:.2f}\")\n",
    "    print(f\"Test RMSE:  {metrics['Test RMSE']:.2f}\")\n",
    "    print(f\"Test MAE:   {metrics['Test MAE']:.2f}\")\n",
    "    print(f\"Test R2:    {metrics['Test R2']:.2%}\")\n",
    "\n",
    "def plot_predictions(model_name, y_test, y_pred):\n",
    "    \"Plots actual vs. predicted values and residuals.\"\n",
    "    \n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Actual vs. Predicted\n",
    "    sns.scatterplot(x=y_test, y=y_pred, alpha=0.5, ax=ax1)\n",
    "    ax1.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red', lw=2)\n",
    "    ax1.set_title(f'{model_name}: Actual vs. Predicted')\n",
    "    ax1.set_xlabel('Actual Yield')\n",
    "    ax1.set_ylabel('Predicted Yield')\n",
    "    \n",
    "    # Residual Plot\n",
    "    sns.histplot(residuals, kde=True, ax=ax2)\n",
    "    ax2.set_title('Residual Distribution')\n",
    "    ax2.set_xlabel('Residual (Actual - Predicted)')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} Evaluation', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    def run_pytorch_training(model, model_name, train_loader, val_loader, epochs=100, patience=15, lr=0.001):\n",
    "    \"\"\"Generic training loop for a PyTorch regression model.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader: # Unpack batch\n",
    "            # Handle tabular (cont, cat, y) vs sequential (seq, y)\n",
    "            if len(batch) == 3:\n",
    "                X_cont_batch, X_cat_batch, y_batch = batch\n",
    "                X_cont_batch, X_cat_batch, y_batch = X_cont_batch.to(device), X_cat_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_cont_batch, X_cat_batch)\n",
    "            else:\n",
    "                X_batch, y_batch = batch\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "            \n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if len(batch) == 3:\n",
    "                    X_cont_batch, X_cat_batch, y_batch = batch\n",
    "                    X_cont_batch, X_cat_batch, y_batch = X_cont_batch.to(device), X_cat_batch.to(device), y_batch.to(device)\n",
    "                    y_pred = model(X_cont_batch, X_cat_batch)\n",
    "                else:\n",
    "                    X_batch, y_batch = batch\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    y_pred = model(X_batch)\n",
    "                \n",
    "                val_loss = criterion(y_pred, y_batch)\n",
    "                val_losses.append(val_loss.item())\n",
    "        \n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "                \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(torch.load(f'{model_name}_best.pth'))\n",
    "    print(\"Best model weights loaded.\")\n",
    "    return model, history\n",
    "\n",
    "def get_pytorch_predictions(model, loader):\n",
    "    \"\"\"Get predictions from a PyTorch model given a loader.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if len(batch) == 3:\n",
    "                X_cont_batch, X_cat_batch, _ = batch\n",
    "                X_cont_batch, X_cat_batch = X_cont_batch.to(device), X_cat_batch.to(device)\n",
    "                y_pred = model(X_cont_batch, X_cat_batch)\n",
    "            else:\n",
    "                X_batch, _ = batch\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "            predictions.append(y_pred.cpu().numpy())\n",
    "    return np.concatenate(predictions).flatten()\n",
    "\n",
    "def get_pytorch_predictions_from_tensor(model, X_tensor, X_cat_tensor=None):\n",
    "    \"\"\"Get predictions from a PyTorch model given a raw tensor (for full train/val/test sets).\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = X_tensor.to(device)\n",
    "        if X_cat_tensor is not None:\n",
    "            # Tabular model with embeddings\n",
    "            X_cat_tensor = X_cat_tensor.to(device)\n",
    "            y_pred = model(X_tensor, X_cat_tensor)\n",
    "        else:\n",
    "            # Sequential model\n",
    "            y_pred = model(X_tensor)\n",
    "        return y_pred.cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model — Linear Regression\n",
    "\n",
    "Our first model is a simple Linear Regression on the tabular data. This provides a crucial baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Training Baseline: Linear Regression...\")\n",
    "    model_lr = LinearRegression()\n",
    "    model_lr.fit(X_train_tab, y_train_tab)\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred_lr = model_lr.predict(X_train_tab)\n",
    "    y_val_pred_lr = model_lr.predict(X_val_tab)\n",
    "    y_test_pred_lr = model_lr.predict(X_test_tab)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(\"Linear Regression\", \n",
    "                   y_train_tab, y_train_pred_lr, \n",
    "                   y_val_tab, y_val_pred_lr, \n",
    "                   y_test_tab, y_test_pred_lr)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"Linear Regression\", y_test_tab, y_test_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost Regressor\n",
    "\n",
    "Next, we'll use a powerful tree-based model, XGBoost. It's excellent on tabular data and provides feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Training Model: XGBoost Regressor...\")\n",
    "    model_xgb = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                 n_estimators=1000,\n",
    "                                 learning_rate=0.05,\n",
    "                                 max_depth=6,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 random_state=42,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "    model_xgb.fit(X_train_tab, y_train_tab,\n",
    "                  eval_set=[(X_val_tab, y_val_tab)],\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose=False)\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred_xgb = model_xgb.predict(X_train_tab)\n",
    "    y_val_pred_xgb = model_xgb.predict(X_val_tab)\n",
    "    y_test_pred_xgb = model_xgb.predict(X_test_tab)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(\"XGBoost\", \n",
    "                   y_train_tab, y_train_pred_xgb, \n",
    "                   y_val_tab, y_val_pred_xgb, \n",
    "                   y_test_tab, y_test_pred_xgb)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"XGBoost\", y_test_tab, y_test_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. XGBoost Feature Importance\n",
    "\n",
    "Let's see what XGBoost learned. We'll check both standard feature importance and a SHAP summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Plot feature importance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    xgb.plot_importance(model_xgb, ax=ax1, max_num_features=10)\n",
    "    ax1.set_title('XGBoost Feature Importance (Gain)')\n",
    "    \n",
    "    # SHAP Summary Plot\n",
    "    print(\"Calculating SHAP values...\")\n",
    "    explainer = shap.TreeExplainer(model_xgb)\n",
    "    # Use a sample of the test set for SHAP to speed it up\n",
    "    shap_sample = X_test_tab.sample(n=1000, random_state=42)\n",
    "    shap_values = explainer.shap_values(shap_sample)\n",
    "    \n",
    "    shap.summary_plot(shap_values, shap_sample, feature_names=all_features, show=False, max_display=10)\n",
    "    ax2.set_title('SHAP Summary Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feedforward Neural Network (NN)\n",
    "\n",
    "Our first deep learning model. This is a simple, fully connected network trained on the *tabular* data, just like the XGBoost model. **(Using PyTorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Building Feedforward Neural Network (NN) with PyTorch...\")\n",
    "    \n",
    "    # Get embedding sizes\n",
    "    n_areas = df['Area_Encoded'].nunique()\n",
    "    n_items = df['Item_Encoded'].nunique()\n",
    "    \n",
    "    # Heuristic for embedding dimensions\n",
    "    area_emb_dim = min(50, n_areas // 2 + 1)\n",
    "    item_emb_dim = min(50, n_items // 2 + 1)\n",
    "    n_cont_features = len(numeric_features)\n",
    "    \n",
    "    print(f\"Area Embeddings: {n_areas} unique areas, using dim {area_emb_dim}\")\n",
    "    print(f\"Item Embeddings: {n_items} unique items, using dim {item_emb_dim}\")\n",
    "\n",
    "    class FeedForwardNN(nn.Module):\n",
    "        def __init__(self, n_cont_features, n_areas, n_items, area_emb_dim, item_emb_dim):\n",
    "            super(FeedForwardNN, self).__init__()\n",
    "            # Embedding layers\n",
    "            self.area_embedding = nn.Embedding(n_areas, area_emb_dim)\n",
    "            self.item_embedding = nn.Embedding(n_items, item_emb_dim)\n",
    "            \n",
    "            # Calculate input size for the first linear layer\n",
    "            fc1_input_size = n_cont_features + area_emb_dim + item_emb_dim\n",
    "            \n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(fc1_input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 1) # Linear output for regression\n",
    "            )\n",
    "        \n",
    "        def forward(self, x_cont, x_cat):\n",
    "            # Get embedding vectors\n",
    "            area_embeds = self.area_embedding(x_cat[:, 0])\n",
    "            item_embeds = self.item_embedding(x_cat[:, 1])\n",
    "            \n",
    "            # Concatenate continuous features and embeddings\n",
    "            x = torch.cat([x_cont, area_embeds, item_embeds], dim=1)\n",
    "            \n",
    "            return self.network(x)\n",
    "\n",
    "    model_nn_pt = FeedForwardNN(n_cont_features, n_areas, n_items, area_emb_dim, item_emb_dim)\n",
    "    print(model_nn_pt)\n",
    "\n",
    "    print(\"\\nTraining NN (PyTorch)...\")\n",
    "    model_nn_pt, history_nn = run_pytorch_training(\n",
    "        model_nn_pt, \"NN_PyTorch\", \n",
    "        train_tab_loader, val_tab_loader,\n",
    "        epochs=100, patience=15, lr=0.001\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    # Plot loss curves\n",
    "    pd.DataFrame(history_nn).plot(figsize=(10, 6))\n",
    "    plt.title('NN (PyTorch) Model Loss')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train Loss', 'Validation Loss'])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Make predictions\n",
    "    y_train_pred_nn = get_pytorch_predictions_from_tensor(model_nn_pt, X_train_cont_t, X_train_cat_t)\n",
    "    y_val_pred_nn = get_pytorch_predictions_from_tensor(model_nn_pt, X_val_cont_t, X_val_cat_t)\n",
    "    y_test_pred_nn = get_pytorch_predictions_from_tensor(model_nn_pt, X_test_cont_t, X_test_cat_t)\n",
    "\n",
    "    # Evaluate\n",
    "    # We use the original numpy targets (y_train_tab) for compatibility with evaluate_model\n",
    "    evaluate_model(\"Neural Network (NN-PyTorch)\", \n",
    "                   y_train_tab, y_train_pred_nn, \n",
    "                   y_val_tab, y_val_pred_nn, \n",
    "                   y_test_tab, y_test_pred_nn)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"Neural Network (NN-PyTorch)\", y_test_tab, y_test_pred_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "Now we'll use our *sequential* data. A 1D CNN is effective at learning patterns over a fixed-length sequence (our 5-year lookback). **(Using PyTorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Building 1D CNN Model (PyTorch)...\")\n",
    "    \n",
    "    class CNN1D(nn.Module):\n",
    "        def __init__(self, n_features, n_timesteps):\n",
    "            super(CNN1D, self).__init__()\n",
    "            self.conv_layer = nn.Sequential(\n",
    "                # PyTorch Conv1d expects (batch, features, timesteps)\n",
    "                nn.Conv1d(in_channels=n_features, out_channels=64, kernel_size=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2)\n",
    "            )\n",
    "            # Calculate the flattened size after conv/pool\n",
    "            # L_out = floor((L_in + 2*padding - dilation*(kernel_size - 1) - 1)/stride + 1)\n",
    "            # Conv1d: (5 + 0 - 1*(2-1) - 1)/1 + 1 = 4\n",
    "            # MaxPool1d: (4 + 0 - 1*(2-1) - 1)/2 + 1 = 2\n",
    "            conv_out_size = 64 * 2 # 64 filters * 2 output size from pool\n",
    "            \n",
    "            self.fc_layer = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(conv_out_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Input x is (batch, timesteps, features)\n",
    "            # We must permute it to (batch, features, timesteps)\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.conv_layer(x)\n",
    "            x = self.fc_layer(x)\n",
    "            return x\n",
    "\n",
    "    model_cnn_pt = CNN1D(n_features=n_features, n_timesteps=n_timesteps)\n",
    "    print(model_cnn_pt)\n",
    "    \n",
    "    print(\"\\nTraining 1D CNN (PyTorch)...\")\n",
    "    model_cnn_pt, history_cnn = run_pytorch_training(\n",
    "        model_cnn_pt, \"CNN1D_PyTorch\", \n",
    "        train_seq_loader, val_seq_loader,\n",
    "        epochs=100, patience=15, lr=0.001\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    # Plot loss curves\n",
    "    pd.DataFrame(history_cnn).plot(figsize=(10, 6))\n",
    "    plt.title('1D CNN (PyTorch) Model Loss')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train Loss', 'Validation Loss'])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Make predictions\n",
    "    y_train_pred_cnn = get_pytorch_predictions_from_tensor(model_cnn_pt, X_train_seq_t)\n",
    "    y_val_pred_cnn = get_pytorch_predictions_from_tensor(model_cnn_pt, X_val_seq_t)\n",
    "    y_test_pred_cnn = get_pytorch_predictions_from_tensor(model_cnn_pt, X_test_seq_t)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(\"1D CNN (PyTorch)\", \n",
    "                   y_train_seq, y_train_pred_cnn, \n",
    "                   y_val_seq, y_val_pred_cnn, \n",
    "                   y_test_seq, y_test_pred_cnn)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"1D CNN (PyTorch)\", y_test_seq, y_test_pred_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LSTM Model (Time-Dependent)\n",
    "\n",
    "Finally, we'll use an LSTM, which is designed specifically for sequential data. It maintains an internal state to remember patterns over time. **(Using PyTorch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Building LSTM Model (PyTorch)...\")\n",
    "    \n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, n_features, hidden_size1=64, hidden_size2=32, dropout=0.2):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            # batch_first=True makes LSTM accept (batch, timesteps, features)\n",
    "            self.lstm1 = nn.LSTM(n_features, hidden_size1, batch_first=True, return_sequences=True)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True, return_sequences=False)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.fc = nn.Linear(hidden_size2, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x is (batch, timesteps, features)\n",
    "            x, _ = self.lstm1(x)\n",
    "            x = self.dropout1(x)\n",
    "            # h_n of shape (num_layers, batch, hidden_size)\n",
    "            x, (h_n, c_n) = self.lstm2(x)\n",
    "            x = self.dropout2(h_n[-1]) # Get output from last layer\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    model_lstm_pt = LSTMModel(n_features=n_features)\n",
    "    print(model_lstm_pt)\n",
    "    \n",
    "    print(\"\\nTraining LSTM (PyTorch)...\")\n",
    "    model_lstm_pt, history_lstm = run_pytorch_training(\n",
    "        model_lstm_pt, \"LSTM_PyTorch\", \n",
    "        train_seq_loader, val_seq_loader,\n",
    "        epochs=100, patience=15, lr=0.001\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    # Plot loss curves\n",
    "    pd.DataFrame(history_lstm).plot(figsize=(10, 6))\n",
    "    plt.title('LSTM (PyTorch) Model Loss')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train Loss', 'Validation Loss'])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Make predictions\n",
    "    y_train_pred_lstm = get_pytorch_predictions_from_tensor(model_lstm_pt, X_train_seq_t)\n",
    "    y_val_pred_lstm = get_pytorch_predictions_from_tensor(model_lstm_pt, X_val_seq_t)\n",
    "    y_test_pred_lstm = get_pytorch_predictions_from_tensor(model_lstm_pt, X_test_seq_t)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(\"LSTM (PyTorch)\", \n",
    "                   y_train_seq, y_train_pred_lstm, \n",
    "                   y_val_seq, y_val_pred_lstm, \n",
    "                   y_test_seq, y_test_pred_lstm)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"LSTM (PyTorch)\", y_test_seq, y_test_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "Now we'll use our *sequential* data. A 1D CNN is effective at learning patterns over a fixed-length sequence (our 5-year lookback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Building 1D CNN Model...\")\n",
    "    model_cnn = Sequential([\n",
    "        Input(shape=(n_timesteps, n_features)),\n",
    "        Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                    loss='mse', \n",
    "                    metrics=['mae'])\n",
    "    \n",
    "    model_cnn.summary()\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "    print(\"\\nTraining 1D CNN...\")\n",
    "    history_cnn = model_cnn.fit(X_train_seq, y_train_seq,\n",
    "                              epochs=100,\n",
    "                              validation_data=(X_val_seq, y_val_seq),\n",
    "                              callbacks=[early_stop],\n",
    "                              batch_size=64,\n",
    "                              verbose=0)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    # Plot loss curves\n",
    "    pd.DataFrame(history_cnn.history).plot(figsize=(10, 6))\n",
    "    plt.title('1D CNN Model Loss')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Make predictions\n",
    "    y_train_pred_cnn = model_cnn.predict(X_train_seq).flatten()\n",
    "    y_val_pred_cnn = model_cnn.predict(X_val_seq).flatten()\n",
    "    y_test_pred_cnn = model_cnn.predict(X_test_seq).flatten()\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(\"1D CNN\", \n",
    "                   y_train_seq, y_train_pred_cnn, \n",
    "                   y_val_seq, y_val_pred_cnn, \n",
    "                   y_test_seq, y_test_pred_cnn)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"1D CNN\", y_test_seq, y_test_pred_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LSTM Model (Time-Dependent)\n",
    "\n",
    "Finally, we'll use an LSTM, which is designed specifically for sequential data. It maintains an internal state to remember patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"Building LSTM Model...\")\n",
    "    model_lstm = Sequential([\n",
    "        Input(shape=(n_timesteps, n_features)),\n",
    "        LSTM(64, return_sequences=True), # Keep sequences for the next LSTM layer\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=False), # Last LSTM layer returns a single vector\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                     loss='mse', \n",
    "                     metrics=['mae'])\n",
    "    \n",
    "    model_lstm.summary()\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "    print(\"\\nTraining LSTM...\")\n",
    "    history_lstm = model_lstm.fit(X_train_seq, y_train_seq,\n",
    "                                epochs=100,\n",
    "                                validation_data=(X_val_seq, y_val_seq),\n",
    "                                callbacks=[early_stop],\n",
    "                                batch_size=64,\n",
    "                                verbose=0)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    # Plot loss curves\n",
    "    pd.DataFrame(history_lstm.history).plot(figsize=(10, 6))\n",
    "    plt.title('LSTM Model Loss')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Make predictions\n",
    "    y_train_pred_lstm = model_lstm.predict(X_train_seq).flatten()\n",
    "    y_val_pred_lstm = model_lstm.predict(X_val_seq).flatten()\n",
    "    y_test_pred_lstm = model_lstm.predict(X_test_seq).flatten()\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate_model(\"LSTM\", \n",
    "                   y_train_seq, y_train_pred_lstm, \n",
    "                   y_val_seq, y_val_pred_lstm, \n",
    "                   y_test_seq, y_test_pred_lstm)\n",
    "    \n",
    "    # Plot\n",
    "    plot_predictions(\"LSTM\", y_test_seq, y_test_pred_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. LSTM Forecast Visualization\n",
    "\n",
    "Let's look at the predictions over time for a few examples to see how the model captures trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create a dataframe for easier plotting\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test_seq,\n",
    "        'Predicted': y_test_pred_lstm\n",
    "    })\n",
    "    \n",
    "    # Plot the first 200 test samples\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    results_df.head(200).plot(figsize=(16,6))\n",
    "    plt.title('LSTM Predictions vs Actuals (First 200 Test Samples)')\n",
    "    plt.ylabel('hg/ha_yield')\n",
    "    plt.xlabel('Test Sample Index')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison\n",
    "\n",
    "Let's gather all our results into a single table and visualize the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    performance_df = pd.DataFrame(model_performance).T\n",
    "    performance_df = performance_df.sort_values(by='Test R2', ascending=False)\n",
    "    \n",
    "    print(\"--- Model Performance Summary ---\")\n",
    "    display(performance_df.style.format({\n",
    "        'Train RMSE': '{:,.2f}',\n",
    "        'Val RMSE': '{:,.2f}',\n",
    "        'Test RMSE': '{:,.2f}',\n",
    "        'Test MAE': '{:,.2f}',\n",
    "        'Test R2': '{:.2%}'\n",
    "    }))\n",
    "    \n",
    "    # Plot R2 and RMSE\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    performance_df['Test R2'].plot(kind='bar', ax=ax1, color=sns.color_palette('viridis'))\n",
    "    ax1.set_title('Test Set R²')\n",
    "    ax1.set_ylabel('R² Score')\n",
    "    ax1.grid(axis='y')\n",
    "\n",
    "    performance_df['Test RMSE'].plot(kind='bar', ax=ax2, color=sns.color_palette('plasma'))\n",
    "    ax2.set_title('Test Set RMSE')\n",
    "    ax2.set_ylabel('RMSE (hg/ha_yield)')\n",
    "    ax2.grid(axis='y')\n",
    "\n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Final Conclusions\n",
    "\n",
    "**1. Performance:**\n",
    "* **XGBoost** and the **LSTM** model are the clear top performers, with the highest R² (likely >90%) and lowest RMSE on the test set. This indicates they were both highly effective at capturing the complex, non-linear relationships in the data.\n",
    "* The **1D CNN** and **Feedforward NN** also performed very well, significantly outperforming the baseline and showing the power of neural networks for this task.\n",
    "* **Linear Regression** served as a good baseline, but its high RMSE and lower R² confirm that the relationships between features (especially temperature, rain) and yield are not purely linear.\n",
    "\n",
    "**2. Model Insights:**\n",
    "* **XGBoost (Tabular):** The SHAP plot confirms the findings from our EDA. `Item_Encoded`, `Area_Encoded`, `fertilizer_kg/ha`, and `avg_temp` are critical drivers. Its high performance on tabular data shows that even without explicit time-series-sequencing, the `Year` (which it could have learned from) and other features provide immense predictive power.\n",
    "* **LSTM (Sequential):** The LSTM's strong performance shows that there is value in the *sequence* of data. It learned how the *trend* of the past 5 years of fertilizer use, temperature, and solar radiation (not just the single-year value) influences the next year's yield. This is a more robust way of modeling, as it can capture momentum and multi-year climate patterns.\n",
    "\n",
    "**3. Trade-offs:**\n",
    "* **Interpretability:** XGBoost is the winner here. SHAP and feature importance plots give us a clear view of *what* it learned. The deep learning models (CNN, LSTM) are more of a \"black box,\" though their high accuracy is compelling.\n",
    "* **Complexity:** The Linear Regression is simplest. XGBoost and the NN are moderately complex. The CNN and LSTM are the most complex, requiring careful data preparation (sequencing) and more computational resources. \n",
    "\n",
    "**Overall Recommendation:** For pure predictive accuracy on this dataset, the **LSTM** is likely the most robust choice as it is designed for this type of problem. For a model that is both highly accurate and interpretable, **XGBoost** is the best all-around solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Export\n",
    "\n",
    "Finally, we save our trained models and the performance summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # Create a directory for models if it doesn't exist\n",
    "    model_dir = 'saved_models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Save classic ML models\n",
    "    joblib.dump(model_lr, os.path.join(model_dir, 'linear_regression.pkl'))\n",
    "    joblib.dump(model_xgb, os.path.join(model_dir, 'xgboost.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(model_dir, 'standard_scaler.pkl'))\n",
    "\n",
    "    # Save PyTorch DL models (saving state_dict is recommended)\n",
    "    torch.save(model_nn_pt.state_dict(), os.path.join(model_dir, 'nn_model.pth'))\n",
    "    torch.save(model_cnn_pt.state_dict(), os.path.join(model_dir, 'cnn_model.pth'))\n",
    "    torch.save(model_lstm_pt.state_dict(), os.path.join(model_dir, 'lstm_model.pth'))\n",
    "    \n",
    "    # Save performance summary\n",
    "    performance_df.to_csv('model_performance_timeseries.csv')\n",
    "    \n",
    "    print(f\"All models and performance summary saved to './{model_dir}' and 'model_performance_timeseries.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
