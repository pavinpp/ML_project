{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Yield Prediction – Final Model Pipeline (Rev5)\n",
    "\n",
    "**Goal**: Predict `hg/ha_yield` fairly across all crops.\n",
    "\n",
    "**Key Features**:\n",
    "- Fair across crops using RMSPE optimization.\n",
    "- Time-series safe.\n",
    "- PyTorch for DL models.\n",
    "- Per-crop reporting.\n",
    "- Baseline Linear Regression.\n",
    "- Fixed alignment issues.\n",
    "- Loss history plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from scipy.signal import detrend\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"All libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (25932, 9)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"cleaned_crop_data.csv\")\n",
    "    print(f\"Loaded: {df.shape}\")\n",
    "except:\n",
    "    raise FileNotFoundError(\"Run EDA first!\")\n",
    "\n",
    "TARGET = 'hg/ha_yield'\n",
    "TIME_COL = 'Year'\n",
    "CAT_COLS = ['Area', 'Item']\n",
    "NUMERIC_COLS = ['average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'fertilizer_kg/ha', 'solar_radiation_MJ/m2-day']\n",
    "\n",
    "# 2.1 De-trend per group\n",
    "df = df.sort_values(CAT_COLS + [TIME_COL])\n",
    "df['yield_detrended'] = df.groupby(CAT_COLS)[TARGET].transform(detrend)\n",
    "df['yield_trend'] = df[TARGET] - df['yield_detrended']\n",
    "TARGET_DET = 'yield_detrended'\n",
    "\n",
    "# 2.2 Lags for ML\n",
    "LAG_COLS = [TARGET_DET] + NUMERIC_COLS\n",
    "for col in LAG_COLS:\n",
    "    for lag in [1, 2]:\n",
    "        df[f'{col}_lag{lag}'] = df.groupby(CAT_COLS)[col].shift(lag)\n",
    "df_ml = df.dropna().copy()\n",
    "\n",
    "# 2.3 Split\n",
    "TRAIN_END = 2007\n",
    "VAL_END = 2010\n",
    "train_df = df_ml[df_ml[TIME_COL] <= TRAIN_END].copy()\n",
    "val_df = df_ml[(df_ml[TIME_COL] > TRAIN_END) & (df_ml[TIME_COL] <= VAL_END)].copy()\n",
    "test_df = df_ml[df_ml[TIME_COL] > VAL_END].copy()\n",
    "\n",
    "# 2.4 Encode\n",
    "le_area = LabelEncoder().fit(df_ml['Area'])\n",
    "le_item = LabelEncoder().fit(df_ml['Item'])\n",
    "for d in [train_df, val_df, test_df]:\n",
    "    d['Area_Encoded'] = le_area.transform(d['Area'])\n",
    "    d['Item_Encoded'] = le_item.transform(d['Item'])\n",
    "\n",
    "# 2.5 Scale\n",
    "lagged_cols = [c for c in df_ml.columns if '_lag' in c]\n",
    "scale_cols = NUMERIC_COLS + lagged_cols\n",
    "scaler = StandardScaler()\n",
    "train_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])\n",
    "val_df[scale_cols] = scaler.transform(val_df[scale_cols])\n",
    "test_df[scale_cols] = scaler.transform(test_df[scale_cols])\n",
    "\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(le_area, 'le_area.joblib')\n",
    "joblib.dump(le_item, 'le_item.joblib')\n",
    "\n",
    "N_AREAS = len(le_area.classes_)\n",
    "N_ITEMS = len(le_item.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "ML_FEATS = NUMERIC_COLS + lagged_cols + ['Area_Encoded', 'Item_Encoded']\n",
    "X_train_ml = train_df[ML_FEATS]\n",
    "y_train_ml = train_df[TARGET_DET]\n",
    "X_val_ml = val_df[ML_FEATS]\n",
    "y_val_ml = val_df[TARGET_DET]\n",
    "X_test_ml = test_df[ML_FEATS]\n",
    "y_test_ml = test_df[TARGET_DET]\n",
    "\n",
    "# DL Sequences\n",
    "LOOKBACK = 5\n",
    "DL_FEATS = NUMERIC_COLS + ['Area_Encoded', 'Item_Encoded']\n",
    "\n",
    "def create_sequences(data, lookback, feats, target):\n",
    "    X, y = [], []\n",
    "    for _, group in data.groupby(CAT_COLS):\n",
    "        if len(group) < lookback:\n",
    "            continue\n",
    "        gf = group[feats].values\n",
    "        gt = group[target].values\n",
    "        for i in range(len(group) - lookback + 1):\n",
    "            X.append(gf[i:i+lookback])\n",
    "            y.append(gt[i+lookback-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "scaler_dl = StandardScaler()\n",
    "train_df_dl = df[df[TIME_COL] <= TRAIN_END].copy()\n",
    "val_df_dl = df[(df[TIME_COL] > TRAIN_END) & (df[TIME_COL] <= VAL_END)].copy()\n",
    "test_df_dl = df[df[TIME_COL] > VAL_END].copy()\n",
    "train_df_dl['Area_Encoded'] = le_area.transform(train_df_dl['Area'])\n",
    "train_df_dl['Item_Encoded'] = le_item.transform(train_df_dl['Item'])\n",
    "val_df_dl['Area_Encoded'] = le_area.transform(val_df_dl['Area'])\n",
    "val_df_dl['Item_Encoded'] = le_item.transform(val_df_dl['Item'])\n",
    "test_df_dl['Area_Encoded'] = le_area.transform(test_df_dl['Area'])\n",
    "test_df_dl['Item_Encoded'] = le_item.transform(test_df_dl['Item'])\n",
    "train_df_dl[NUMERIC_COLS] = scaler_dl.fit_transform(train_df_dl[NUMERIC_COLS])\n",
    "val_df_dl[NUMERIC_COLS] = scaler_dl.transform(val_df_dl[NUMERIC_COLS])\n",
    "test_df_dl[NUMERIC_COLS] = scaler_dl.transform(test_df_dl[NUMERIC_COLS])\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(train_df_dl, LOOKBACK, DL_FEATS, TARGET_DET)\n",
    "X_val_seq, y_val_seq = create_sequences(val_df_dl, LOOKBACK, DL_FEATS, TARGET_DET)\n",
    "X_test_seq, y_test_seq = create_sequences(test_df_dl, LOOKBACK, DL_FEATS, TARGET_DET)\n",
    "\n",
    "def split_dl(X):\n",
    "    return [\n",
    "        torch.tensor(X[..., :-2], dtype=torch.float32),\n",
    "        torch.tensor(X[..., -2], dtype=torch.long),\n",
    "        torch.tensor(X[..., -1], dtype=torch.long)\n",
    "    ]\n",
    "\n",
    "X_train_dl = split_dl(X_train_seq)\n",
    "X_val_dl = split_dl(X_val_seq)\n",
    "X_test_dl = split_dl(X_test_seq)\n",
    "\n",
    "y_train_t = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_val_t = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_t = torch.tensor(y_test_seq, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(((y_true - y_pred) / (y_true + 1e-8)) ** 2)) * 100\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optuna Objectives (Minimize RMSPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0)\n",
    "    }\n",
    "    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train_ml):\n",
    "        model.fit(X_train_ml.iloc[train_idx], y_train_ml.iloc[train_idx])\n",
    "        pred = model.predict(X_train_ml.iloc[val_idx])\n",
    "        scores.append(rmspe(y_train_ml.iloc[val_idx], pred))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(random_state=42, **params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train_ml):\n",
    "        model.fit(X_train_ml.iloc[train_idx], y_train_ml.iloc[train_idx])\n",
    "        pred = model.predict(X_train_ml.iloc[val_idx])\n",
    "        scores.append(rmspe(y_train_ml.iloc[val_idx], pred))\n",
    "    return np.mean(scores)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_areas, n_items, lstm_units, dense_units, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_area = nn.Embedding(n_areas, 10)\n",
    "        self.embed_item = nn.Embedding(n_items, 5)\n",
    "        self.lstm = nn.LSTM(len(NUMERIC_COLS) + 10 + 5, lstm_units, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(lstm_units, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "    def forward(self, num, area, item):\n",
    "        e_area = self.embed_area(area)\n",
    "        e_item = self.embed_item(item)\n",
    "        x = torch.cat([num, e_area, e_item], dim=-1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.drop(out[:, -1])\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        return self.fc2(out)\n",
    "\n",
    "def train_dl(model, opt, loss_fn, train_loader, val_loader, epochs=100, patience=10, is_final=False):\n",
    "    best = float('inf')\n",
    "    wait = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x1, x2, x3, y in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(x1, x2, x3)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs = [x.to(next(model.parameters()).device) for x in val_loader.dataset.tensors[:3]]\n",
    "            val_y = val_loader.dataset.tensors[3]\n",
    "            val_pred = model(*val_inputs)\n",
    "            val_mse = loss_fn(val_pred, val_y).item()\n",
    "            val_rmspe = rmspe(val_y.numpy().flatten(), val_pred.numpy().flatten())\n",
    "        val_losses.append(val_mse)\n",
    "        if val_rmspe < best:\n",
    "            best = val_rmspe\n",
    "            wait = 0\n",
    "            if is_final:\n",
    "                torch.save(model.state_dict(), f'model_{model.__class__.__name__}.pth')\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def objective_lstm(trial):\n",
    "    params = {\n",
    "        'lstm_units': trial.suggest_categorical('lstm_units', [64, 128]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [32, 64]),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "    lr = params.pop('lr')  # Remove lr from params\n",
    "    model = LSTMModel(N_AREAS, N_ITEMS, **params)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_ds = TensorDataset(*X_train_dl, y_train_t)\n",
    "    val_ds = TensorDataset(*X_val_dl, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    val_loss = train_dl(model, opt, nn.MSELoss(), train_loader, val_loader)\n",
    "    return val_loss\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_areas, n_items, filters, kernel, dense_units):\n",
    "        super().__init__()\n",
    "        self.embed_area = nn.Embedding(n_areas, 10)\n",
    "        self.embed_item = nn.Embedding(n_items, 5)\n",
    "        self.conv = nn.Conv1d(len(NUMERIC_COLS) + 10 + 5, filters, kernel)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(filters, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "    def forward(self, num, area, item):\n",
    "        e_area = self.embed_area(area)\n",
    "        e_item = self.embed_item(item)\n",
    "        x = torch.cat([num, e_area, e_item], dim=-1).transpose(1, 2)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def objective_cnn(trial):\n",
    "    params = {\n",
    "        'filters': trial.suggest_categorical('filters', [64, 128]),\n",
    "        'kernel': trial.suggest_categorical('kernel', [2, 3]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [32, 64]),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "    lr = params.pop('lr')  # Remove lr from params\n",
    "    model = CNNModel(N_AREAS, N_ITEMS, **params)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_ds = TensorDataset(*X_train_dl, y_train_t)\n",
    "    val_ds = TensorDataset(*X_val_dl, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    val_loss = train_dl(model, opt, nn.MSELoss(), train_loader, val_loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 20:29:28,486] A new study created in memory with name: no-name-bf130ce2-58b3-4892-905a-e8303ea6650c\n",
      "Best trial: 0. Best value: 1548.42:   2%|▏         | 1/50 [00:07<05:45,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 20:29:35,530] Trial 0 finished with value: 1548.416938512844 and parameters: {'n_estimators': 213, 'max_depth': 9, 'min_samples_leaf': 10, 'max_features': 0.8540927686223944}. Best is trial 0 with value: 1548.416938512844.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1548.42:   4%|▍         | 2/50 [00:09<03:41,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-16 20:29:38,461] Trial 1 finished with value: 1790.7367591298196 and parameters: {'n_estimators': 55, 'max_depth': 23, 'min_samples_leaf': 10, 'max_features': 0.7936716811277902}. Best is trial 0 with value: 1548.416938512844.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 1548.42:   4%|▍         | 2/50 [00:18<07:19,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-11-16 20:29:46,724] Trial 2 failed with parameters: {'n_estimators': 324, 'max_depth': 29, 'min_samples_leaf': 6, 'max_features': 0.8889283298692876} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Temp\\ipykernel_8104\\1710434768.py\", line 12, in objective_rf\n",
      "    model.fit(X_train_ml.iloc[train_idx], y_train_ml.iloc[train_idx])\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py\", line 486, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py\", line 82, in __call__\n",
      "    return super().__call__(iterable_with_config_and_warning_filters)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 2072, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1682, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 1800, in _retrieve\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-16 20:29:46,792] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m n_trials = \u001b[32m50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mRF\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mXGB\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m30\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m completed_trials = study.get_trials(deepcopy=\u001b[38;5;28;01mFalse\u001b[39;00m, states=(optuna.trial.TrialState.COMPLETE,))\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(completed_trials) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mobjective_rf\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     10\u001b[39m scores = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, val_idx \u001b[38;5;129;01min\u001b[39;00m tscv.split(X_train_ml):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     pred = model.predict(X_train_ml.iloc[val_idx])\n\u001b[32m     14\u001b[39m     scores.append(rmspe(y_train_ml.iloc[val_idx], pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_tuning = True\n",
    "best_params = {}\n",
    "if run_tuning:\n",
    "    for name, obj in [('RF', objective_rf), ('XGB', objective_xgb), ('LSTM', objective_lstm), ('CNN', objective_cnn)]:\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        n_trials = 50 if name in ['RF', 'XGB'] else 30\n",
    "        study.optimize(obj, n_trials=n_trials, show_progress_bar=True)\n",
    "        completed_trials = study.get_trials(deepcopy=False, states=(optuna.trial.TrialState.COMPLETE,))\n",
    "        if len(completed_trials) > 0:\n",
    "            best_params[name] = study.best_params\n",
    "            print(f\"{name} best RMSPE: {study.best_value:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Warning: No completed trials for {name}. Skipping best_params entry.\")\n",
    "    joblib.dump(best_params, 'best_params_rmspe.joblib')\n",
    "else:\n",
    "    best_params = joblib.load('best_params_rmspe.joblib')\n",
    "    print(\"Loaded pre-tuned params.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LSTM'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m train_loader = DataLoader(train_full_ds, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m test_loader = DataLoader(test_ds, batch_size=\u001b[32m64\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m model_lstm = LSTMModel(N_AREAS, N_ITEMS, **\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLSTM\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m).to(device)\n\u001b[32m     31\u001b[39m opt_lstm = optim.Adam(model_lstm.parameters(), lr=best_params[\u001b[33m'\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     32\u001b[39m train_losses_lstm, val_losses_lstm = train_dl(model_lstm, opt_lstm, nn.MSELoss(), train_loader, test_loader, epochs=\u001b[32m150\u001b[39m, patience=\u001b[32m15\u001b[39m, is_final=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyError\u001b[39m: 'LSTM'"
     ]
    }
   ],
   "source": [
    "# Combine train+val\n",
    "X_train_full_ml = pd.concat([X_train_ml, X_val_ml])\n",
    "y_train_full_ml = pd.concat([y_train_ml, y_val_ml])\n",
    "X_train_full_seq = np.concatenate([X_train_seq, X_val_seq])\n",
    "y_train_full_seq = np.concatenate([y_train_seq, y_val_seq])\n",
    "X_train_full_dl = split_dl(X_train_full_seq)\n",
    "y_train_full_t = torch.tensor(y_train_full_seq, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Models\n",
    "models = {}\n",
    "model_rf = RandomForestRegressor(random_state=42, n_jobs=-1, **best_params['RF'])\n",
    "model_rf.fit(X_train_full_ml, y_train_full_ml)\n",
    "models['RF'] = model_rf\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(random_state=42, **best_params['XGB'])\n",
    "model_xgb.fit(X_train_full_ml, y_train_full_ml)\n",
    "models['XGB'] = model_xgb\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train_full_ml, y_train_full_ml)\n",
    "models['LR'] = model_lr\n",
    "\n",
    "# DL\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_full_ds = TensorDataset(*[x.to(device) for x in X_train_full_dl], y_train_full_t.to(device))\n",
    "test_ds = TensorDataset(*[x.to(device) for x in X_test_dl], y_test_t.to(device))\n",
    "train_loader = DataLoader(train_full_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "model_lstm = LSTMModel(N_AREAS, N_ITEMS, **best_params['LSTM']).to(device)\n",
    "opt_lstm = optim.Adam(model_lstm.parameters(), lr=best_params['LSTM']['lr'])\n",
    "train_losses_lstm, val_losses_lstm = train_dl(model_lstm, opt_lstm, nn.MSELoss(), train_loader, test_loader, epochs=150, patience=15, is_final=True)\n",
    "models['LSTM'] = model_lstm\n",
    "\n",
    "model_cnn = CNNModel(N_AREAS, N_ITEMS, **best_params['CNN']).to(device)\n",
    "opt_cnn = optim.Adam(model_cnn.parameters(), lr=best_params['CNN']['lr'])\n",
    "train_losses_cnn, val_losses_cnn = train_dl(model_cnn, opt_cnn, nn.MSELoss(), train_loader, test_loader, epochs=150, patience=15, is_final=True)\n",
    "models['CNN'] = model_cnn\n",
    "\n",
    "joblib.dump(model_rf, 'model_rf.joblib')\n",
    "joblib.dump(model_xgb, 'model_xgb.joblib')\n",
    "joblib.dump(model_lr, 'model_lr.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot DL Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
    "ax1.plot(train_losses_lstm, label='Train Loss')\n",
    "ax1.plot(val_losses_lstm, label='Validation (Test) Loss')\n",
    "ax1.set_title('LSTM Model Loss', fontsize=16)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Mean Squared Error')\n",
    "ax1.legend()\n",
    "ax2.plot(train_losses_cnn, label='Train Loss')\n",
    "ax2.plot(val_losses_cnn, label='Validation (Test) Loss')\n",
    "ax2.set_title('CNN Model Loss', fontsize=16)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Squared Error')\n",
    "ax2.legend()\n",
    "plt.suptitle('Deep Learning Training Curves', fontsize=20)\n",
    "plt.savefig(\"loss_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align test sets\n",
    "matched_test_df = pd.concat([group.iloc[LOOKBACK-1:] for _, group in test_df_dl.groupby(CAT_COLS) if len(group) >= LOOKBACK])\n",
    "matched_test_ml_df = pd.concat([group.iloc[LOOKBACK-1:] for _, group in test_df.groupby(CAT_COLS) if len(group) >= LOOKBACK])\n",
    "X_test_ml_matched = matched_test_ml_df[ML_FEATS]\n",
    "X_test_dl_matched, y_test_det_matched = create_sequences(matched_test_df, 1, DL_FEATS, TARGET_DET)  # LOOKBACK=1 to get single steps\n",
    "y_test_det_matched = y_test_det_matched.flatten()  # Since LOOKBACK=1\n",
    "trend_test = matched_test_df['yield_trend'].values\n",
    "y_true_original = matched_test_df[TARGET].values\n",
    "\n",
    "# Predictions\n",
    "test_preds = {}\n",
    "for name in ['LR', 'RF', 'XGB']:\n",
    "    test_preds[name] = models[name].predict(X_test_ml_matched)\n",
    "\n",
    "X_test_dl_inputs_m = split_dl(X_test_seq)  # Original sequences for DL\n",
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds['LSTM'] = model_lstm(*[x.to(device) for x in X_test_dl_inputs_m]).cpu().numpy().flatten()\n",
    "model_cnn.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds['CNN'] = model_cnn(*[x.to(device) for x in X_test_dl_inputs_m]).cpu().numpy().flatten()\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "y_preds_original = {}\n",
    "for name, pred_det in test_preds.items():\n",
    "    pred_orig = pred_det + trend_test\n",
    "    y_preds_original[name] = pred_orig\n",
    "    mae = mean_absolute_error(y_true_original, pred_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_original, pred_orig))\n",
    "    map_e = mape(y_true_original, pred_orig)\n",
    "    rms_pe = rmspe(y_true_original, pred_orig)\n",
    "    r_2 = r2_score(y_true_original, pred_orig)\n",
    "    results.append({'Model': name, 'MAE': mae, 'RMSE': rmse, 'MAPE (%)': map_e, 'RMSPE (%)': rms_pe, 'R²': r_2})\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model').sort_values('RMSPE (%)')\n",
    "print(\"\\n--- Final Performance (Test Set) ---\")\n",
    "print(results_df.round(2))\n",
    "results_df.to_csv(\"final_model_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "sns.barplot(data=results_df.reset_index(), x='Model', y='RMSE', ax=axs[0])\n",
    "axs[0].set_title('RMSE Comparison')\n",
    "sns.barplot(x='Model', y='MAE', data=results_df.reset_index(), ax=axs[1])\n",
    "axs[1].set_title('MAE Comparison')\n",
    "sns.barplot(x='Model', y='MAPE (%)', data=results_df.reset_index(), ax=axs[2])\n",
    "axs[2].set_title('MAPE Comparison')\n",
    "sns.barplot(x='Model', y='R²', data=results_df.reset_index(), ax=axs[3])\n",
    "axs[3].set_title('R² Comparison')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_performance_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Per-Crop Reporting (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.index[0]\n",
    "print(f\"Per-crop report for best model: {best_model_name}\")\n",
    "crop_results = []\n",
    "items = matched_test_df['Item'].values\n",
    "for crop in np.unique(items):\n",
    "    mask = items == crop\n",
    "    true = y_true_original[mask]\n",
    "    pred = y_preds_original[best_model_name][mask]\n",
    "    crop_results.append({\n",
    "        'Crop': crop,\n",
    "        'RMSPE (%)': rmspe(true, pred),\n",
    "        'MAPE (%)': mape(true, pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(true, pred)),\n",
    "        'R²': r2_score(true, pred)\n",
    "    })\n",
    "crop_df = pd.DataFrame(crop_results).sort_values('RMSPE (%)')\n",
    "print(crop_df.round(2))\n",
    "crop_df.to_csv('per_crop_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SHAP Analysis (If Tree Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.index[0]\n",
    "best_model = models[best_model_name]\n",
    "if best_model_name in ['RF', 'XGB']:\n",
    "    print(f\"Running SHAP on {best_model_name}\")\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test_ml_matched)\n",
    "    shap.summary_plot(shap_values, X_test_ml_matched, plot_type=\"beeswarm\", show=False)\n",
    "    plt.title(f\"SHAP Beeswarm ({best_model_name})\", fontsize=16)\n",
    "    plt.savefig(\"shap_beeswarm.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    shap.summary_plot(shap_values, X_test_ml_matched, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"Feature Importance ({best_model_name})\", fontsize=16)\n",
    "    plt.savefig(\"shap_importance.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP skipped for non-tree model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = matched_test_df.copy()\n",
    "final_predictions_df['true_yield_original'] = y_true_original\n",
    "for name in test_preds:\n",
    "    final_predictions_df[f'predicted_{name}'] = y_preds_original[name]\n",
    "export_cols = ['Year', 'Area', 'Item', 'true_yield_original'] + [f'predicted_{name}' for name in test_preds]\n",
    "final_predictions_df[export_cols].to_csv(\"final_test_predictions.csv\", index=False)\n",
    "print(\"Exported predictions.\")\n",
    "print(\"\\n--- Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
