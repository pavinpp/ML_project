{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Optuna for hyperparameter tuning\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"Optuna imported successfully.\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not installed. Install with `pip install optuna` to use tuning.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21a9c1",
   "metadata": {},
   "source": [
    "# LSTM Regression for Y_rice\n",
    "\n",
    "This notebook trains an LSTM model (PyTorch) on tabular data with target **Y_rice**, \n",
    "adds prediction vs. true plots, and (optionally) performs Optuna hyperparameter tuning.\n",
    "\n",
    "> ðŸ”§ **Note:** Replace the data-loading part below with your actual dataset if needed, but keep the variables `X_train_scaled`, `X_val_scaled`, `X_test_scaled`, `y_train`, `y_val`, `y_test` in the same format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "RANDOM_STATE = 42\n",
    "VAL_SIZE     = 0.15\n",
    "TEST_SIZE    = 0.15\n",
    "BATCH_SIZE   = 64\n",
    "\n",
    "# === DATA LOADING (EXAMPLE) ===\n",
    "# TODO: Replace this section with your own data-loading code if you already prepared\n",
    "# X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test elsewhere.\n",
    "\n",
    "# Example assumes a CSV file with 'Y_rice' as target and the rest as features.\n",
    "# COMMENT OUT this block if you already have the scaled splits in memory.\n",
    "\n",
    "csv_path = \"your_data.csv\"  # <- change this\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    assert \"Y_rice\" in df.columns, \"Target column 'Y_rice' not found. Adjust the name.\"\n",
    "    X = df.drop(columns=[\"Y_rice\"]).values.astype(np.float32)\n",
    "    y = df[\"Y_rice\"].values.astype(np.float32)\n",
    "\n",
    "    # Train/val/test split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "    val_ratio = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Scale features (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Data loaded from:\", csv_path)\n",
    "    print(\"Shapes:\")\n",
    "    print(\"X_train_scaled:\", X_train_scaled.shape)\n",
    "    print(\"X_val_scaled:  \", X_val_scaled.shape)\n",
    "    print(\"X_test_scaled: \", X_test_scaled.shape)\n",
    "    print(\"y_train:\", y_train.shape, \"y_val:\", y_val.shape, \"y_test:\", y_test.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n[!] CSV file not found at\", csv_path)\n",
    "    print(\"If you already have X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\")\n",
    "    print(\"defined in your environment, you can ignore this block.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae2893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SANITY CHECKS ===\n",
    "def check_array(name, arr):\n",
    "    arr_np = np.array(arr)\n",
    "    print(f\"{name}: NaN={np.isnan(arr_np).any()}, inf={np.isinf(arr_np).any()}, \"\n",
    "          f\"shape={arr_np.shape}\")\n",
    "    if arr_np.size > 0 and np.isfinite(arr_np).any():\n",
    "        print(f\"    min={np.nanmin(arr_np):.4f}, max={np.nanmax(arr_np):.4f}\")\n",
    "\n",
    "check_array(\"X_train_scaled\", X_train_scaled)\n",
    "check_array(\"X_val_scaled\",   X_val_scaled)\n",
    "check_array(\"X_test_scaled\",  X_test_scaled)\n",
    "check_array(\"y_train\",        y_train)\n",
    "check_array(\"y_val\",          y_val)\n",
    "check_array(\"y_test\",         y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TORCH DATASETS & DATALOADERS ===\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val_scaled,   dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_scaled,  dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.float32).view(-1, 1)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# For LSTM, we treat each sample as sequence length 1: (batch, seq_len=1, input_size)\n",
    "def make_seq(x):\n",
    "    return x.unsqueeze(1)  # (batch, 1, features)\n",
    "\n",
    "train_ds = TensorDataset(make_seq(X_train_t), y_train_t)\n",
    "val_ds   = TensorDataset(make_seq(X_val_t),   y_val_t)\n",
    "test_ds  = TensorDataset(make_seq(X_test_t),  y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "input_size = X_train_t.shape[1]\n",
    "print(\"Input feature size:\", input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL DEFINITION ===\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len=1, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        # take last time step\n",
    "        last = out[:, -1, :]  # (batch, hidden_size)\n",
    "        out = self.fc(last)\n",
    "        return out\n",
    "\n",
    "def create_model(hidden_size=64, num_layers=2, dropout=0.1, lr=1e-3):\n",
    "    model = LSTMRegressor(input_size, hidden_size, num_layers, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    criterion = nn.MSELoss()\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e844d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING & EVALUATION UTILITIES ===\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            out = model(xb)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "    preds = np.vstack(preds).ravel()\n",
    "    trues = np.vstack(trues).ravel()\n",
    "    return preds, trues\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs=100,\n",
    "    clip_norm=1.0,\n",
    "):\n",
    "    history = {\n",
    "        \"train_rmse\": [],\n",
    "        \"val_rmse\":   [],\n",
    "        \"train_r2\":   [],\n",
    "        \"val_r2\":     [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"[Epoch {epoch}] NaN loss detected, breaking.\")\n",
    "                return history\n",
    "            loss.backward()\n",
    "            if clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Evaluation\n",
    "        train_pred, train_true = evaluate_model(model, train_loader)\n",
    "        val_pred,   val_true   = evaluate_model(model, val_loader)\n",
    "\n",
    "        train_rmse_val = rmse(train_true, train_pred)\n",
    "        val_rmse_val   = rmse(val_true, val_pred)\n",
    "        train_r2_val   = r2_score(train_true, train_pred)\n",
    "        val_r2_val     = r2_score(val_true, val_pred)\n",
    "\n",
    "        history[\"train_rmse\"].append(train_rmse_val)\n",
    "        history[\"val_rmse\"].append(val_rmse_val)\n",
    "        history[\"train_r2\"].append(train_r2_val)\n",
    "        history[\"val_r2\"].append(val_r2_val)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{n_epochs} - \"\n",
    "            f\"Train RMSE: {train_rmse_val:.4f}, Val RMSE: {val_rmse_val:.4f}, \"\n",
    "            f\"Train R2: {train_r2_val:.4f}, Val R2: {val_r2_val:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168849ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINE LSTM TRAINING ===\n",
    "BASE_HIDDEN_SIZE = 64\n",
    "BASE_NUM_LAYERS  = 2\n",
    "BASE_DROPOUT     = 0.1\n",
    "BASE_LR          = 1e-3\n",
    "BASE_EPOCHS      = 100\n",
    "\n",
    "baseline_model, baseline_opt, baseline_crit = create_model(\n",
    "    hidden_size=BASE_HIDDEN_SIZE,\n",
    "    num_layers=BASE_NUM_LAYERS,\n",
    "    dropout=BASE_DROPOUT,\n",
    "    lr=BASE_LR,\n",
    ")\n",
    "\n",
    "baseline_history = train_model(\n",
    "    baseline_model,\n",
    "    baseline_opt,\n",
    "    baseline_crit,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs=BASE_EPOCHS,\n",
    "    clip_norm=1.0,\n",
    ")\n",
    "\n",
    "# Evaluate on val & test\n",
    "val_pred,  val_true  = evaluate_model(baseline_model, val_loader)\n",
    "test_pred, test_true = evaluate_model(baseline_model, test_loader)\n",
    "\n",
    "baseline_val_rmse = rmse(val_true, val_pred)\n",
    "baseline_val_r2   = r2_score(val_true, val_pred)\n",
    "baseline_test_rmse = rmse(test_true, test_pred)\n",
    "baseline_test_r2   = r2_score(test_true, test_pred)\n",
    "\n",
    "print(\"\\nBaseline LSTM performance:\")\n",
    "print(f\"Val  RMSE: {baseline_val_rmse:.4f}, R2: {baseline_val_r2:.4f}\")\n",
    "print(f\"Test RMSE: {baseline_test_rmse:.4f}, R2: {baseline_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcf9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOTS: LOSS CURVES (RMSE) ===\n",
    "epochs = range(1, len(baseline_history[\"train_rmse\"]) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, baseline_history[\"train_rmse\"], label=\"Train RMSE\")\n",
    "plt.plot(epochs, baseline_history[\"val_rmse\"],   label=\"Val RMSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LSTM Training vs Validation RMSE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, baseline_history[\"train_r2\"], label=\"Train R2\")\n",
    "plt.plot(epochs, baseline_history[\"val_r2\"],   label=\"Val R2\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RÂ²\")\n",
    "plt.title(\"LSTM Training vs Validation RÂ²\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOTS: TRUE vs PREDICTED ===\n",
    "def plot_scatter(y_true, y_pred, title):\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')\n",
    "    plt.xlabel(\"True Y_rice\")\n",
    "    plt.ylabel(\"Predicted Y_rice\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter(val_true,  val_pred,  \"Validation: True vs Predicted Y_rice (LSTM)\")\n",
    "plot_scatter(test_true, test_pred, \"Test: True vs Predicted Y_rice (LSTM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf345d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTUNA HYPERPARAMETER TUNING (OPTIONAL) ===\n",
    "if not OPTUNA_AVAILABLE:\n",
    "    print(\"Optuna not available, skipping tuning. Install with `pip install optuna`.\")\n",
    "else:\n",
    "    def objective(trial):\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, log=True)\n",
    "        num_layers  = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "        lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        epochs      = trial.suggest_int(\"epochs\", 40, 100)\n",
    "\n",
    "        model, optimizer, criterion = create_model(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        history = train_model(\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            n_epochs=epochs,\n",
    "            clip_norm=1.0,\n",
    "        )\n",
    "\n",
    "        # use last val RMSE as objective\n",
    "        val_rmse_hist = history[\"val_rmse\"]\n",
    "        if len(val_rmse_hist) == 0:\n",
    "            return float(\"inf\")\n",
    "        return val_rmse_hist[-1]\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\nBest trial:\")\n",
    "    print(study.best_trial)\n",
    "\n",
    "    # Retrain best model\n",
    "    best_params = study.best_trial.params\n",
    "    best_model, best_opt, best_crit = create_model(\n",
    "        hidden_size=best_params[\"hidden_size\"],\n",
    "        num_layers=best_params[\"num_layers\"],\n",
    "        dropout=best_params[\"dropout\"],\n",
    "        lr=best_params[\"lr\"],\n",
    "    )\n",
    "    best_history = train_model(\n",
    "        best_model,\n",
    "        best_opt,\n",
    "        best_crit,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        n_epochs=best_params[\"epochs\"],\n",
    "        clip_norm=1.0,\n",
    "    )\n",
    "\n",
    "    best_val_pred,  best_val_true  = evaluate_model(best_model, val_loader)\n",
    "    best_test_pred, best_test_true = evaluate_model(best_model, test_loader)\n",
    "\n",
    "    best_val_rmse  = rmse(best_val_true,  best_val_pred)\n",
    "    best_val_r2    = r2_score(best_val_true,  best_val_pred)\n",
    "    best_test_rmse = rmse(best_test_true, best_test_pred)\n",
    "    best_test_r2   = r2_score(best_test_true, best_test_pred)\n",
    "\n",
    "    print(\"\\nBest LSTM (Optuna) performance:\")\n",
    "    print(f\"Val  RMSE: {best_val_rmse:.4f}, R2: {best_val_r2:.4f}\")\n",
    "    print(f\"Test RMSE: {best_test_rmse:.4f}, R2: {best_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16028cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PERFORMANCE COMPARISON & OPTUNA VISUALIZATION ===\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    \"Model\": \"Baseline_LSTM\",\n",
    "    \"Val_RMSE\": baseline_val_rmse,\n",
    "    \"Val_R2\":   baseline_val_r2,\n",
    "    \"Test_RMSE\": baseline_test_rmse,\n",
    "    \"Test_R2\":   baseline_test_r2,\n",
    "})\n",
    "\n",
    "# If you have metrics from your initial feed-forward NN, put them here:\n",
    "# initial_nn_val_rmse = ...\n",
    "# initial_nn_val_r2   = ...\n",
    "# initial_nn_test_rmse = ...\n",
    "# initial_nn_test_r2   = ...\n",
    "# results.append({\n",
    "#     \"Model\": \"Initial_NN\",\n",
    "#     \"Val_RMSE\": initial_nn_val_rmse,\n",
    "#     \"Val_R2\":   initial_nn_val_r2,\n",
    "#     \"Test_RMSE\": initial_nn_test_rmse,\n",
    "#     \"Test_R2\":   initial_nn_test_r2,\n",
    "# })\n",
    "\n",
    "if OPTUNA_AVAILABLE and 'best_val_rmse' in globals():\n",
    "    results.append({\n",
    "        \"Model\": \"Best_LSTM_Optuna\",\n",
    "        \"Val_RMSE\": best_val_rmse,\n",
    "        \"Val_R2\":   best_val_r2,\n",
    "        \"Test_RMSE\": best_test_rmse,\n",
    "        \"Test_R2\":   best_test_r2,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Bar plots for RMSE and R2 comparison\n",
    "plt.figure()\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Val_RMSE\"])\n",
    "plt.ylabel(\"Val RMSE\")\n",
    "plt.title(\"Model Comparison - Validation RMSE\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Val_R2\"])\n",
    "plt.ylabel(\"Val RÂ²\")\n",
    "plt.title(\"Model Comparison - Validation RÂ²\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optuna visualizations (if available)\n",
    "if OPTUNA_AVAILABLE and 'study' in globals():\n",
    "    try:\n",
    "        from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "        fig1 = plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "\n",
    "        fig2 = plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(\"Could not generate Optuna visualizations:\", e)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
