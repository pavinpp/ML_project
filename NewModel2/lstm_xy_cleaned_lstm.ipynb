{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Optuna for hyperparameter tuning\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"Optuna imported successfully.\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not installed. Install with `pip install optuna` to use tuning.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07ae98",
   "metadata": {},
   "source": [
    "# LSTM Regression for **Y_rice** with NaN Cleaning\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads your dataset from `xy_cleaned.csv`\n",
    "2. **Cleans NaNs**:\n",
    "   - Drops rows where `Y_rice` (target) is missing  \n",
    "   - Drops feature columns with more than 50% missing values  \n",
    "   - Imputes remaining NaNs in features with the **median**\n",
    "3. Splits data into **train / validation / test** with sizes `3449 / 570 / 570`\n",
    "4. Scales features using `StandardScaler`\n",
    "5. Trains an **LSTM regression** model in PyTorch\n",
    "6. Plots training curves and **True vs Predicted** scatter\n",
    "7. (Optionally) runs **Optuna** hyperparameter tuning and compares performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc88943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATA LOADING & CLEANING ===\n",
    "\n",
    "csv_path = \"xy_cleaned.csv\"  # Make sure this file is in the same folder as the notebook\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Original shape:\", df.shape)\n",
    "\n",
    "target_col = \"Y_rice\"\n",
    "\n",
    "# 1) Drop rows where target is NaN\n",
    "df = df.dropna(subset=[target_col]).copy()\n",
    "print(\"After dropping NaN Y_rice rows:\", df.shape)\n",
    "\n",
    "# 2) Define feature columns: all except Y_* targets, but keep 'year' and yield features\n",
    "feature_cols = [c for c in df.columns if c != target_col and not c.startswith(\"Y_\")]\n",
    "print(\"Initial feature count:\", len(feature_cols))\n",
    "\n",
    "# 3) Drop columns with > 50% missing values\n",
    "nan_counts = df[feature_cols].isna().sum()\n",
    "missing_frac = nan_counts / len(df)\n",
    "drop_cols = missing_frac[missing_frac > 0.5].index.tolist()\n",
    "print(\"Dropping columns with >50% NaN:\", len(drop_cols))\n",
    "\n",
    "feature_cols_kept = [c for c in feature_cols if c not in drop_cols]\n",
    "print(\"Remaining feature count:\", len(feature_cols_kept))\n",
    "\n",
    "X_raw = df[feature_cols_kept]\n",
    "y = df[target_col].values.astype(np.float32)\n",
    "\n",
    "# 4) Impute remaining NaNs in features with median\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imputer.fit_transform(X_raw)\n",
    "\n",
    "print(\"Any NaN in X_imputed:\", np.isnan(X_imputed).any())\n",
    "print(\"Any NaN in y:\", np.isnan(y).any())\n",
    "\n",
    "print(\"X_imputed shape:\", X_imputed.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32636c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN / VAL / TEST SPLIT + SCALING ===\n",
    "\n",
    "X = X_imputed.astype(np.float32)\n",
    "\n",
    "# We want exact sizes: 3449 train, 570 val, 570 test (total 4589 rows)\n",
    "n_total = X.shape[0]\n",
    "print(\"Total rows after cleaning:\", n_total)\n",
    "\n",
    "# Compute fractions to match 570 / 4589\n",
    "test_size = 570 / n_total\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=42\n",
    ")\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "val_size = 570 / X_temp.shape[0]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_size, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Val size:\", X_val.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "def check_array(name, arr):\n",
    "    arr_np = np.array(arr)\n",
    "    print(f\"{name}: NaN={np.isnan(arr_np).any()}, inf={np.isinf(arr_np).any()}, \"\n",
    "          f\"shape={arr_np.shape}\")\n",
    "    if arr_np.size > 0 and np.isfinite(arr_np).any():\n",
    "        print(f\"    min={np.nanmin(arr_np):.4f}, max={np.nanmax(arr_np):.4f}\")\n",
    "\n",
    "print(\"\\n=== Sanity checks after scaling ===\")\n",
    "check_array(\"X_train_scaled\", X_train_scaled)\n",
    "check_array(\"X_val_scaled\",   X_val_scaled)\n",
    "check_array(\"X_test_scaled\",  X_test_scaled)\n",
    "check_array(\"y_train\",        y_train)\n",
    "check_array(\"y_val\",          y_val)\n",
    "check_array(\"y_test\",         y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TORCH DATASETS & DATALOADERS ===\n",
    "\n",
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val_scaled,   dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_scaled,  dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.float32).view(-1, 1)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# For LSTM: treat each row as sequence length 1\n",
    "def make_seq(x):\n",
    "    return x.unsqueeze(1)  # (batch, 1, features)\n",
    "\n",
    "train_ds = TensorDataset(make_seq(X_train_t), y_train_t)\n",
    "val_ds   = TensorDataset(make_seq(X_val_t),   y_val_t)\n",
    "test_ds  = TensorDataset(make_seq(X_test_t),  y_test_t)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "input_size = X_train_t.shape[1]\n",
    "print(\"Input feature size:\", input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL DEFINITION ===\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len=1, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]  # (batch, hidden_size)\n",
    "        out = self.fc(last)\n",
    "        return out\n",
    "\n",
    "def create_model(hidden_size=64, num_layers=2, dropout=0.1, lr=1e-3):\n",
    "    model = LSTMRegressor(input_size, hidden_size, num_layers, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    criterion = nn.MSELoss()\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING & EVALUATION UTILITIES ===\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=np.float64).ravel()\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64).ravel()\n",
    "\n",
    "    print(\n",
    "        \"RMSE debug -> \"\n",
    "        f\"y_true NaN:{np.isnan(y_true).any()} Inf:{np.isinf(y_true).any()} | \"\n",
    "        f\"y_pred NaN:{np.isnan(y_pred).any()} Inf:{np.isinf(y_pred).any()}\"\n",
    "    )\n",
    "\n",
    "    if not np.all(np.isfinite(y_true)) or not np.all(np.isfinite(y_pred)):\n",
    "        raise ValueError(\"Non-finite values in y_true / y_pred (NaN or Inf).\")\n",
    "\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            out = model(xb)\n",
    "            preds.append(out.cpu().numpy())\n",
    "            trues.append(yb.cpu().numpy())\n",
    "    preds = np.vstack(preds).ravel()\n",
    "    trues = np.vstack(trues).ravel()\n",
    "    return preds, trues\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs=100,\n",
    "    clip_norm=1.0,\n",
    "):\n",
    "    history = {\n",
    "        \"train_rmse\": [],\n",
    "        \"val_rmse\":   [],\n",
    "        \"train_r2\":   [],\n",
    "        \"val_r2\":     [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"[Epoch {epoch}] NaN/Inf loss detected, aborting training.\")\n",
    "                return history\n",
    "\n",
    "            loss.backward()\n",
    "            if clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        train_pred, train_true = evaluate_model(model, train_loader)\n",
    "        val_pred,   val_true   = evaluate_model(model, val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"[Epoch {epoch}] eval debug -> \"\n",
    "            f\"train_pred NaN:{np.isnan(train_pred).any()} Inf:{np.isinf(train_pred).any()} | \"\n",
    "            f\"val_pred NaN:{np.isnan(val_pred).any()} Inf:{np.isinf(val_pred).any()}\"\n",
    "        )\n",
    "\n",
    "        train_rmse_val = rmse(train_true, train_pred)\n",
    "        val_rmse_val   = rmse(val_true, val_pred)\n",
    "        train_r2_val   = r2_score(train_true, train_pred)\n",
    "        val_r2_val     = r2_score(val_true, val_pred)\n",
    "\n",
    "        history[\"train_rmse\"].append(train_rmse_val)\n",
    "        history[\"val_rmse\"].append(val_rmse_val)\n",
    "        history[\"train_r2\"].append(train_r2_val)\n",
    "        history[\"val_r2\"].append(val_r2_val)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{n_epochs} - \"\n",
    "            f\"Train RMSE: {train_rmse_val:.4f}, Val RMSE: {val_rmse_val:.4f}, \"\n",
    "            f\"Train R2: {train_r2_val:.4f}, Val R2: {val_r2_val:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e469b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BASELINE LSTM TRAINING ===\n",
    "\n",
    "BASE_HIDDEN_SIZE = 64\n",
    "BASE_NUM_LAYERS  = 2\n",
    "BASE_DROPOUT     = 0.1\n",
    "BASE_LR          = 1e-3\n",
    "BASE_EPOCHS      = 100\n",
    "\n",
    "baseline_model, baseline_opt, baseline_crit = create_model(\n",
    "    hidden_size=BASE_HIDDEN_SIZE,\n",
    "    num_layers=BASE_NUM_LAYERS,\n",
    "    dropout=BASE_DROPOUT,\n",
    "    lr=BASE_LR,\n",
    ")\n",
    "\n",
    "baseline_history = train_model(\n",
    "    baseline_model,\n",
    "    baseline_opt,\n",
    "    baseline_crit,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs=BASE_EPOCHS,\n",
    "    clip_norm=1.0,\n",
    ")\n",
    "\n",
    "# Evaluate on val & test\n",
    "val_pred,  val_true  = evaluate_model(baseline_model, val_loader)\n",
    "test_pred, test_true = evaluate_model(baseline_model, test_loader)\n",
    "\n",
    "print(\"\\nFinal eval debug:\")\n",
    "print(\"val_true NaN:\", np.isnan(val_true).any(), \"Inf:\", np.isinf(val_true).any())\n",
    "print(\"val_pred NaN:\", np.isnan(val_pred).any(), \"Inf:\", np.isinf(val_pred).any())\n",
    "print(\"test_true NaN:\", np.isnan(test_true).any(), \"Inf:\", np.isinf(test_true).any())\n",
    "print(\"test_pred NaN:\", np.isnan(test_pred).any(), \"Inf:\", np.isinf(test_pred).any())\n",
    "\n",
    "baseline_val_rmse  = rmse(val_true,  val_pred)\n",
    "baseline_val_r2    = r2_score(val_true,  val_pred)\n",
    "baseline_test_rmse = rmse(test_true, test_pred)\n",
    "baseline_test_r2   = r2_score(test_true, test_pred)\n",
    "\n",
    "print(\"\\nBaseline LSTM performance:\")\n",
    "print(f\"Val  RMSE: {baseline_val_rmse:.4f}, R2: {baseline_val_r2:.4f}\")\n",
    "print(f\"Test RMSE: {baseline_test_rmse:.4f}, R2: {baseline_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOTS: LOSS CURVES (RMSE & R²) ===\n",
    "\n",
    "epochs = range(1, len(baseline_history[\"train_rmse\"]) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, baseline_history[\"train_rmse\"], label=\"Train RMSE\")\n",
    "plt.plot(epochs, baseline_history[\"val_rmse\"],   label=\"Val RMSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"LSTM Training vs Validation RMSE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, baseline_history[\"train_r2\"], label=\"Train R²\")\n",
    "plt.plot(epochs, baseline_history[\"val_r2\"],   label=\"Val R²\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"R²\")\n",
    "plt.title(\"LSTM Training vs Validation R²\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PLOTS: TRUE vs PREDICTED ===\n",
    "\n",
    "def plot_scatter(y_true, y_pred, title):\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    y_pred = np.array(y_pred).ravel()\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')\n",
    "    plt.xlabel(\"True Y_rice\")\n",
    "    plt.ylabel(\"Predicted Y_rice\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter(val_true,  val_pred,  \"Validation: True vs Predicted Y_rice (LSTM)\")\n",
    "plot_scatter(test_true, test_pred, \"Test: True vs Predicted Y_rice (LSTM)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTUNA HYPERPARAMETER TUNING (OPTIONAL) ===\n",
    "\n",
    "if not OPTUNA_AVAILABLE:\n",
    "    print(\"Optuna not available, skipping tuning. Install with `pip install optuna`.\")\n",
    "else:\n",
    "    def objective(trial):\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, log=True)\n",
    "        num_layers  = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "        lr          = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        epochs      = trial.suggest_int(\"epochs\", 40, 100)\n",
    "\n",
    "        model, optimizer, criterion = create_model(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        history = train_model(\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            n_epochs=epochs,\n",
    "            clip_norm=1.0,\n",
    "        )\n",
    "\n",
    "        val_rmse_hist = history[\"val_rmse\"]\n",
    "        if len(val_rmse_hist) == 0:\n",
    "            return float(\"inf\")\n",
    "        return val_rmse_hist[-1]\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "    print(\"\\nBest trial:\")\n",
    "    print(study.best_trial)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_model, best_opt, best_crit = create_model(\n",
    "        hidden_size=best_params[\"hidden_size\"],\n",
    "        num_layers=best_params[\"num_layers\"],\n",
    "        dropout=best_params[\"dropout\"],\n",
    "        lr=best_params[\"lr\"],\n",
    "    )\n",
    "    best_history = train_model(\n",
    "        best_model,\n",
    "        best_opt,\n",
    "        best_crit,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        n_epochs=best_params[\"epochs\"],\n",
    "        clip_norm=1.0,\n",
    "    )\n",
    "\n",
    "    best_val_pred,  best_val_true  = evaluate_model(best_model, val_loader)\n",
    "    best_test_pred, best_test_true = evaluate_model(best_model, test_loader)\n",
    "\n",
    "    best_val_rmse  = rmse(best_val_true,  best_val_pred)\n",
    "    best_val_r2    = r2_score(best_val_true,  best_val_pred)\n",
    "    best_test_rmse = rmse(best_test_true, best_test_pred)\n",
    "    best_test_r2   = r2_score(best_test_true, best_test_pred)\n",
    "\n",
    "    print(\"\\nBest LSTM (Optuna) performance:\")\n",
    "    print(f\"Val  RMSE: {best_val_rmse:.4f}, R2: {best_val_r2:.4f}\")\n",
    "    print(f\"Test RMSE: {best_test_rmse:.4f}, R2: {best_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da03ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PERFORMANCE COMPARISON & OPTUNA VISUALIZATION ===\n",
    "\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    \"Model\": \"Baseline_LSTM\",\n",
    "    \"Val_RMSE\": baseline_val_rmse,\n",
    "    \"Val_R2\":   baseline_val_r2,\n",
    "    \"Test_RMSE\": baseline_test_rmse,\n",
    "    \"Test_R2\":   baseline_test_r2,\n",
    "})\n",
    "\n",
    "# If you have metrics from your initial feed-forward NN, put them here:\n",
    "# initial_nn_val_rmse  = ...\n",
    "# initial_nn_val_r2    = ...\n",
    "# initial_nn_test_rmse = ...\n",
    "# initial_nn_test_r2   = ...\n",
    "# results.append({\n",
    "#     \"Model\": \"Initial_NN\",\n",
    "#     \"Val_RMSE\": initial_nn_val_rmse,\n",
    "#     \"Val_R2\":   initial_nn_val_r2,\n",
    "#     \"Test_RMSE\": initial_nn_test_rmse,\n",
    "#     \"Test_R2\":   initial_nn_test_r2,\n",
    "# })\n",
    "\n",
    "if OPTUNA_AVAILABLE and 'best_val_rmse' in globals():\n",
    "    results.append({\n",
    "        \"Model\": \"Best_LSTM_Optuna\",\n",
    "        \"Val_RMSE\": best_val_rmse,\n",
    "        \"Val_R2\":   best_val_r2,\n",
    "        \"Test_RMSE\": best_test_rmse,\n",
    "        \"Test_R2\":   best_test_r2,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Bar plots for RMSE and R² comparison\n",
    "plt.figure()\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Val_RMSE\"])\n",
    "plt.ylabel(\"Val RMSE\")\n",
    "plt.title(\"Model Comparison - Validation RMSE\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(results_df[\"Model\"], results_df[\"Val_R2\"])\n",
    "plt.ylabel(\"Val R²\")\n",
    "plt.title(\"Model Comparison - Validation R²\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optuna visualizations (if available)\n",
    "if OPTUNA_AVAILABLE and 'study' in globals():\n",
    "    try:\n",
    "        from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "        fig1 = plot_optimization_history(study)\n",
    "        fig1.show()\n",
    "        fig2 = plot_param_importances(study)\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(\"Could not generate Optuna visualizations:\", e)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
