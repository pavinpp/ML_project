{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Yield Prediction – Final Model Pipeline (Rev9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PavinP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from scipy.signal import detrend\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"All libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection Menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODELS = {\n",
    "    'LR': True,   # Baseline Linear Regression\n",
    "    'RF': True,   # Random Forest\n",
    "    'XGB': True,  # XGBoost\n",
    "    'LSTM': True, # LSTM\n",
    "    'CNN': True   # CNN\n",
    "}\n",
    "\n",
    "RUN_OPTUNA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Sudan'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_encode.py:167\u001b[39m, in \u001b[36m_nandict.__missing__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nan_value\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Sudan'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m pipeline.resplit()\n\u001b[32m    109\u001b[39m pipeline.fit_encode_scale()\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_dl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m pipeline.save_transformers()\n\u001b[32m    113\u001b[39m N_AREAS = \u001b[38;5;28mlen\u001b[39m(pipeline.le_area.classes_)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mCropDataPipeline.prepare_dl\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m.test_df_dl = \u001b[38;5;28mself\u001b[39m.df_dl[\u001b[38;5;28mself\u001b[39m.df_dl[\u001b[38;5;28mself\u001b[39m.time_col] > \u001b[38;5;28mself\u001b[39m.val_end].copy()\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.train_df_dl, \u001b[38;5;28mself\u001b[39m.val_df_dl, \u001b[38;5;28mself\u001b[39m.test_df_dl]:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     d[\u001b[33m'\u001b[39m\u001b[33mArea_Encoded\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mle_area\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mArea\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     d[\u001b[33m'\u001b[39m\u001b[33mItem_Encoded\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.le_item.transform(d[\u001b[33m'\u001b[39m\u001b[33mItem\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler_dl.fit(\u001b[38;5;28mself\u001b[39m.train_df_dl[\u001b[38;5;28mself\u001b[39m.numeric_cols])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\preprocessing\\_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_encode.py:237\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[31mValueError\u001b[39m: y contains previously unseen labels: 'Sudan'"
     ]
    }
   ],
   "source": [
    "class CropDataPipeline:\n",
    "    def __init__(self, data_path):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.target = 'hg/ha_yield'\n",
    "        self.time_col = 'Year'\n",
    "        self.cat_cols = ['Area', 'Item']\n",
    "        self.numeric_cols = ['average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp', 'fertilizer_kg/ha', 'solar_radiation_MJ/m2-day']\n",
    "        self.trend_models = {}\n",
    "        self.le_area = LabelEncoder()\n",
    "        self.le_item = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler_dl = StandardScaler()\n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.train_end = 2007\n",
    "        self.val_end = 2010\n",
    "        self.lookback = 5\n",
    "        self.lag_cols = []  # Will set after detrend\n",
    "        self.lags = [1, 2]\n",
    "        self.ml_feats = []  # Will set later\n",
    "        self.dl_feats = self.numeric_cols + ['Area_Encoded', 'Item_Encoded']\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.train_df = self.df[self.df[self.time_col] <= self.train_end].copy()\n",
    "        self.val_df = self.df[(self.df[self.time_col] > self.train_end) & (self.df[self.time_col] <= self.val_end)].copy()\n",
    "        self.test_df = self.df[self.df[self.time_col] > self.val_end].copy()\n",
    "    \n",
    "    def fit_trend(self):\n",
    "        for key, group in self.train_df.groupby(self.cat_cols):\n",
    "            if len(group) < 2:\n",
    "                continue\n",
    "            X = group[[self.time_col]]\n",
    "            y = group[self.target]\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            self.trend_models[tuple(key)] = model\n",
    "    \n",
    "    def add_detrend(self):\n",
    "        for d in [self.train_df, self.val_df, self.test_df]:\n",
    "            d['yield_trend'] = np.nan\n",
    "            d['yield_detrended'] = np.nan\n",
    "            for idx in d.index:\n",
    "                row = d.loc[idx]\n",
    "                key = (row[self.cat_cols[0]], row[self.cat_cols[1]])\n",
    "                if key in self.trend_models:\n",
    "                    year = [[row[self.time_col]]]\n",
    "                    trend = self.trend_models[key].predict(year)[0]\n",
    "                    d.loc[idx, 'yield_trend'] = trend\n",
    "                    d.loc[idx, 'yield_detrended'] = row[self.target] - trend\n",
    "        self.lag_cols = ['yield_detrended'] + self.numeric_cols\n",
    "    \n",
    "    def create_lags(self):\n",
    "        self.df_full = pd.concat([self.train_df, self.val_df, self.test_df]).sort_values(self.cat_cols + [self.time_col])\n",
    "        for col in self.lag_cols:\n",
    "            for lag in self.lags:\n",
    "                self.df_full[f'{col}_lag{lag}'] = self.df_full.groupby(self.cat_cols)[col].shift(lag)\n",
    "        self.df_ml = self.df_full.dropna().copy()\n",
    "        self.lagged_cols = [c for c in self.df_ml.columns if '_lag' in c]\n",
    "        self.ml_feats = self.numeric_cols + self.lagged_cols + ['Area_Encoded', 'Item_Encoded']\n",
    "    \n",
    "    def resplit(self):\n",
    "        self.train_df = self.df_ml[self.df_ml[self.time_col] <= self.train_end].copy()\n",
    "        self.val_df = self.df_ml[(self.df_ml[self.time_col] > self.train_end) & (self.df_ml[self.time_col] <= self.val_end)].copy()\n",
    "        self.test_df = self.df_ml[self.df_ml[self.time_col] > self.val_end].copy()\n",
    "    \n",
    "    def fit_encode_scale(self):\n",
    "        all_cats = pd.concat([self.train_df, self.val_df, self.test_df])\n",
    "        self.le_area.fit(all_cats['Area'])\n",
    "        self.le_item.fit(all_cats['Item'])\n",
    "        for d in [self.train_df, self.val_df, self.test_df]:\n",
    "            d['Area_Encoded'] = self.le_area.transform(d['Area'])\n",
    "            d['Item_Encoded'] = self.le_item.transform(d['Item'])\n",
    "        scale_cols = self.numeric_cols + self.lagged_cols\n",
    "        self.scaler.fit(self.train_df[scale_cols])\n",
    "        self.train_df[scale_cols] = self.scaler.transform(self.train_df[scale_cols])\n",
    "        self.val_df[scale_cols] = self.scaler.transform(self.val_df[scale_cols])\n",
    "        self.test_df[scale_cols] = self.scaler.transform(self.test_df[scale_cols])\n",
    "    \n",
    "    def prepare_dl(self):\n",
    "        self.df_dl = self.df_full.copy()  # Use full for sequences (no dropna)\n",
    "        self.train_df_dl = self.df_dl[self.df_dl[self.time_col] <= self.train_end].copy()\n",
    "        self.val_df_dl = self.df_dl[(self.df_dl[self.time_col] > self.train_end) & (self.df_dl[self.time_col] <= self.val_end)].copy()\n",
    "        self.test_df_dl = self.df_dl[self.df_dl[self.time_col] > self.val_end].copy()\n",
    "        for d in [self.train_df_dl, self.val_df_dl, self.test_df_dl]:\n",
    "            d['Area_Encoded'] = self.le_area.transform(d['Area'])\n",
    "            d['Item_Encoded'] = self.le_item.transform(d['Item'])\n",
    "        self.scaler_dl.fit(self.train_df_dl[self.numeric_cols])\n",
    "        self.train_df_dl[self.numeric_cols] = self.scaler_dl.transform(self.train_df_dl[self.numeric_cols])\n",
    "        self.val_df_dl[self.numeric_cols] = self.scaler_dl.transform(self.val_df_dl[self.numeric_cols])\n",
    "        self.test_df_dl[self.numeric_cols] = self.scaler_dl.transform(self.test_df_dl[self.numeric_cols])\n",
    "        self.target_scaler.fit(self.train_df_dl[['yield_detrended']])\n",
    "        self.train_df_dl['yield_detrended'] = self.target_scaler.transform(self.train_df_dl[['yield_detrended']])\n",
    "        self.val_df_dl['yield_detrended'] = self.target_scaler.transform(self.val_df_dl[['yield_detrended']])\n",
    "        self.test_df_dl['yield_detrended'] = self.target_scaler.transform(self.test_df_dl[['yield_detrended']])\n",
    "    \n",
    "    def save_transformers(self):\n",
    "        joblib.dump(self.scaler, 'scaler.joblib')\n",
    "        joblib.dump(self.le_area, 'le_area.joblib')\n",
    "        joblib.dump(self.le_item, 'le_item.joblib')\n",
    "        joblib.dump(self.trend_models, 'trend_models.joblib')\n",
    "        joblib.dump(self.scaler_dl, 'scaler_dl.joblib')\n",
    "        joblib.dump(self.target_scaler, 'target_scaler.joblib')\n",
    "\n",
    "# Usage in notebook (replaces sections 3 and 4)\n",
    "pipeline = CropDataPipeline('cleaned_crop_data.csv')\n",
    "pipeline.split_data()\n",
    "pipeline.fit_trend()\n",
    "pipeline.add_detrend()\n",
    "pipeline.create_lags()\n",
    "pipeline.resplit()\n",
    "pipeline.fit_encode_scale()\n",
    "pipeline.prepare_dl()\n",
    "pipeline.save_transformers()\n",
    "\n",
    "N_AREAS = len(pipeline.le_area.classes_)\n",
    "N_ITEMS = len(pipeline.le_item.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML inputs\n",
    "X_train_ml = pipeline.train_df[pipeline.ml_feats]\n",
    "y_train_ml = pipeline.train_df['yield_detrended']\n",
    "X_val_ml = pipeline.val_df[pipeline.ml_feats]\n",
    "y_val_ml = pipeline.val_df['yield_detrended']\n",
    "X_test_ml = pipeline.test_df[pipeline.ml_feats]\n",
    "y_test_ml = pipeline.test_df['yield_detrended']\n",
    "\n",
    "def create_sequences(data, lookback, feats, target):\n",
    "    X, y = [], []\n",
    "    for _, group in data.groupby(CAT_COLS):\n",
    "        if len(group) < lookback:\n",
    "            continue\n",
    "        gf = group[feats].values\n",
    "        gt = group[target].values\n",
    "        for i in range(len(group) - lookback + 1):\n",
    "            X.append(gf[i:i+lookback])\n",
    "            y.append(gt[i+lookback-1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def split_dl(X):\n",
    "    return [\n",
    "        torch.tensor(X[..., :-2], dtype=torch.float32),\n",
    "        torch.tensor(X[..., -2], dtype=torch.long),\n",
    "        torch.tensor(X[..., -1], dtype=torch.long)\n",
    "    ]\n",
    "\n",
    "# DL sequences (use existing create_sequences and split_dl functions)\n",
    "X_train_seq, y_train_seq = create_sequences(pipeline.train_df_dl, pipeline.lookback, pipeline.dl_feats, 'yield_detrended')\n",
    "X_val_seq, y_val_seq = create_sequences(pipeline.val_df_dl, pipeline.lookback, pipeline.dl_feats, 'yield_detrended')\n",
    "X_test_seq, y_test_seq = create_sequences(pipeline.test_df_dl, pipeline.lookback, pipeline.dl_feats, 'yield_detrended')\n",
    "\n",
    "X_train_dl = split_dl(X_train_seq)\n",
    "X_val_dl = split_dl(X_val_seq)\n",
    "X_test_dl = split_dl(X_test_seq)\n",
    "\n",
    "y_train_t = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_val_t = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_t = torch.tensor(y_test_seq, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(((y_true - y_pred) / (y_true + 1e-8)) ** 2)) * 100\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optuna Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lr(trial):\n",
    "    # No hyperparameters to tune for Linear Regression\n",
    "    return 0\n",
    "\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_float('max_features', 0.5, 1.0)\n",
    "    }\n",
    "    model = RandomForestRegressor(random_state=42, n_jobs=-1, **params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train_ml):\n",
    "        model.fit(X_train_ml.iloc[train_idx], y_train_ml.iloc[train_idx])\n",
    "        pred = model.predict(X_train_ml.iloc[val_idx])\n",
    "        scores.append(rmspe(y_train_ml.iloc[val_idx], pred))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(random_state=42, **params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train_ml):\n",
    "        model.fit(X_train_ml.iloc[train_idx], y_train_ml.iloc[train_idx])\n",
    "        pred = model.predict(X_train_ml.iloc[val_idx])\n",
    "        scores.append(rmspe(y_train_ml.iloc[val_idx], pred))\n",
    "    return np.mean(scores)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_areas, n_items, lstm_units, dense_units, dropout):\n",
    "        super().__init__()\n",
    "        self.embed_area = nn.Embedding(n_areas, 10)\n",
    "        self.embed_item = nn.Embedding(n_items, 5)\n",
    "        self.lstm = nn.LSTM(len(NUMERIC_COLS) + 10 + 5, lstm_units, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(lstm_units, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "    def forward(self, num, area, item):\n",
    "        e_area = self.embed_area(area)\n",
    "        e_item = self.embed_item(item)\n",
    "        x = torch.cat([num, e_area, e_item], dim=-1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.drop(out[:, -1])\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        return self.fc2(out)\n",
    "\n",
    "def train_dl(model, opt, loss_fn, train_loader, val_loader, epochs=100, patience=10, is_final=False):\n",
    "    best = float('inf')\n",
    "    wait = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x1, x2, x3, y in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(x1, x2, x3)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs = [x.to(next(model.parameters()).device) for x in val_loader.dataset.tensors[:3]]\n",
    "            val_y = val_loader.dataset.tensors[3]\n",
    "            val_pred = model(*val_inputs)\n",
    "            val_mse = loss_fn(val_pred, val_y).item()\n",
    "            val_rmspe = rmspe(val_y.numpy().flatten(), val_pred.numpy().flatten())\n",
    "        val_losses.append(val_mse)\n",
    "        if val_rmspe < best:\n",
    "            best = val_rmspe\n",
    "            wait = 0\n",
    "            if is_final:\n",
    "                torch.save(model.state_dict(), f'model_{{model.__class__.__name__}}.pth')\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def objective_lstm(trial):\n",
    "    params = {\n",
    "        'lstm_units': trial.suggest_categorical('lstm_units', [64, 128]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [32, 64]),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "    lr = params.pop('lr')\n",
    "    model = LSTMModel(N_AREAS, N_ITEMS, **params)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_ds = TensorDataset(*X_train_dl, y_train_t)\n",
    "    val_ds = TensorDataset(*X_val_dl, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    _, val_losses = train_dl(model, opt, nn.MSELoss(), train_loader, val_loader)\n",
    "    return val_losses[-1]\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_areas, n_items, filters, kernel, dense_units): \n",
    "        super().__init__()\n",
    "        self.embed_area = nn.Embedding(n_areas, 10)\n",
    "        self.embed_item = nn.Embedding(n_items, 5)\n",
    "        self.conv = nn.Conv1d(len(NUMERIC_COLS) + 10 + 5, filters, kernel)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(filters, dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "    def forward(self, num, area, item):\n",
    "        e_area = self.embed_area(area)\n",
    "        e_item = self.embed_item(item)\n",
    "        x = torch.cat([num, e_area, e_item], dim=-1).transpose(1, 2)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def objective_cnn(trial):\n",
    "    params = {\n",
    "        'filters': trial.suggest_categorical('filters', [64, 128]),\n",
    "        'kernel': trial.suggest_categorical('kernel', [2, 3]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [32, 64]),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "    lr = params.pop('lr')\n",
    "    model = CNNModel(N_AREAS, N_ITEMS, **params)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_ds = TensorDataset(*X_train_dl, y_train_t)\n",
    "    val_ds = TensorDataset(*X_val_dl, y_val_t)\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=64)\n",
    "    _, val_losses = train_dl(model, opt, nn.MSELoss(), train_loader, val_loader)\n",
    "    return val_losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('best_params_optuna.joblib'):\n",
    "    best_params = joblib.load('best_params_optuna.joblib')\n",
    "else:\n",
    "    best_params = {}\n",
    "\n",
    "if RUN_OPTUNA:\n",
    "    objectives = {\n",
    "        'LR': objective_lr,\n",
    "        'RF': objective_rf,\n",
    "        'XGB': objective_xgb,\n",
    "        'LSTM': objective_lstm,\n",
    "        'CNN': objective_cnn\n",
    "    }\n",
    "    for name, run in RUN_MODELS.items():\n",
    "        if run:\n",
    "            print(f'--- Tuning {name} ---')\n",
    "            study = optuna.create_study(direction='minimize')\n",
    "            n_trials = 50 if name in ['RF', 'XGB'] else 30\n",
    "            if name == 'LR':\n",
    "                n_trials = 1\n",
    "            study.optimize(objectives[name], n_trials=n_trials, show_progress_bar=True)\n",
    "            best_params[name] = study.best_params\n",
    "            joblib.dump(best_params, 'best_params_optuna.joblib') # Save after each study\n",
    "            print(f'Best params for {name}: {study.best_params}')\n",
    "else:\n",
    "    print('Skipping Optuna tuning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train+val\n",
    "X_train_full_ml = pd.concat([X_train_ml, X_val_ml])\n",
    "y_train_full_ml = pd.concat([y_train_ml, y_val_ml])\n",
    "X_train_full_seq = np.concatenate([X_train_seq, X_val_seq])\n",
    "y_train_full_seq = np.concatenate([y_train_seq, y_val_seq])\n",
    "X_train_full_dl = split_dl(X_train_full_seq)\n",
    "y_train_full_t = torch.tensor(y_train_full_seq, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "models = {}\n",
    "\n",
    "if RUN_MODELS['LR']:\n",
    "    model_lr = LinearRegression()\n",
    "    model_lr.fit(X_train_full_ml, y_train_full_ml)\n",
    "    models['LR'] = model_lr\n",
    "    joblib.dump(model_lr, 'model_lr.joblib')\n",
    "\n",
    "if RUN_MODELS['RF']:\n",
    "    model_rf = RandomForestRegressor(random_state=42, n_jobs=-1, **best_params['RF'])\n",
    "    model_rf.fit(X_train_full_ml, y_train_full_ml)\n",
    "    models['RF'] = model_rf\n",
    "    joblib.dump(model_rf, 'model_rf.joblib')\n",
    "\n",
    "if RUN_MODELS['XGB']:\n",
    "    model_xgb = xgb.XGBRegressor(random_state=42, **best_params['XGB'])\n",
    "    model_xgb.fit(X_train_full_ml, y_train_full_ml)\n",
    "    models['XGB'] = model_xgb\n",
    "    joblib.dump(model_xgb, 'model_xgb.joblib')\n",
    "\n",
    "# DL\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_full_ds = TensorDataset(*[x.to(device) for x in X_train_full_dl], y_train_full_t.to(device))\n",
    "test_ds = TensorDataset(*[x.to(device) for x in X_test_dl], y_test_t.to(device))\n",
    "train_loader = DataLoader(train_full_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "if RUN_MODELS['LSTM']:\n",
    "    lstm_params = best_params['LSTM']\n",
    "    lr = lstm_params.pop('lr')\n",
    "    model_lstm = LSTMModel(N_AREAS, N_ITEMS, **lstm_params).to(device)\n",
    "    opt_lstm = optim.Adam(model_lstm.parameters(), lr=lr)\n",
    "    train_losses_lstm, val_losses_lstm = train_dl(model_lstm, opt_lstm, nn.MSELoss(), train_loader, test_loader, epochs=150, patience=15, is_final=True)\n",
    "    models['LSTM'] = model_lstm\n",
    "\n",
    "if RUN_MODELS['CNN']:\n",
    "    cnn_params = best_params['CNN']\n",
    "    lr = cnn_params.pop('lr')\n",
    "    model_cnn = CNNModel(N_AREAS, N_ITEMS, **cnn_params).to(device)\n",
    "    opt_cnn = optim.Adam(model_cnn.parameters(), lr=lr)\n",
    "    train_losses_cnn, val_losses_cnn = train_dl(model_cnn, opt_cnn, nn.MSELoss(), train_loader, test_loader, epochs=150, patience=15, is_final=True)\n",
    "    models['CNN'] = model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot DL Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['LSTM'] and RUN_MODELS['CNN']:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
    "    ax1.plot(train_losses_lstm, label='Train Loss')\n",
    "    ax1.plot(val_losses_lstm, label='Validation (Test) Loss')\n",
    "    ax1.set_title('LSTM Model Loss', fontsize=16)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "    ax1.legend()\n",
    "    ax2.plot(train_losses_cnn, label='Train Loss')\n",
    "    ax2.plot(val_losses_cnn, label='Validation (Test) Loss')\n",
    "    ax2.set_title('CNN Model Loss', fontsize=16)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Mean Squared Error')\n",
    "    ax2.legend()\n",
    "    plt.suptitle('Deep Learning Training Curves', fontsize=20)\n",
    "    plt.savefig(\"loss_curves.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align test sets\n",
    "matched_test_df = pd.concat([group.iloc[LOOKBACK-1:] for _, group in test_df_dl.groupby(CAT_COLS) if len(group) >= LOOKBACK])\n",
    "matched_test_ml_df = pd.concat([group.iloc[LOOKBACK-1:] for _, group in test_df.groupby(CAT_COLS) if len(group) >= LOOKBACK])\n",
    "X_test_ml_matched = matched_test_ml_df[ML_FEATS]\n",
    "X_test_dl_matched, y_test_det_matched = create_sequences(matched_test_df, 1, DL_FEATS, TARGET_DET)\n",
    "y_test_det_matched = y_test_det_matched.flatten()\n",
    "trend_test = matched_test_df['yield_trend'].values\n",
    "y_true_original = matched_test_df[TARGET].values\n",
    "\n",
    "# Predictions\n",
    "test_preds = {}\n",
    "for name in models:\n",
    "    if name in ['LR', 'RF', 'XGB']:\n",
    "        test_preds[name] = models[name].predict(X_test_ml_matched)\n",
    "    elif name in ['LSTM', 'CNN']:\n",
    "        X_test_dl_inputs_m = split_dl(X_test_seq)\n",
    "        models[name].eval()\n",
    "        with torch.no_grad():\n",
    "            test_preds[name] = models[name](*[x.to(device) for x in X_test_dl_inputs_m]).cpu().numpy().flatten()\n",
    "\n",
    "# Evaluate\n",
    "results = []\n",
    "y_preds_original = {}\n",
    "for name, pred_det in test_preds.items():\n",
    "    pred_orig = pred_det + trend_test\n",
    "    y_preds_original[name] = pred_orig\n",
    "    mae = mean_absolute_error(y_true_original, pred_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_original, pred_orig))\n",
    "    map_e = mape(y_true_original, pred_orig)\n",
    "    rms_pe = rmspe(y_true_original, pred_orig)\n",
    "    r_2 = r2_score(y_true_original, pred_orig)\n",
    "    results.append({'Model': name, 'MAE': mae, 'RMSE': rmse, 'MAPE (%)': map_e, 'RMSPE (%)': rms_pe, 'R²': r_2})\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model').sort_values('RMSPE (%)')\n",
    "print(\"\\n--- Final Performance (Test Set) ---\")\n",
    "print(results_df.round(2))\n",
    "results_df.to_csv(\"final_model_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Model Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "sns.barplot(data=results_df.reset_index(), x='Model', y='RMSE', ax=axs[0])\n",
    "axs[0].set_title('RMSE Comparison')\n",
    "sns.barplot(x='Model', y='MAE', data=results_df.reset_index(), ax=axs[1])\n",
    "axs[1].set_title('MAE Comparison')\n",
    "sns.barplot(x='Model', y='MAPE (%)', data=results_df.reset_index(), ax=axs[2])\n",
    "axs[2].set_title('MAPE Comparison')\n",
    "sns.barplot(x='Model', y='R²', data=results_df.reset_index(), ax=axs[3])\n",
    "axs[3].set_title('R² Comparison')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_performance_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Crop Reporting (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.index[0]\n",
    "print(f\"Per-crop report for best model: {best_model_name}\")\n",
    "crop_results = []\n",
    "items = matched_test_df['Item'].values\n",
    "for crop in np.unique(items):\n",
    "    mask = items == crop\n",
    "    true = y_true_original[mask]\n",
    "    pred = y_preds_original[best_model_name][mask]\n",
    "    crop_results.append({\n",
    "        'Crop': crop,\n",
    "        'RMSPE (%)': rmspe(true, pred),\n",
    "        'MAPE (%)': mape(true, pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(true, pred)),\n",
    "        'R²': r2_score(true, pred)\n",
    "    })\n",
    "crop_df = pd.DataFrame(crop_results).sort_values('RMSPE (%)')\n",
    "print(crop_df.round(2))\n",
    "crop_df.to_csv('per_crop_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. SHAP Analysis (If Tree Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = results_df.index[0]\n",
    "if best_model_name in models and best_model_name in ['RF', 'XGB']:\n",
    "    best_model = models[best_model_name]\n",
    "    print(f\"Running SHAP on {best_model_name}\")\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_test_ml_matched)\n",
    "    shap.summary_plot(shap_values, X_test_ml_matched, plot_type=\"beeswarm\", show=False)\n",
    "    plt.title(f\"SHAP Beeswarm ({best_model_name})\", fontsize=16)\n",
    "    plt.savefig(\"shap_beeswarm.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    shap.summary_plot(shap_values, X_test_ml_matched, plot_type=\"bar\", show=False)\n",
    "    plt.title(f\"Feature Importance ({best_model_name})\", fontsize=16)\n",
    "    plt.savefig(\"shap_importance.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP skipped for non-tree model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions_df = matched_test_df.copy()\n",
    "final_predictions_df['true_yield_original'] = y_true_original\n",
    "for name in test_preds:\n",
    "    final_predictions_df[f'predicted_{name}'] = y_preds_original[name]\n",
    "export_cols = ['Year', 'Area', 'Item', 'true_yield_original'] + [f'predicted_{name}' for name in test_preds]\n",
    "final_predictions_df[export_cols].to_csv(\"final_test_predictions.csv\", index=False)\n",
    "print(\"Exported predictions.\")\n",
    "print(\"\\n--- Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
