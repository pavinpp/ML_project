{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import copy\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optuna Visualization Tools\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b460f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. LOAD DATA & BASIC CONFIG\n",
    "# ==============================\n",
    "\n",
    "# Choose crop (must match suffix after 'Y_' in your df)\n",
    "CHOSEN_CROP = \"rice\"  # e.g. \"rice\", \"wheat\", \"maize_corn\", ...\n",
    "TARGET_COL = f\"Y_{CHOSEN_CROP}\"\n",
    "\n",
    "# Sequence length for LSTM\n",
    "SEQ_LEN = 3\n",
    "\n",
    "# Paths\n",
    "PARQUET_PATH = \"Parquet/XY_v2.parquet\"\n",
    "\n",
    "# Load parquet\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "print(\"Raw data shape:\", df.shape)\n",
    "\n",
    "# ==============================\n",
    "# 2. BASIC FILTERING & TARGET HANDLING\n",
    "# ==============================\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert \"year\" in df.columns, \"'year' column not found!\"\n",
    "assert \"area\" in df.columns, \"'area' column not found!\"\n",
    "assert TARGET_COL in df.columns, f\"{TARGET_COL} not found in df columns!\"\n",
    "\n",
    "# Keep only years within the needed range (optional, depending on your data)\n",
    "df = df[(df[\"year\"] >= 1982) & (df[\"year\"] <= 2023)].copy()\n",
    "\n",
    "# Drop rows where the chosen target is NaN\n",
    "df = df.dropna(subset=[TARGET_COL]).copy()\n",
    "print(\"After dropping NaN targets:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FEATURE SELECTION\n",
    "# ==============================\n",
    "# We will:\n",
    "# - Keep all non-yield features (weather, inputs, coords, year, etc.)\n",
    "# - Keep lagged average yields only for the chosen crop: avg_yield_<CHOSEN_CROP>_*\n",
    "# - Drop all other avg_yield_* columns that belong to other crops\n",
    "# - Remove all Y_* columns from the feature set (we keep TARGET_COL only for labels)\n",
    "# - We will NOT use 'area' as a numeric feature; only for grouping sequences.\n",
    "#   (Simpler than one-hot encoding or embeddings. Comment this choice.)\n",
    "\n",
    "df_model = df.copy()\n",
    "\n",
    "# Identify all yield target columns\n",
    "all_target_cols = [c for c in df_model.columns if c.startswith(\"Y_\")]\n",
    "\n",
    "# Drop all target columns except the chosen one\n",
    "other_targets = [c for c in all_target_cols if c != TARGET_COL]\n",
    "df_model = df_model.drop(columns=other_targets)\n",
    "\n",
    "# Identify all avg_yield_* columns\n",
    "avg_yield_cols = [c for c in df_model.columns if c.startswith(\"avg_yield_\")]\n",
    "\n",
    "# Keep only lagged avg_yield columns for the chosen crop\n",
    "chosen_prefix = f\"avg_yield_{CHOSEN_CROP}_\"\n",
    "keep_yield_lag_cols = [c for c in avg_yield_cols if c.startswith(chosen_prefix)]\n",
    "drop_yield_lag_cols = sorted(set(avg_yield_cols) - set(keep_yield_lag_cols))\n",
    "\n",
    "df_model = df_model.drop(columns=drop_yield_lag_cols)\n",
    "print(f\"Kept lagged yield columns for crop '{CHOSEN_CROP}':\")\n",
    "print(keep_yield_lag_cols)\n",
    "print(f\"Dropped {len(drop_yield_lag_cols)} lagged yield columns of other crops.\")\n",
    "\n",
    "# Sort by area and year for time consistency\n",
    "df_model = df_model.sort_values([\"area\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# Now define feature columns:\n",
    "feature_cols = [\n",
    "    c for c in df_model.columns\n",
    "    if c not in [\"area\", \"year\", TARGET_COL] and not c.startswith(\"Y_\")\n",
    "]\n",
    "\n",
    "print(\"Number of feature columns:\", len(feature_cols))\n",
    "\n",
    "# ==============================\n",
    "# 4. GROUPED FILLING OF NaNs (FFILL + BFILL) ON FEATURES\n",
    "# ==============================\n",
    "# Step 1: forward-fill and backward-fill per 'area' in time order for feature columns\n",
    "# Target column is NOT touched here (we already dropped NaNs in target)\n",
    "\n",
    "df_model[feature_cols] = (\n",
    "    df_model\n",
    "    .groupby(\"area\", group_keys=False)[feature_cols]\n",
    "    .apply(lambda g: g.ffill().bfill())\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 5. TIME-BASED SPLIT BY YEAR\n",
    "# ==============================\n",
    "# Rules:\n",
    "# Train: year < 2014\n",
    "# Val:   2014 <= year <= 2018\n",
    "# Test:  2019 <= year <= 2023\n",
    "\n",
    "train_mask = df_model[\"year\"] < 2014\n",
    "val_mask   = (df_model[\"year\"] >= 2014) & (df_model[\"year\"] <= 2018)\n",
    "test_mask  = df_model[\"year\"] >= 2019\n",
    "\n",
    "def print_split_info(df_, mask, name):\n",
    "    years = sorted(df_.loc[mask, \"year\"].unique())\n",
    "    print(f\"{name} years: {years}\")\n",
    "    print(f\"{name} rows: {mask.sum()}\")\n",
    "\n",
    "print_split_info(df_model, train_mask, \"TRAIN\")\n",
    "print_split_info(df_model, val_mask,   \"VAL\")\n",
    "print_split_info(df_model, test_mask,  \"TEST\")\n",
    "\n",
    "# Extract raw feature and target subsets\n",
    "X_train_raw = df_model.loc[train_mask, feature_cols].copy()\n",
    "X_val_raw   = df_model.loc[val_mask,   feature_cols].copy()\n",
    "X_test_raw  = df_model.loc[test_mask,  feature_cols].copy()\n",
    "\n",
    "y_train = df_model.loc[train_mask, TARGET_COL].values.astype(np.float32)\n",
    "y_val   = df_model.loc[val_mask,   TARGET_COL].values.astype(np.float32)\n",
    "y_test  = df_model.loc[test_mask,  TARGET_COL].values.astype(np.float32)\n",
    "\n",
    "# ==============================\n",
    "# 6. REMAINING NaNs → TRAIN MEAN IMPUTATION\n",
    "# ==============================\n",
    "# Compute column means from the TRAIN set (after ffill/bfill)\n",
    "col_means = X_train_raw.mean(axis=0)\n",
    "\n",
    "# Fill remaining NaNs with these means\n",
    "X_train_filled = X_train_raw.fillna(col_means)\n",
    "X_val_filled   = X_val_raw.fillna(col_means)\n",
    "X_test_filled  = X_test_raw.fillna(col_means)\n",
    "\n",
    "# Optional sanity check\n",
    "print(\"Any NaNs remaining in train features?\", X_train_filled.isna().any().any())\n",
    "print(\"Any NaNs remaining in val features?\",   X_val_filled.isna().any().any())\n",
    "print(\"Any NaNs remaining in test features?\",  X_test_filled.isna().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f56ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7. SCALING (StandardScaler on FEATURES ONLY)\n",
    "# ==============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filled.values)\n",
    "X_val_scaled   = scaler.transform(X_val_filled.values)\n",
    "X_test_scaled  = scaler.transform(X_test_filled.values)\n",
    "\n",
    "# Create DataFrames with scaled features (to keep alignment with area/year)\n",
    "X_train_scaled_df = pd.DataFrame(\n",
    "    X_train_scaled, index=X_train_filled.index, columns=feature_cols\n",
    ")\n",
    "X_val_scaled_df = pd.DataFrame(\n",
    "    X_val_scaled, index=X_val_filled.index, columns=feature_cols\n",
    ")\n",
    "X_test_scaled_df = pd.DataFrame(\n",
    "    X_test_scaled, index=X_test_filled.index, columns=feature_cols\n",
    ")\n",
    "\n",
    "# Rebuild \"split\" dataframes including area/year/target for sequence building\n",
    "train_df = df_model.loc[train_mask, [\"area\", \"year\", TARGET_COL]].join(X_train_scaled_df)\n",
    "val_df   = df_model.loc[val_mask,   [\"area\", \"year\", TARGET_COL]].join(X_val_scaled_df)\n",
    "test_df  = df_model.loc[test_mask,  [\"area\", \"year\", TARGET_COL]].join(X_test_scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 8. SEQUENCE BUILDING FUNCTION\n",
    "# ==============================\n",
    "def build_sequences_panel(df_split, feature_cols, target_col, seq_len=3):\n",
    "    \"\"\"\n",
    "    Build rolling sequences per area, using only rows from df_split.\n",
    "    No leakage across splits since each df_split is already split by year.\n",
    "    \n",
    "    For each 'area':\n",
    "    - Sort by year\n",
    "    - Create rolling windows of length seq_len of features\n",
    "    - The label is the target at the last timestep of the window\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: (num_sequences, seq_len, num_features)\n",
    "        y_seq: (num_sequences,)\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for area, g in df_split.groupby(\"area\"):\n",
    "        g = g.sort_values(\"year\")\n",
    "        feat_vals = g[feature_cols].values\n",
    "        targ_vals = g[target_col].values\n",
    "\n",
    "        if len(g) < seq_len:\n",
    "            # Not enough timesteps in this split for this area\n",
    "            continue\n",
    "\n",
    "        for i in range(len(g) - seq_len + 1):\n",
    "            x_seq = feat_vals[i : i + seq_len]\n",
    "            y_seq = targ_vals[i + seq_len - 1]  # target at last time step\n",
    "            X_list.append(x_seq)\n",
    "            y_list.append(y_seq)\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0, seq_len, len(feature_cols)), dtype=np.float32), np.empty((0,), dtype=np.float32)\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)\n",
    "    y_seq = np.array(y_list, dtype=np.float32)\n",
    "    return X_seq, y_seq\n",
    "\n",
    "X_train_seq, y_train_seq = build_sequences_panel(train_df, feature_cols, TARGET_COL, seq_len=SEQ_LEN)\n",
    "X_val_seq,   y_val_seq   = build_sequences_panel(val_df,   feature_cols, TARGET_COL, seq_len=SEQ_LEN)\n",
    "X_test_seq,  y_test_seq  = build_sequences_panel(test_df,  feature_cols, TARGET_COL, seq_len=SEQ_LEN)\n",
    "\n",
    "print(\"Train sequence shape:\", X_train_seq.shape, y_train_seq.shape)\n",
    "print(\"Val sequence shape:  \", X_val_seq.shape, y_val_seq.shape)\n",
    "print(\"Test sequence shape: \", X_test_seq.shape, y_test_seq.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbf9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 9. DATALOADERS\n",
    "# ==============================\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train_seq), torch.from_numpy(y_train_seq)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val_seq), torch.from_numpy(y_val_seq)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test_seq), torch.from_numpy(y_test_seq)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af11a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 10. LSTM MODEL DEFINITION\n",
    "# ==============================\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))  # out: (batch, seq_len, hidden_dim)\n",
    "        # Use output at last time step\n",
    "        last_out = out[:, -1, :]  # (batch, hidden_dim)\n",
    "        y_hat = self.fc(last_out).squeeze(-1)  # (batch,)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "# Instantiate model\n",
    "input_dim = len(feature_cols)\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187737fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 11. TRAINING SETUP\n",
    "# ==============================\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "NUM_EPOCHS = 120\n",
    "PATIENCE = 15  # for early stopping\n",
    "\n",
    "best_val_rmse = float(\"inf\")\n",
    "best_state = None\n",
    "no_improve_epochs = 0\n",
    "\n",
    "train_rmse_history = []\n",
    "val_rmse_history = []\n",
    "\n",
    "# ==============================\n",
    "# 12. TRAINING LOOP WITH EARLY STOPPING\n",
    "# ==============================\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_preds = []\n",
    "    train_trues = []\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_preds.append(y_pred.detach().cpu().numpy())\n",
    "        train_trues.append(y_batch.detach().cpu().numpy())\n",
    "\n",
    "    train_preds = np.concatenate(train_preds)\n",
    "    train_trues = np.concatenate(train_trues)\n",
    "    train_rmse = mean_squared_error(train_trues, train_preds)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            val_preds.append(y_pred.cpu().numpy())\n",
    "            val_trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_trues = np.concatenate(val_trues)\n",
    "    val_rmse = mean_squared_error(val_trues, val_preds)\n",
    "\n",
    "    train_rmse_history.append(train_rmse)\n",
    "    val_rmse_history.append(val_rmse)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d}/{NUM_EPOCHS} - Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_rmse < best_val_rmse - 1e-6:  # small tolerance\n",
    "        best_val_rmse = val_rmse\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (no improvement for {PATIENCE} epochs).\")\n",
    "            break\n",
    "\n",
    "# Restore best model weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ==============================\n",
    "# 13. FINAL EVALUATION (TRAIN / VAL / TEST)\n",
    "# ==============================\n",
    "def evaluate_loader(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            all_preds.append(y_pred.cpu().numpy())\n",
    "            all_trues.append(y_batch.cpu().numpy())\n",
    "    if len(all_preds) == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_trues)\n",
    "\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "\n",
    "    return rmse, mae, r2, y_true, y_pred\n",
    "\n",
    "train_rmse, train_mae, train_r2, y_train_true, y_train_pred = evaluate_loader(model, train_loader)\n",
    "val_rmse,   val_mae,   val_r2,   y_val_true,   y_val_pred   = evaluate_loader(model, val_loader)\n",
    "test_rmse,  test_mae,  test_r2,  y_test_true,  y_test_pred  = evaluate_loader(model, test_loader)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS (in original target units) =====\")\n",
    "print(f\"TRAIN - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "print(f\"VAL   - RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, R²: {val_r2:.4f}\")\n",
    "print(f\"TEST  - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# 14. PLOTS\n",
    "# ==============================\n",
    "\n",
    "# ---- RMSE vs Epoch ----\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(train_rmse_history, label=\"Train RMSE\")\n",
    "plt.plot(val_rmse_history,   label=\"Val RMSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(f\"Train & Validation RMSE (Crop: {CHOSEN_CROP})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
