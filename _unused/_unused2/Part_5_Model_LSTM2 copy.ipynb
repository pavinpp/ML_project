{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import copy\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optuna Visualization Tools\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4805a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. LOAD & CONFIG\n",
    "# ==============================\n",
    "CHOSEN_CROP = \"rice\" \n",
    "TARGET_COL = f\"Y_{CHOSEN_CROP}\"\n",
    "SEQ_LEN = 3\n",
    "PARQUET_PATH = \"Parquet/XY_v2.parquet\"\n",
    "\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df = df[(df[\"year\"] >= 1982) & (df[\"year\"] <= 2023)].copy()\n",
    "df = df.dropna(subset=[TARGET_COL]).copy()\n",
    "print(f\"--> Filtered years (1982-2023) and dropped NaN targets. New Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79828526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2. FEATURE SELECTION\n",
    "# ==============================\n",
    "df_model = df.copy()\n",
    "\n",
    "# Drop other target columns\n",
    "all_targets = [c for c in df_model.columns if c.startswith(\"Y_\")]\n",
    "df_model = df_model.drop(columns=[c for c in all_targets if c != TARGET_COL])\n",
    "\n",
    "# Drop unrelated crop yields\n",
    "avg_yield_cols = [c for c in df_model.columns if c.startswith(\"avg_yield_\")]\n",
    "chosen_prefix = f\"avg_yield_{CHOSEN_CROP}_\"\n",
    "keep_cols = [c for c in avg_yield_cols if c.startswith(chosen_prefix)]\n",
    "drop_cols = list(set(avg_yield_cols) - set(keep_cols))\n",
    "df_model = df_model.drop(columns=drop_cols)\n",
    "\n",
    "# Sort for time consistency\n",
    "df_model = df_model.sort_values([\"area\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [c for c in df_model.columns if c not in [\"area\", \"year\", TARGET_COL] and not c.startswith(\"Y_\")]\n",
    "print(f\"--> Selected {len(feature_cols)} input features for prediction.\")\n",
    "\n",
    "# Fill NaNs grouped by area\n",
    "df_model[feature_cols] = df_model.groupby(\"area\", group_keys=False)[feature_cols].apply(lambda g: g.ffill().bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c46e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. SPLIT & SCALE\n",
    "# ==============================\n",
    "train_mask = df_model[\"year\"] < 2014\n",
    "val_mask   = (df_model[\"year\"] >= 2014) & (df_model[\"year\"] <= 2018)\n",
    "test_mask  = df_model[\"year\"] >= 2019\n",
    "\n",
    "print(f\"--> Train samples: {sum(train_mask)}\")\n",
    "print(f\"--> Val samples:   {sum(val_mask)}\")\n",
    "print(f\"--> Test samples:  {sum(test_mask)}\")\n",
    "\n",
    "# Separate Features and Targets\n",
    "X_raw = df_model[feature_cols].values\n",
    "y_raw = df_model[[TARGET_COL]].values \n",
    "\n",
    "# Impute remaining NaNs (if any) with Train Mean\n",
    "# We calculate mean only on training data to avoid leakage\n",
    "train_mean = np.nanmean(X_raw[train_mask], axis=0)\n",
    "inds = np.where(np.isnan(X_raw))\n",
    "X_raw[inds] = np.take(train_mean, inds[1])\n",
    "\n",
    "# Initialize Scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler() # Important: Scale the target for LSTM stability\n",
    "\n",
    "# Fit on TRAIN only\n",
    "X_scaled = X_raw.copy()\n",
    "y_scaled = y_raw.copy()\n",
    "\n",
    "X_scaled[train_mask] = scaler_X.fit_transform(X_raw[train_mask])\n",
    "X_scaled[val_mask]   = scaler_X.transform(X_raw[val_mask])\n",
    "X_scaled[test_mask]  = scaler_X.transform(X_raw[test_mask])\n",
    "\n",
    "y_scaled[train_mask] = scaler_y.fit_transform(y_raw[train_mask])\n",
    "y_scaled[val_mask]   = scaler_y.transform(y_raw[val_mask])\n",
    "y_scaled[test_mask]  = scaler_y.transform(y_raw[test_mask])\n",
    "\n",
    "# Reconstruct DataFrame for sequence building\n",
    "df_scaled = df_model[[\"area\", \"year\"]].copy()\n",
    "df_scaled[\"target_scaled\"] = y_scaled\n",
    "X_df = pd.DataFrame(X_scaled, columns=feature_cols, index=df_model.index)\n",
    "df_final = pd.concat([df_scaled, X_df], axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. SEQUENCE BUILDING\n",
    "# ==============================\n",
    "def build_sequences(df, feat_cols, target_col, seq_len=3):\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    # Group by area to prevent data leakage between different regions\n",
    "    for _, g in df.groupby(\"area\"):\n",
    "        g = g.sort_values(\"year\")\n",
    "        feats = g[feat_cols].values\n",
    "        targs = g[target_col].values\n",
    "        \n",
    "        if len(g) <= seq_len: continue\n",
    "            \n",
    "        # Sliding window: Features [t-3, t-2, t-1] -> Target [t]\n",
    "        for i in range(len(g) - seq_len):\n",
    "            X_list.append(feats[i : i+seq_len])\n",
    "            y_list.append(targs[i+seq_len])\n",
    "            \n",
    "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.float32)\n",
    "\n",
    "# Generate sequences for each split\n",
    "def get_split_data(mask):\n",
    "    subset = df_final[mask].copy()\n",
    "    return build_sequences(subset, feature_cols, \"target_scaled\", SEQ_LEN)\n",
    "\n",
    "X_train, y_train = get_split_data(train_mask)\n",
    "X_val, y_val     = get_split_data(val_mask)\n",
    "X_test, y_test   = get_split_data(test_mask)\n",
    "\n",
    "print(f\"--> Train Sequences shape: {X_train.shape}\")\n",
    "print(f\"--> Val Sequences shape:   {X_val.shape}\")\n",
    "print(f\"--> Test Sequences shape:  {X_test.shape}\")\n",
    "\n",
    "# DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)), batch_size=BATCH_SIZE, shuffle= False)\n",
    "val_loader   = DataLoader(TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val)),   batch_size=BATCH_SIZE, shuffle= False)\n",
    "test_loader  = DataLoader(TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test)),  batch_size=BATCH_SIZE, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# ==============================\n",
    "# 5. MODEL DEFINITION\n",
    "# ==============================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2, activation=\"ReLU\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 2. Modified Output Head (Linear -> Activation -> Linear)\n",
    "        # This adds the non-linearity you saw in the example notebook\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Intermediate Dense Layer\n",
    "            getattr(nn, activation)(),          # Activation (ReLU, Tanh, etc.)\n",
    "            nn.Linear(hidden_dim, 1)            # Final Output Layer\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        last_step_out = out[:, -1, :]\n",
    "        \n",
    "        # Pass through the new sequential head\n",
    "        prediction = self.fc(last_step_out)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "# Initialize the modified model\n",
    "# Note: We added the 'activation' parameter here\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    input_dim=len(feature_cols), \n",
    "    hidden_dim=64, \n",
    "    activation=\"ReLU\"  # You can change this to \"Tanh\" or \"Sigmoid\" if you want\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ==============================\n",
    "# 6. TRAINING (Modified to Track Train RMSE)\n",
    "# ==============================\n",
    "epochs = 100\n",
    "patience = 15\n",
    "best_rmse = float('inf')\n",
    "counter = 0\n",
    "best_weights = None\n",
    "\n",
    "# History lists to store RMSE in ORIGINAL units\n",
    "train_rmse_history = []\n",
    "val_rmse_history = []\n",
    "\n",
    "print(f\"Starting training on {len(train_loader)} batches...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # --- TRAIN LOOP ---\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_b, y_b in train_loader:\n",
    "        X_b, y_b = X_b.to(device), y_b.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_b)\n",
    "        loss = criterion(pred, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # 1. Calculate Train RMSE (Scaled)\n",
    "    avg_train_loss_scaled = train_loss / len(train_loader)\n",
    "    train_rmse_scaled = np.sqrt(avg_train_loss_scaled)\n",
    "    \n",
    "    # 2. Convert to Original Units\n",
    "    # We multiply by the scaler's std dev to get the approximate RMSE in original units\n",
    "    train_rmse_orig = train_rmse_scaled * scaler_y.scale_[0]\n",
    "    train_rmse_history.append(train_rmse_orig)\n",
    "    \n",
    "    # --- VALIDATION LOOP ---\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in val_loader:\n",
    "            X_b = X_b.to(device)\n",
    "            pred = model(X_b).cpu().numpy()\n",
    "            val_preds.append(pred)\n",
    "            val_trues.append(y_b.numpy())\n",
    "            \n",
    "    if val_preds:\n",
    "        vp = np.concatenate(val_preds).reshape(-1, 1)\n",
    "        vt = np.concatenate(val_trues).reshape(-1, 1)\n",
    "        \n",
    "        # INVERSE TRANSFORM (Scale back to original units)\n",
    "        vp_inv = scaler_y.inverse_transform(vp)\n",
    "        vt_inv = scaler_y.inverse_transform(vt)\n",
    "        \n",
    "        # Calculate Val RMSE\n",
    "        val_rmse = np.sqrt(mean_squared_error(vt_inv, vp_inv))\n",
    "        val_rmse_history.append(val_rmse)\n",
    "        \n",
    "        # PRINT: Train RMSE vs Val RMSE (Both in Original Units)\n",
    "        print(f\"Epoch {epoch+1:03d}: Train RMSE {train_rmse_orig:.4f} | Val RMSE {val_rmse:.4f}\")\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "                break\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# ==============================\n",
    "# PLOT: Train RMSE vs Val RMSE\n",
    "# ==============================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_rmse_history, label='Train RMSE (Original Units)', color='blue')\n",
    "plt.plot(val_rmse_history, label='Val RMSE (Original Units)', color='orange')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(f\"RMSE (Yield: {CHOSEN_CROP})\")\n",
    "plt.title(\"Learning Curve: Train vs Validation Error\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474aa557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7. FINAL EVALUATION\n",
    "# ==============================\n",
    "if best_weights:\n",
    "    model.load_state_dict(best_weights)\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "def evaluate(loader):\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in loader:\n",
    "            preds.append(model(X_b.to(device)).cpu().numpy())\n",
    "            trues.append(y_b.numpy())\n",
    "    \n",
    "    if not preds: return 0, 0, 0, np.array([]), np.array([])\n",
    "    \n",
    "    p_inv = scaler_y.inverse_transform(np.concatenate(preds).reshape(-1, 1))\n",
    "    t_inv = scaler_y.inverse_transform(np.concatenate(trues).reshape(-1, 1))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(t_inv, p_inv))\n",
    "    mae = mean_absolute_error(t_inv, p_inv)\n",
    "    r2 = r2_score(t_inv, p_inv)\n",
    "    return rmse, mae, r2, p_inv, t_inv\n",
    "\n",
    "test_rmse, test_mae, test_r2, p_test, t_test = evaluate(test_loader)\n",
    "\n",
    "print(\"\\n=== Final Test Results ===\")\n",
    "print(f\"RMSE: {test_rmse:.2f}\")\n",
    "print(f\"MAE:  {test_mae:.2f}\")\n",
    "print(f\"R^2:  {test_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
