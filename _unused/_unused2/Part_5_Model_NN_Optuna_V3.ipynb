{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5986cd18",
   "metadata": {},
   "source": [
    "# Crop Yield Prediction: PyTorch Neural Network with Optuna (Part 5)\n",
    "\n",
    "## Overview\n",
    "This notebook trains a **Feedforward Neural Network (PyTorch)** model to predict crop yields. You can configure the specific target crop in the data loading section.\n",
    "\n",
    "## Methodology\n",
    "1.  **Crop Selection:** Choose the specific crop to predict.\n",
    "2.  **Feature Analysis:** Review the input variables.\n",
    "3.  **Time-Series Split:** Divide data by year to ensure we don't predict the past using the future:\n",
    "    * **Train:** Learn patterns.\n",
    "    * **Validation:** Tune settings.\n",
    "    * **Test:** Final evaluation.\n",
    "4.  **Data Scaling:** Normalize features for Neural Network stability.\n",
    "5.  **Baseline:** Compare against a simple guess (Last Year's Yield).\n",
    "6.  **Initial Model:** Train a default NN model and check learning curves for errors.\n",
    "7.  **Optimization:** Use **Optuna** to automatically find the best network architecture and hyperparameters.\n",
    "8.  **Final Evaluation:** Compare accuracy (RMSE) across all stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a747c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Optuna Visualization Tools\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008b386",
   "metadata": {},
   "source": [
    "### 1. Data Preparation and Crop Choice\n",
    "We load the main dataset and identify the available crops. For this analysis, we focus specifically on **Rice**. We clean the data by removing columns related to other crops and deleting any rows where the target yield information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_parquet('Parquet/XY_v3.parquet')\n",
    "\n",
    "# --- LIST AVAILABLE CROPS ---\n",
    "# Assumes targets start with 'Y_'\n",
    "target_columns = [col for col in df.columns if col.startswith('Y_')]\n",
    "available_crops = [col.replace('Y_', '') for col in target_columns]\n",
    "\n",
    "print(\"--- Available Crops found in Dataset ---\")\n",
    "print(available_crops)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- CONFIGURATION: SET CROP HERE ---\n",
    "CHOSEN_CROP = 'rice'  # <--- CHANGE THIS to 'lettuce', 'pepper', etc. based on list above\n",
    "# ------------------------------------\n",
    "\n",
    "# Define Target and Dynamic Lag Features\n",
    "TARGET_COL = f'Y_{CHOSEN_CROP}'\n",
    "LAG_1_FEATURE = f'avg_yield_{CHOSEN_CROP}_1y'\n",
    "\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Target {TARGET_COL} not found in dataset. Check spelling.\")\n",
    "\n",
    "print(f\"Predicting Target: {TARGET_COL}\")\n",
    "print(f\"Using Lag 1 Feature: {LAG_1_FEATURE}\")\n",
    "\n",
    "# Clean Missing Targets for the chosen crop\n",
    "df_model = df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "print(f\"Data Loaded. Rows with valid target: {len(df_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce99f69",
   "metadata": {},
   "source": [
    "### 2. Selecting Features, Splitting, and Scaling Data\n",
    "We identify the input variables. We split data by year to avoid data leakage. **Crucially**, for Neural Networks, we must scale the data (StandardScaler) so that all features have a mean of 0 and variance of 1, preventing gradient instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS (Add these if not already present) ---\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# --- DROP UNWANTED COLUMNS ---\n",
    "# Drop all columns that start with \"avg_yield_\" but do NOT match the chosen crop\n",
    "cols_to_drop = [c for c in df_model.columns \n",
    "                if c.startswith(\"avg_yield_\") and CHOSEN_CROP not in c]\n",
    "\n",
    "df_model = df_model.drop(columns=cols_to_drop)\n",
    "\n",
    "# --- FEATURE SELECTION ---\n",
    "# Select independent variables (exclude 'Y_' columns and metadata)\n",
    "feature_cols = [c for c in df_model.columns \n",
    "                if not c.startswith('Y_') and c not in ['area']]\n",
    "\n",
    "# --- DISPLAY FEATURES TABLE ---\n",
    "print(f\"Total Features Used: {len(feature_cols)}\")\n",
    "print(\"-\" * 30)\n",
    "feature_preview = pd.DataFrame(feature_cols, columns=['Feature Name']).T\n",
    "display(feature_preview)\n",
    "\n",
    "# --- TIME-SERIES SPLIT ---\n",
    "TRAIN_END_YEAR = 2014\n",
    "VAL_END_YEAR = 2019\n",
    "\n",
    "# 1. Training Set (< 2014)\n",
    "mask_train = df_model['year'] < TRAIN_END_YEAR\n",
    "X_train_raw = df_model[mask_train][feature_cols]\n",
    "y_train = df_model[mask_train][TARGET_COL]\n",
    "\n",
    "# 2. Validation Set (>= 2014 and < 2019)\n",
    "mask_val = (df_model['year'] >= TRAIN_END_YEAR) & (df_model['year'] < VAL_END_YEAR)\n",
    "X_val_raw = df_model[mask_val][feature_cols]\n",
    "y_val = df_model[mask_val][TARGET_COL]\n",
    "\n",
    "# 3. Test Set (>= 2019)\n",
    "mask_test = df_model['year'] >= VAL_END_YEAR\n",
    "X_test_raw = df_model[mask_test][feature_cols]\n",
    "y_test = df_model[mask_test][TARGET_COL]\n",
    "\n",
    "# --- IMPUTATION (Handle NaNs before scaling) ---\n",
    "imputer = SimpleImputer(strategy='mean')  # Or 'median' if data is skewed\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_raw), columns=feature_cols)\n",
    "X_val_imputed = pd.DataFrame(imputer.transform(X_val_raw), columns=feature_cols)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test_raw), columns=feature_cols)\n",
    "\n",
    "# Optional: Print NaN counts to verify (should be 0 after imputation)\n",
    "print(\"NaNs in X_train_imputed:\", X_train_imputed.isnull().sum().sum())\n",
    "print(\"NaNs in X_val_imputed:\", X_val_imputed.isnull().sum().sum())\n",
    "print(\"NaNs in X_test_imputed:\", X_test_imputed.isnull().sum().sum())\n",
    "\n",
    "# --- SCALING (Required for Neural Networks) ---\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_imputed)\n",
    "X_val = scaler.transform(X_val_imputed)\n",
    "X_test = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"\\nTraining Samples   (<{TRAIN_END_YEAR})     : {len(X_train)}\")\n",
    "print(f\"Validation Samples ({TRAIN_END_YEAR}-{VAL_END_YEAR - 1}): {len(X_val)}\")\n",
    "print(f\"Testing Samples    (>={VAL_END_YEAR})    : {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1eb4cf",
   "metadata": {},
   "source": [
    "### 3. Setting a Baseline\n",
    "Before using complex AI, we create a simple baseline to measure success. We assume that the yield this year will be exactly the same as last year. We calculate the error (RMSE) of this simple guess to establish a score we must beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: yield(t) = yield(t-1)\n",
    "# Note: We use the raw dataframe for baseline lag feature access\n",
    "y_pred_baseline = df_model[mask_test][LAG_1_FEATURE]\n",
    "\n",
    "# Clean NaNs for metric calculation\n",
    "mask_valid = ~y_pred_baseline.isna() & ~y_test.isna()\n",
    "y_test_clean = y_test[mask_valid]\n",
    "y_pred_clean = y_pred_baseline[mask_valid]\n",
    "\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test_clean, y_pred_clean))\n",
    "r2_baseline = r2_score(y_test_clean, y_pred_clean)\n",
    "\n",
    "print(f\"Baseline RMSE: {rmse_baseline:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607fbb7",
   "metadata": {},
   "source": [
    "### 4. Initial Model Testing\n",
    "We train a basic Feedforward Neural Network using standard settings. We plot the training vs validation loss to check for overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774242a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINE NEURAL NETWORK STRUCTURE ---\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- TRAINING HELPER FUNCTION ---\n",
    "def train_model(model, X_t, y_t, X_v, y_v, lr=0.001, epochs=150, batch_size=32, verbose=True):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_t, y_t), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_X.size(0)\n",
    "            \n",
    "        # Calculate average losses (RMSE representation)\n",
    "        train_mse = epoch_loss / len(X_t)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_v)\n",
    "            val_loss = criterion(val_outputs, y_v)\n",
    "            val_rmse = np.sqrt(val_loss.item())\n",
    "            \n",
    "        train_losses.append(train_rmse)\n",
    "        val_losses.append(val_rmse)\n",
    "        \n",
    "        if verbose and (epoch % 20 == 0 or epoch == epochs-1):\n",
    "            print(f\"Epoch {epoch}/{epochs} | Train RMSE: {train_rmse:.2f} | Val RMSE: {val_rmse:.2f}\")\n",
    "            \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# --- INITIAL MODEL TRAINING ---\n",
    "input_dim = X_train.shape[1]\n",
    "model_init = SimpleNN(input_dim).to(device)\n",
    "\n",
    "train_hist, val_hist = train_model(model_init, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor)\n",
    "\n",
    "# --- PLOT LEARNING CURVE ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_hist, label='Training RMSE', color='blue')\n",
    "plt.plot(val_hist, label='Validation RMSE', color='red')\n",
    "plt.title(f'Neural Network Learning Curve ({CHOSEN_CROP})', fontsize=15)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on TEST Set\n",
    "model_init.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_init_test = model_init(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "rmse_init_test = np.sqrt(mean_squared_error(y_test, y_pred_init_test))\n",
    "r2_init_test = r2_score(y_test, y_pred_init_test)\n",
    "\n",
    "print(f\"Initial Model Test RMSE: {rmse_init_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d9630",
   "metadata": {},
   "source": [
    "### 5. Tuning the Model (Optuna)\n",
    "To improve performance, we use **Optuna** to find the best neural network architecture. We run trials adjusting hidden layers, units, dropout rate, learning rate, and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DYNAMIC NN BUILDER ---\n",
    "class DynamicNN(nn.Module):\n",
    "    def __init__(self, input_dim, n_layers, n_units, dropout, activation_name):\n",
    "        super(DynamicNN, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        \n",
    "        # Choose Activation Function\n",
    "        if activation_name == \"ReLU\":\n",
    "            activation = nn.ReLU()\n",
    "        elif activation_name == \"LeakyReLU\":\n",
    "            activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            activation = nn.Tanh()\n",
    "\n",
    "        # Build Hidden Layers\n",
    "        for _ in range(n_layers):\n",
    "            layers.append(nn.Linear(in_features, n_units))\n",
    "            layers.append(activation)\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_features = n_units\n",
    "            \n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- OPTUNA OBJECTIVE FUNCTION ---\n",
    "def objective(trial):\n",
    "    # 1. Suggest Hyperparameters\n",
    "    # Scoped Optuna Search Space Based on Best Params\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 3, 3)   # fixed\n",
    "    n_units = trial.suggest_int(\"n_units\", 32, 48)   # small band around 42\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.14, 0.18)\n",
    "    lr = trial.suggest_float(\"lr\", 0.0006, 0.0010, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"LeakyReLU\", \"ReLU\"])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\"])\n",
    "\n",
    "\n",
    "    # 2. Build Model\n",
    "    model = DynamicNN(input_dim, n_layers, n_units, dropout, activation).to(device)\n",
    "    \n",
    "    # 3. Setup Training\n",
    "    criterion = nn.MSELoss()\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), \n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 4. Training Loop with Pruning\n",
    "    epochs = 50  # Reduced slightly for speed during tuning\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate on Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_mse = criterion(val_pred, y_val_tensor).item()\n",
    "            val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "        # Pruning check\n",
    "        trial.report(val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_rmse\n",
    "\n",
    "# --- RUN OPTIMIZATION ---\n",
    "study_name = f'{CHOSEN_CROP.capitalize()}_Yield_NN'\n",
    "study = optuna.create_study(direction='minimize', study_name=study_name)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"\\nBest Parameters found:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a3401",
   "metadata": {},
   "source": [
    "### 6. Visualizing Optimization\n",
    "We generate charts to understand the tuning process. These visual tools show us which specific settings had the biggest impact on reducing the model's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d396ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTUNA VISUALIZATIONS ---\n",
    "name = f\"{CHOSEN_CROP.capitalize()}_Yield_NN\"\n",
    "\n",
    "# 1. Optimization History\n",
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(title=f'{name} Optimization History', width=900, height=500)\n",
    "fig.show()\n",
    "\n",
    "# 2. Parallel Coordinate (Hyperparameter Relationships)\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.update_layout(title=f'{name} Parallel Coordinate Plot', width=900, height=500)\n",
    "fig.show()\n",
    "\n",
    "# 3. Slice Plot (Individual Parameter impact)\n",
    "fig = plot_slice(study)\n",
    "fig.update_layout(title=f'{name} Slice Plot', width=900, height=500)\n",
    "fig.show()\n",
    "\n",
    "# 4. Parameter Importance\n",
    "try:\n",
    "    fig = plot_param_importances(study)\n",
    "    fig.update_layout(title=f'{name} Hyperparameter Importance', width=900, height=500)\n",
    "    fig.show()\n",
    "except (ValueError, RuntimeError) as e:\n",
    "    print(f'Could not plot parameter importance: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99bf3cf",
   "metadata": {},
   "source": [
    "### 7. Final Model Training\n",
    "Using the best settings found during the tuning phase, we build the final neural network. We train this model on both the Training and Validation data combined to maximize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combine Train + Validation for Final Training\n",
    "X_train_full = np.vstack((X_train, X_val))\n",
    "y_train_full = np.concatenate((y_train, y_val))\n",
    "\n",
    "# Scale full dataset (technically scaler was fitted on Train only, which is correct practice)\n",
    "# We can convert full set to tensor now\n",
    "X_train_full_tensor = torch.tensor(X_train_full, dtype=torch.float32).to(device)\n",
    "y_train_full_tensor = torch.tensor(y_train_full, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# 2. Retrieve Best Params\n",
    "bp = study.best_params\n",
    "\n",
    "# 3. Initialize Best Model\n",
    "final_model = DynamicNN(\n",
    "    input_dim,\n",
    "    bp['n_layers'], \n",
    "    bp['n_units'], \n",
    "    bp['dropout'], \n",
    "    bp['activation']\n",
    ").to(device)\n",
    "\n",
    "# 4. Train on Full History\n",
    "# We train for more epochs since we are using more data, or sticking to the tuned amount\n",
    "# Here we default to a robust 150 epochs\n",
    "optimizer_name = bp['optimizer']\n",
    "if optimizer_name == \"Adam\":\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=bp['lr'])\n",
    "else:\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=bp['lr'], momentum=0.9)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "train_loader = DataLoader(TensorDataset(X_train_full_tensor, y_train_full_tensor), \n",
    "                          batch_size=bp['batch_size'], shuffle=True)\n",
    "\n",
    "print(\"Training Final Model...\")\n",
    "final_model.train()\n",
    "for epoch in range(150):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 5. Final Prediction on TEST Data\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_final_test = final_model(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "rmse_final_test = np.sqrt(mean_squared_error(y_test, y_pred_final_test))\n",
    "r2_final_test = r2_score(y_test, y_pred_final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f1bd7",
   "metadata": {},
   "source": [
    "### 8. Results and Analysis\n",
    "We evaluate the final performance on the Test data (2019–2023).\n",
    "* **Comparison:** We check if the Tuned Model beats the Baseline and the Initial Model.\n",
    "* **Trend Analysis:** We plot the predicted yields against actual yields over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Improvement %\n",
    "imp_final = (rmse_baseline - rmse_final_test) / rmse_baseline * 100\n",
    "\n",
    "print(\"--- Final Performance Report (Test Set) ---\")\n",
    "print(f\"Baseline Model: RMSE={rmse_baseline:.2f}, R2={r2_baseline:.4f}\")\n",
    "print(f\"Initial Model:  RMSE={rmse_init_test:.2f}, R2={r2_init_test:.4f}\")\n",
    "print(f\"Tuned NN Model: RMSE={rmse_final_test:.2f}, R2={r2_final_test:.4f} (RMSE Improved {imp_final:.2f}%)\")\n",
    "\n",
    "# --- PLOTTING RESULTS ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Axis Limits\n",
    "all_preds = np.concatenate([y_pred_clean, y_pred_init_test, y_pred_final_test])\n",
    "all_true = np.concatenate([y_test_clean, y_test, y_test])\n",
    "min_val, max_val = min(min(all_preds), min(all_true)), max(max(all_preds), max(all_true))\n",
    "\n",
    "# 1. Baseline Plot\n",
    "axes[0].scatter(y_test_clean, y_pred_clean, alpha=0.4, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[0].set_title(f'Baseline Model\\nRMSE: {rmse_baseline:.2f} | R2: {r2_baseline:.3f}')\n",
    "\n",
    "# 2. Initial Model Plot\n",
    "axes[1].scatter(y_test, y_pred_init_test, alpha=0.4, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[1].set_title(f'Initial NN Model\\nRMSE: {rmse_init_test:.2f} | R2: {r2_init_test:.3f}')\n",
    "\n",
    "# 3. Tuned Model Plot\n",
    "axes[2].scatter(y_test, y_pred_final_test, alpha=0.4, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[2].set_title(f'Tuned NN Model\\nRMSE: {rmse_final_test:.2f} | R2: {r2_final_test:.3f}')\n",
    "\n",
    "plt.suptitle(f'{CHOSEN_CROP.capitalize()} Yield: Performance Comparison (Actual vs Predicted)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FULL TIMELINE PLOT (FILTER BY COUNTRY) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Country to plot (parameter)\n",
    "TARGET_COUNTRY = \"Thailand\"    # <<--- Change here anytime\n",
    "\n",
    "# 1. Generate Predictions for all data (scaled)\n",
    "X_all_scaled = scaler.transform(df_model[feature_cols])\n",
    "X_all_tensor = torch.tensor(X_all_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = final_model(X_all_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# 2. Create DataFrame with Area column\n",
    "df_full_trend = pd.DataFrame({\n",
    "    'Year': df_model['year'],\n",
    "    'Area': df_model['area'],\n",
    "    'Actual': df_model[TARGET_COL],\n",
    "    'Predicted': all_predictions\n",
    "})\n",
    "\n",
    "# 3. Filter for selected country\n",
    "country_trend = df_full_trend[df_full_trend['Area'] == TARGET_COUNTRY]\n",
    "\n",
    "# 4. Aggregate by Year\n",
    "yearly_trend = country_trend.groupby('Year')[['Actual', 'Predicted']].mean()\n",
    "\n",
    "# 5. Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.plot(yearly_trend.index, yearly_trend['Actual'], \n",
    "         marker='o', label=f'Actual Yield ({TARGET_COUNTRY})', linewidth=2)\n",
    "plt.plot(yearly_trend.index, yearly_trend['Predicted'], \n",
    "         marker='x', linestyle='--', label=f'Predicted Yield ({TARGET_COUNTRY})', linewidth=2)\n",
    "\n",
    "# Define split boundaries\n",
    "MIN_YEAR = yearly_trend.index.min()\n",
    "MAX_YEAR = yearly_trend.index.max()\n",
    "train_boundary = TRAIN_END_YEAR - 0.5\n",
    "val_boundary   = VAL_END_YEAR - 0.5\n",
    "\n",
    "# Highlight training / validation / testing\n",
    "plt.axvspan(MIN_YEAR - 0.5, train_boundary, color='green',  alpha=0.1)\n",
    "plt.axvspan(train_boundary, val_boundary,   color='yellow', alpha=0.1)\n",
    "plt.axvspan(val_boundary, MAX_YEAR + 0.5,   color='red',    alpha=0.1)\n",
    "\n",
    "# Text labels\n",
    "y_max = yearly_trend['Actual'].max()\n",
    "text_y = y_max * 1.05\n",
    "\n",
    "plt.text((MIN_YEAR + train_boundary)/2, text_y, 'TRAINING',   ha='center', fontsize=12, fontweight='bold', color='green')\n",
    "plt.text((train_boundary + val_boundary)/2, text_y, 'VALIDATION', ha='center', fontsize=12, fontweight='bold', color='#D4AC0D')\n",
    "plt.text((val_boundary + MAX_YEAR)/2, text_y, 'TESTING',     ha='center', fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "# Final formatting\n",
    "plt.title(f'Full Timeline Analysis: Actual vs Predicted Yield ({CHOSEN_CROP}, {TARGET_COUNTRY})',\n",
    "          fontsize=16)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Yield (hg/ha)', fontsize=12)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(np.arange(MIN_YEAR, MAX_YEAR + 1, 2))\n",
    "plt.xlim(MIN_YEAR - 0.5, MAX_YEAR + 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02369b59",
   "metadata": {},
   "source": [
    "* **Geographic Error:** We map the error rates by country to see where the model performs best and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RE-CREATE COMPARISON DF WITH FEATURE JOINED ---\n",
    "# We need the original 'area' column from the test set for joining\n",
    "mask_test = df_model['year'] >= VAL_END_YEAR\n",
    "test_set_context = df_model[mask_test][['area', 'year']] \n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual_Value': y_test,\n",
    "    'Predicted_Value': y_pred_final_test\n",
    "})\n",
    "\n",
    "# Join works automatically because y_test retained index from original df_model\n",
    "comparison_df = comparison_df.join(test_set_context)\n",
    "comparison_df = comparison_df[['year', 'area', 'Actual_Value', 'Predicted_Value']]\n",
    "\n",
    "print(\"--- Actual vs. Predicted Test Set Results ---\")\n",
    "print(comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Name Cleaning for Map Plotting\n",
    "comparison_df['area'] = comparison_df['area'].replace({\n",
    "    'United_States_of_America': 'United States',\n",
    "    'United_Kingdom_of_Great_Britain_and_Northern_Ireland': 'United Kingdom',\n",
    "    'Russian_Federation': 'Russia',\n",
    "    'Viet_Nam': 'Vietnam',\n",
    "    'Türkiye': 'Turkey',\n",
    "    'Bolivia_(Plurinational_State_of)': 'Bolivia',\n",
    "    'Iran_(Islamic_Republic_of)': 'Iran',\n",
    "    \"Lao_People's_Democratic_Republic\": 'Laos',\n",
    "    'China,_mainland': 'China',\n",
    "    'China,_Taiwan_Province_of': 'Taiwan',\n",
    "    \"Democratic_People's_Republic_of_Korea\": 'North Korea',\n",
    "    'Republic_of_Korea': 'South Korea',\n",
    "    'Côte_d\\'Ivoire': \"Cote d'Ivoire\",\n",
    "    'United_Republic_of_Tanzania': 'Tanzania',\n",
    "    'Micronesia_(Federated_States_of)': 'Micronesia',\n",
    "    'Venezuela_(Bolivarian_Republic_of)': 'Venezuela'\n",
    "})\n",
    "\n",
    "def plot_geographic_error(comparison_df):\n",
    "    # Squared Error (for RMSE)\n",
    "    comparison_df['Squared_Error'] = (comparison_df['Actual_Value'] - comparison_df['Predicted_Value']) ** 2\n",
    "    # Squared Percentage Error (for RMSPE)\n",
    "    epsilon = 1e-6 \n",
    "    comparison_df['Squared_Percentage_Error'] = (\n",
    "        (comparison_df['Actual_Value'] - comparison_df['Predicted_Value']) / \n",
    "        (comparison_df['Actual_Value'] + epsilon)\n",
    "    ) ** 2\n",
    "\n",
    "    # Aggregate Errors by Country\n",
    "    rmse_df = (\n",
    "        comparison_df.groupby('area')['Squared_Error']\n",
    "        .mean().apply(np.sqrt).reset_index()\n",
    "        .rename(columns={'area': 'Country', 'Squared_Error': 'RMSE'})\n",
    "    )\n",
    "    rmspe_df = (\n",
    "        comparison_df.groupby('area')['Squared_Percentage_Error']\n",
    "        .mean().apply(np.sqrt).multiply(100).reset_index()\n",
    "        .rename(columns={'area': 'Country', 'Squared_Percentage_Error': 'RMSPE'})\n",
    "    )\n",
    "    ap_df = comparison_df.groupby('area')[['Actual_Value', 'Predicted_Value']].mean().reset_index()\n",
    "    ap_df = ap_df.rename(columns={'area': 'Country'})\n",
    "\n",
    "    # Merge stats\n",
    "    error_stats = rmspe_df.merge(rmse_df, on='Country', how='left')\n",
    "    error_stats = error_stats.merge(ap_df, on='Country', how='left') \n",
    "\n",
    "    # Plot\n",
    "    fig = px.choropleth(\n",
    "        error_stats,\n",
    "        locations='Country',\n",
    "        color='RMSPE',\n",
    "        locationmode='country names',\n",
    "        color_continuous_scale=['green', 'red'], \n",
    "        range_color=[0, 50], \n",
    "        title='Geographic Distribution of Prediction Error (RMSPE)',\n",
    "        labels={'RMSPE': 'RMSPE (%)'},\n",
    "        hover_name='Country',\n",
    "        hover_data={'RMSPE': ':.2f', 'RMSE': ':.2f', 'Actual_Value': ':.2f', 'Predicted_Value': ':.2f'},\n",
    "        projection='natural earth'\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        coloraxis_colorbar=dict(title='RMSPE (%)', orientation='h', len=0.5, yanchor='bottom', y=-0.12),\n",
    "        geo=dict(showframe=False, showcoastlines=True, showcountries=True, countrycolor='black', bgcolor='lightgrey')\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_geographic_error(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fc4f6",
   "metadata": {},
   "source": [
    "### 9. Key Factors (Feature Importance)\n",
    "Unlike Decision Trees, Neural Networks don't have a built-in feature importance metric. To estimate this, we use **Permutation Importance**: we randomly shuffle one feature column at a time and measure how much the prediction error increases. A large increase means that feature was important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e313a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRAPPER FOR SKLEARN COMPATIBILITY ---\n",
    "class PyTorchEstimator:\n",
    "    \"\"\"Wrapper to make PyTorch model behave like a Sklearn estimator for permutation_importance\"\"\"\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass # Model is already trained\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            preds = self.model(X_tensor).cpu().numpy().flatten()\n",
    "        return preds\n",
    "\n",
    "# --- CALCULATE PERMUTATION IMPORTANCE ---\n",
    "# Using validation set to gauge importance generalization\n",
    "wrapped_model = PyTorchEstimator(final_model, device)\n",
    "results = permutation_importance(wrapped_model, X_val, y_val, scoring='neg_root_mean_squared_error', n_repeats=5, random_state=42)\n",
    "\n",
    "# --- PROCESS RESULTS ---\n",
    "# Note: permutation_importance returns negative RMSE changes for 'neg_root...', \n",
    "# so lower (more negative) is worse performance when shuffled -> higher importance.\n",
    "# We take absolute value of the mean importances to show magnitude of impact.\n",
    "importance_means = np.abs(results.importances_mean)\n",
    "feature_names = np.array(feature_cols)\n",
    "\n",
    "# Create DataFrame\n",
    "fi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importance_means\n",
    "}).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Print Top 20\n",
    "print(\"\\n--- Top 20 Most Important Features (Permutation Importance) ---\")\n",
    "print(fi_df.head(20))\n",
    "\n",
    "# --- PLOT ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=fi_df.head(20), palette='viridis')\n",
    "plt.title(f'Feature Importance (Permutation) - {CHOSEN_CROP.capitalize()} NN Model', fontsize=15)\n",
    "plt.xlabel('Increase in RMSE when shuffled')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
